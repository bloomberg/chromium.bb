diff -r c06933b0e3e6 llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
--- a/llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
+++ b/llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
@@ -54,6 +54,17 @@
                                         unsigned Alignment) = 0;
 
   virtual void AddSelectionDAGCSEId(FoldingSetNodeID &ID) = 0;
+
+  // @LOCALMOD-START
+  /// getJumpTableIndex - Check if this is a reference to a jump table.
+  /// If so, return a pointer to the jump table index value that is stored
+  /// in the constant pool, else return 0.
+  /// The default behavior is to indicate that the value is not a jump table
+  /// index. This is used by BranchFolder::runOnMachineFunction() and only in
+  /// conjunction with ARM targets
+  /// TODO: this should be cleaned up as it does tripple duty: tester, setter, getter
+  virtual unsigned *getJumpTableIndex() { return 0; }
+  // @LOCALMOD-END
 
   /// print - Implement operator<<
   virtual void print(raw_ostream &O) const = 0;
diff -r c06933b0e3e6 llvm-trunk/lib/CodeGen/BranchFolding.cpp
--- a/llvm-trunk/lib/CodeGen/BranchFolding.cpp
+++ b/llvm-trunk/lib/CodeGen/BranchFolding.cpp
@@ -20,6 +20,7 @@
 #include "BranchFolding.h"
 #include "llvm/Function.h"
 #include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/MachineConstantPool.h" //  @LOCALMOD
 #include "llvm/CodeGen/MachineModuleInfo.h"
 #include "llvm/CodeGen/MachineFunctionPass.h"
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
@@ -241,6 +242,24 @@
           JTIsLive.set(NewIdx);
         }
     }
+
+    // @LOCALMOD-START
+    // This currently only used on ARM targets where the ConstantPool
+    // subclass is overloading getJumpTableIndex()
+    const std::vector<MachineConstantPoolEntry>& CPs =
+      MF.getConstantPool()->getConstants();
+    for (unsigned i = 0, e = CPs.size(); i != e; ++i) {
+      if (!CPs[i].isMachineConstantPoolEntry()) continue;
+      unsigned *JTIndex = CPs[i].Val.MachineCPVal->getJumpTableIndex();
+      if (!JTIndex) continue;
+      // Remap index.
+      const unsigned NewIdx = JTMapping[*JTIndex];
+      *JTIndex = NewIdx;
+      // Remember that this JT is live.
+      JTIsLive.set(NewIdx);
+    }
+    // @LOCALMOD-END
+
 
     // Finally, remove dead jump tables.  This happens either because the
     // indirect jump was unreachable (and thus deleted) or because the jump
diff -r c06933b0e3e6 llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
--- a/llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+++ b/llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
@@ -5853,6 +5853,11 @@
   if (ConstantFPSDNode *TV = dyn_cast<ConstantFPSDNode>(N2))
     if (ConstantFPSDNode *FV = dyn_cast<ConstantFPSDNode>(N3)) {
       if (TLI.isTypeLegal(N2.getValueType()) &&
+          // @LOCALMOD-START
+          // when we combine two 8byte constants into a 16byte one
+          // we get constant pool entries which are too big
+          TLI.getTargetData()->getTypeAllocSize(FV->getConstantFPValue()->getType()) <= 4 &&
+          // @LOCALMOD-STOP
           (TLI.getOperationAction(ISD::ConstantFP, N2.getValueType()) !=
            TargetLowering::Legal) &&
           // If both constants have multiple uses, then we won't need to do an
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARM.h
--- a/llvm-trunk/lib/Target/ARM/ARM.h
+++ b/llvm-trunk/lib/Target/ARM/ARM.h
@@ -105,6 +105,7 @@
 FunctionPass *createARMLoadStoreOptimizationPass(bool PreAlloc = false);
 FunctionPass *createARMExpandPseudoPass();
 FunctionPass *createARMConstantIslandPass();
+FunctionPass *createARMSFIPlacementPass();
 FunctionPass *createNEONPreAllocPass();
 FunctionPass *createNEONMoveFixPass();
 FunctionPass *createThumb2ITBlockPass();
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
@@ -34,6 +34,9 @@
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 using namespace llvm;
+
+
+extern cl::opt<bool> FlagSfiStack; // @LOCALMOD
 
 static cl::opt<bool>
 EnableARM3Addr("enable-arm-3-addr-conv", cl::Hidden,
@@ -411,6 +414,9 @@
   return JT[JTI].MBBs.size();
 }
 
+// @LOCALMOD-START
+// @NOTE: this needs to be fixe to make the constand island estimates better
+// @LOCALMOD-END
 /// GetInstSize - Return the size of the specified MachineInstr.
 ///
 unsigned ARMBaseInstrInfo::GetInstSizeInBytes(const MachineInstr *MI) const {
@@ -634,8 +640,17 @@
   }
 
   if (DestRC == ARM::GPRRegisterClass) {
+    // @LOCALMOD-START
+    // NOTE: rename stack loads/moves so we have an sfi hook
+    if (FlagSfiStack && DestReg == ARM::SP ) {
+      AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::STACK_MOVr),
+					  DestReg).addReg(SrcReg)));
+    } else {
+      // ORGIGNAL
     AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::MOVr),
                                         DestReg).addReg(SrcReg)));
+    }
+    // @LOCALMOD-END
   } else if (DestRC == ARM::SPRRegisterClass) {
     AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::VMOVS), DestReg)
                    .addReg(SrcReg));
@@ -1047,7 +1062,17 @@
     assert(ARM_AM::getSOImmVal(ThisVal) != -1 && "Bit extraction didn't work?");
 
     // Build the new ADD / SUB.
-    unsigned Opc = isSub ? ARM::SUBri : ARM::ADDri;
+    // @LOCALMOD-START
+    // NOTE: add stackhook
+    unsigned Opc;
+    if (FlagSfiStack && DestReg == ARM::SP ) {
+      Opc = isSub ? ARM::STACK_SUBri : ARM::STACK_ADDri;
+
+      /* assert(BaseReg == DestReg); */
+    } else {
+      Opc = isSub ? ARM::SUBri : ARM::ADDri;
+    }
+    // @LOCALMOD-END
     BuildMI(MBB, MBBI, dl, TII.get(Opc), DestReg)
       .addReg(BaseReg, RegState::Kill).addImm(ThisVal)
       .addImm((unsigned)Pred).addReg(PredReg).addReg(0);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
@@ -35,6 +35,15 @@
 #include <algorithm>
 using namespace llvm;
 
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+
+cl::opt<bool> FlagSfiCpFudge("sfi-cp-fudge");
+cl::opt<int> FlagSfiCpFudgePercent("sfi-cp-fudge-percent", cl::init(85));
+extern cl::opt<bool> FlagSfiBranch;
+// @LOCALMOD-END
+
+
 STATISTIC(NumCPEs,       "Number of constpool entries");
 STATISTIC(NumSplit,      "Number of uncond branches inserted");
 STATISTIC(NumCBrFixed,   "Number of cond branches fixed");
@@ -52,6 +61,7 @@
           cl::desc("Adjust basic block layout to better use TB[BH]"));
 
 namespace {
+
   /// ARMConstantIslands - Due to limited PC-relative displacements, ARM
   /// requires constant pool entries to be scattered among the instructions
   /// inside a function.  To do this, it completely ignores the normal LLVM
@@ -179,6 +189,13 @@
     }
 
   private:
+    // @LOCALMOD-BEGIN
+    unsigned GetFudge(const MachineInstr* I,
+                      unsigned  offset,
+                      bool is_start,
+                      bool is_end,
+                      bool is_jump_target) const;
+    // @LOCALMOD-END
     void DoInitialPlacement(MachineFunction &MF,
                             std::vector<MachineInstr*> &CPEMIs);
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
@@ -259,7 +276,6 @@
 
 bool ARMConstantIslands::runOnMachineFunction(MachineFunction &MF) {
   MachineConstantPool &MCP = *MF.getConstantPool();
-
   TII = MF.getTarget().getInstrInfo();
   AFI = MF.getInfo<ARMFunctionInfo>();
   STI = &MF.getTarget().getSubtarget<ARMSubtarget>();
@@ -467,11 +483,132 @@
   }
 }
 
+
+//@LOCALMOD-START
+// We try to account for extra sfi space overhead here
+// NOTE: This function needs to be updatd whenever changes
+//       to the sfi scheme are made
+// NOTE: this is very likely missing a few cases
+//       we will add those as neeeded and otherwise
+//       rely on artificially reducing the ldr offset range.
+// NOTE: one missing case: jump table targets are 16 bytes aligned
+unsigned ARMConstantIslands::GetFudge(const MachineInstr* I,
+                                      unsigned  offset,
+                                      bool is_start,
+                                      bool is_end,
+                                      bool is_jump_target) const {
+  if (!FlagSfiCpFudge) return 0;
+  const int kBundleSize = 16;
+  unsigned fudge = 0;
+  const int Opc = I->getOpcode();
+
+  if (is_jump_target && is_start) {
+    while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+  }
+
+  switch(Opc) {
+   case ARM::BL:
+   case ARM::BLX:
+   case ARM::BL_pred:
+   case ARM::BLr9:
+   case ARM::BLXr9:
+   case ARM::BLr9_pred:
+   case ARM::TPsoft:
+    // branches must be in the last slot
+    while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    break;
+
+   case ARM::CONSTPOOL_ENTRY:
+    if (is_start) {
+      while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+    }
+
+    if (is_end) {
+      while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    }
+
+    {
+      const int size = TII->GetInstSizeInBytes(I);
+      assert (size == 4 || size == 8);
+      // we do not want the data to cross bundle boundaries
+      if (size == 8) {
+        if((offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      }
+    }
+    // illegal if at data bundle beginning
+    if ((offset + fudge) % kBundleSize == 0) fudge += 4;
+    break;
+
+   case ARM::STACK_ADDri:
+   case ARM::STACK_SUBri:
+   case ARM::STACK_MOVr:
+    // stack adjusts must not be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // add masking
+    fudge += 4;
+    break;
+
+   case ARM::STR:
+   case ARM::STRB:
+   case ARM::STRH:
+   case ARM::STRD:
+    // TODO: there are vfp stores missing
+   case ARM::VSTRS:
+   case ARM::VSTRD:
+
+    // case ARM::STM:// TODO: make this work
+    {
+    const MachineOperand &MO1 = I->getOperand(1);
+    if (MO1.getReg() != ARM::SP) {
+      // cannot be in the last slot
+      if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      // one mask
+      fudge += 4;
+    }
+    break;
+    }
+  case ARM::BX:
+  case ARM::BXr9:
+  case ARM::BX_RET:
+  case ARM::BRIND:
+    // cannot be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // one mask
+    fudge += 4;
+    break;
+  }
+
+  return fudge;
+}
+
+static void UpdateJumpTargetAlignment(MachineFunction &MF) {
+  if (!FlagSfiBranch) return;
+
+  // JUMP TABLE TARGETS
+  MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+  const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+  for (unsigned i=0; i < JT.size(); ++i) {
+    std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+    //cout << "JUMPTABLE "<< i << " " << MBBs.size() << "\n";
+    for (unsigned j=0; j < MBBs.size(); ++j) {
+      if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+        continue;
+      }
+      MBBs[j]->setAlignment(16);
+    }
+  }
+}
+
+//@LOCALMOD-END
+
 /// InitialFunctionScan - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
 void ARMConstantIslands::InitialFunctionScan(MachineFunction &MF,
                                  const std::vector<MachineInstr*> &CPEMIs) {
+  UpdateJumpTargetAlignment(MF); // @LOCALMOD
+
   unsigned Offset = 0;
   for (MachineFunction::iterator MBBI = MF.begin(), E = MF.end();
        MBBI != E; ++MBBI) {
@@ -483,12 +620,24 @@
       WaterList.push_back(&MBB);
 
     unsigned MBBSize = 0;
+
     for (MachineBasicBlock::iterator I = MBB.begin(), E = MBB.end();
          I != E; ++I) {
+      //@LOCALMOD-START
+      // TODO: also account for jump_targets more
+      MBBSize += GetFudge(I,
+                          Offset,
+                          I == MBB.begin(),
+                          I == E,
+                          MF.begin() == MBBI || MBBI->getAlignment() == 16);
+      //@LOCALMOD-END
+
       // Add instruction size to MBBSize.
       MBBSize += TII->GetInstSizeInBytes(I);
 
       int Opc = I->getOpcode();
+
+
       if (I->getDesc().isBranch()) {
         bool isCond = false;
         unsigned Bits = 0;
@@ -545,6 +694,7 @@
 
       if (Opc == ARM::tPUSH || Opc == ARM::tPOP_RET)
         PushPopMIs.push_back(I);
+
 
       if (Opc == ARM::CONSTPOOL_ENTRY)
         continue;
@@ -611,6 +761,13 @@
           unsigned CPI = I->getOperand(op).getIndex();
           MachineInstr *CPEMI = CPEMIs[CPI];
           unsigned MaxOffs = ((1 << Bits)-1) * Scale;
+
+          // @LOCALMOD-BEGIN
+          if (FlagSfiCpFudge) {
+            MaxOffs *= FlagSfiCpFudgePercent;
+            MaxOffs /= 100;
+          }
+          // @LOCALMOD-END
           CPUsers.push_back(CPUser(I, CPEMI, MaxOffs, NegOk, IsSoImm));
 
           // Increment corresponding CPEntry reference count.
@@ -661,6 +818,11 @@
     assert(I != MBB->end() && "Didn't find MI in its own basic block?");
     if (&*I == MI) return Offset;
     Offset += TII->GetInstSizeInBytes(I);
+
+    // @LOCALMOD-START
+    // TODO: take jump targets into account
+    Offset += GetFudge(I, Offset, I ==  MBB->begin(), I == MBB->end(), 0);
+    // @LOCALMOD-END
   }
 }
 
@@ -763,9 +925,12 @@
   // contain a constpool_entry or tablejump.)
   unsigned NewBBSize = 0;
   for (MachineBasicBlock::iterator I = NewBB->begin(), E = NewBB->end();
-       I != E; ++I)
+       I != E; ++I) {
+    // @LOCALMOD-START
+    NewBBSize += GetFudge(I, NewBBSize, false, false, 0);
+    // @LOCALMOD-END
     NewBBSize += TII->GetInstSizeInBytes(I);
-
+  }
   unsigned OrigBBI = OrigBB->getNumber();
   unsigned NewBBI = NewBB->getNumber();
   // Set the size of NewBB in BBSizes.
@@ -882,9 +1047,19 @@
 void ARMConstantIslands::AdjustBBOffsetsAfter(MachineBasicBlock *BB,
                                               int delta) {
   MachineFunction::iterator MBBI = BB; MBBI = next(MBBI);
+  // @LOCALMOD-START
+#if 1
+  if (delta > 0) {
+    BBSizes[BB->getNumber()] += 4;  // @LOCALMOD
+    delta += 4;
+  }
+#endif
+  // @LOCALMOD-END
+
   for(unsigned i = BB->getNumber()+1, e = BB->getParent()->getNumBlockIDs();
       i < e; ++i) {
     BBOffsets[i] += delta;
+
     // If some existing blocks have padding, adjust the padding as needed, a
     // bit tricky.  delta can be negative so don't use % on that.
     if (!isThumb)
@@ -1122,7 +1297,15 @@
     // The 4 in the following is for the unconditional branch we'll be
     // inserting (allows for long branch on Thumb1).  Alignment of the
     // island is handled inside OffsetIsInRange.
-    unsigned BaseInsertOffset = UserOffset + U.MaxDisp -4;
+     // @LOCALMOD-START
+     unsigned BaseInsertOffset = UserOffset - 4;
+     if (FlagSfiCpFudge) {
+       BaseInsertOffset += U.MaxDisp * FlagSfiCpFudgePercent / 100;
+     } else {
+       BaseInsertOffset += U.MaxDisp;
+     }
+     // @LOCALMOD-END
+     
     // This could point off the end of the block if we've already got
     // constant pool entries following this block; only the last one is
     // in the water list.  Back past any possible branches (allow for a
@@ -1135,10 +1318,17 @@
     MachineBasicBlock::iterator MI = UserMI;
     ++MI;
     unsigned CPUIndex = CPUserIndex+1;
+    // @LOCALMOD: TODO: GetInstSizeInBytes() should be replaced with
+    //                  our estimator
     for (unsigned Offset = UserOffset+TII->GetInstSizeInBytes(UserMI);
          Offset < BaseInsertOffset;
-         Offset += TII->GetInstSizeInBytes(MI),
-            MI = next(MI)) {
+         // @LOCALMOD-START
+         Offset +=  GetFudge(MI, Offset, false, false, false) +
+                           TII->GetInstSizeInBytes(MI),
+        // @LOCALMOD-END 
+         MI = next(MI)) {
+      
+      assert(MI != UserMBB->end() && "getting out of range"); // @LOCALMOD
       if (CPUIndex < CPUsers.size() && CPUsers[CPUIndex].MI == MI) {
         CPUser &U = CPUsers[CPUIndex];
         if (!OffsetIsInRange(Offset, EndInsertOffset,
@@ -1354,6 +1544,9 @@
 /// Otherwise, add an intermediate branch instruction to a branch.
 bool
 ARMConstantIslands::FixUpUnconditionalBr(MachineFunction &MF, ImmBranch &Br) {
+  // @LOCALMOD-start
+  assert(0 && "fix up uncond br not implemented");
+  // @LOCALMOD-end
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *MBB = MI->getParent();
   if (!isThumb1)
@@ -1377,6 +1570,9 @@
 /// conditional branch + an unconditional branch to the destination.
 bool
 ARMConstantIslands::FixUpConditionalBr(MachineFunction &MF, ImmBranch &Br) {
+  // @LOCALMOD-start
+  assert(0 && "fix up cond br not implemented");
+  // @LOCALMOD-end
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *DestBB = MI->getOperand(0).getMBB();
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
@@ -27,7 +27,8 @@
                                            const char *Modif,
                                            bool AddCA)
   : MachineConstantPoolValue((const Type*)cval->getType()),
-    CVal(cval), S(NULL), LabelId(id), Kind(K), PCAdjust(PCAdj),
+    // @LOCALMOD
+    CVal(cval), S(NULL),  JumpTableIndex(0), LabelId(id), Kind(K), PCAdjust(PCAdj),
     Modifier(Modif), AddCurrentAddress(AddCA) {}
 
 ARMConstantPoolValue::ARMConstantPoolValue(LLVMContext &C,
@@ -36,13 +37,24 @@
                                            const char *Modif,
                                            bool AddCA)
   : MachineConstantPoolValue((const Type*)Type::getInt32Ty(C)),
-    CVal(NULL), S(strdup(s)), LabelId(id), Kind(ARMCP::CPExtSymbol),
+    // @LOCALMOD
+    CVal(NULL), S(strdup(s)), JumpTableIndex(0), LabelId(id), Kind(ARMCP::CPExtSymbol),
     PCAdjust(PCAdj), Modifier(Modif), AddCurrentAddress(AddCA) {}
 
 ARMConstantPoolValue::ARMConstantPoolValue(GlobalValue *gv, const char *Modif)
   : MachineConstantPoolValue((const Type*)Type::getInt32Ty(gv->getContext())),
-    CVal(gv), S(NULL), LabelId(0), Kind(ARMCP::CPValue), PCAdjust(0),
+    // @LOCALMOD
+    CVal(gv), S(NULL), JumpTableIndex(0), LabelId(0), Kind(ARMCP::CPValue), PCAdjust(0),
     Modifier(Modif) {}
+
+// @LOCALMOD-START
+ARMConstantPoolValue::ARMConstantPoolValue(LLVMContext &C, unsigned jt)
+  : MachineConstantPoolValue((const Type*)Type::getInt32Ty(C)),
+      CVal(NULL), S(NULL), JumpTableIndex(jt), LabelId(0), Kind(ARMCP::CPJumpTable),
+    PCAdjust(0), Modifier(NULL) {}
+// @LOCALMOD-END
+
+
 
 GlobalValue *ARMConstantPoolValue::getGV() const {
   return dyn_cast_or_null<GlobalValue>(CVal);
@@ -63,6 +75,7 @@
         (ARMConstantPoolValue *)Constants[i].Val.MachineCPVal;
       if (CPV->CVal == CVal &&
           CPV->LabelId == LabelId &&
+          CPV->JumpTableIndex == JumpTableIndex && // @LOCALMOD
           CPV->PCAdjust == PCAdjust &&
           (CPV->S == S || strcmp(CPV->S, S) == 0) &&
           (CPV->Modifier == Modifier || strcmp(CPV->Modifier, Modifier) == 0))
@@ -81,6 +94,7 @@
 ARMConstantPoolValue::AddSelectionDAGCSEId(FoldingSetNodeID &ID) {
   ID.AddPointer(CVal);
   ID.AddPointer(S);
+  ID.AddInteger(JumpTableIndex);   // @LOCALMOD
   ID.AddInteger(LabelId);
   ID.AddInteger(PCAdjust);
 }
@@ -89,6 +103,7 @@
 ARMConstantPoolValue::hasSameValue(ARMConstantPoolValue *ACPV) {
   if (ACPV->Kind == Kind &&
       ACPV->CVal == CVal &&
+      ACPV->JumpTableIndex == JumpTableIndex && // @LOCALMOD
       ACPV->PCAdjust == PCAdjust &&
       (ACPV->S == S || strcmp(ACPV->S, S) == 0) &&
       (ACPV->Modifier == Modifier || strcmp(ACPV->Modifier, Modifier) == 0)) {
@@ -110,6 +125,10 @@
 void ARMConstantPoolValue::print(raw_ostream &O) const {
   if (CVal)
     O << CVal->getName();
+  // @LOCALMOD-START
+  else if (isJumpTable())
+    O << "jumptable_" << JumpTableIndex;
+  // @LOCALMOD-END
   else
     O << S;
   if (Modifier) O << "(" << Modifier << ")";
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
--- a/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
@@ -28,7 +28,8 @@
     CPValue,
     CPExtSymbol,
     CPBlockAddress,
-    CPLSDA
+    CPLSDA,
+    CPJumpTable // @LOCALMOD
   };
 }
 
@@ -38,6 +39,7 @@
 class ARMConstantPoolValue : public MachineConstantPoolValue {
   Constant *CVal;          // Constant being loaded.
   const char *S;           // ExtSymbol being loaded.
+  unsigned JumpTableIndex; // Index of a jump table. // @LOCALMOD
   unsigned LabelId;        // Label id of the load.
   ARMCP::ARMCPKind Kind;   // Kind of constant.
   unsigned char PCAdjust;  // Extra adjustment if constantpool is pc-relative.
@@ -54,6 +56,7 @@
                        unsigned char PCAdj = 0, const char *Modifier = NULL,
                        bool AddCurrentAddress = false);
   ARMConstantPoolValue(GlobalValue *GV, const char *Modifier);
+  ARMConstantPoolValue(LLVMContext &C, unsigned jt); // @LOCALMOD
   ARMConstantPoolValue();
   ~ARMConstantPoolValue();
 
@@ -69,6 +72,13 @@
   bool isExtSymbol() const { return Kind == ARMCP::CPExtSymbol; }
   bool isBlockAddress() { return Kind == ARMCP::CPBlockAddress; }
   bool isLSDA() { return Kind == ARMCP::CPLSDA; }
+  // @LOCALMOD-START
+  bool isValue() const { return Kind == ARMCP::CPValue; }
+  bool isJumpTable() const { return Kind == ARMCP::CPJumpTable; }
+  virtual unsigned *getJumpTableIndex() {
+    return isJumpTable() ? &JumpTableIndex : 0;
+  }
+  // @LOCALMOD-END
 
   virtual unsigned getRelocationInfo() const {
     // FIXME: This is conservatively claiming that these entries require a
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
@@ -34,6 +34,10 @@
 #include "llvm/Support/raw_ostream.h"
 
 using namespace llvm;
+
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
+
 
 //===--------------------------------------------------------------------===//
 /// ARMDAGToDAGISel - ARM specific code to select ARM machine
@@ -218,6 +222,11 @@
 bool ARMDAGToDAGISel::SelectAddrMode2(SDValue Op, SDValue N,
                                       SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
+
+
+  const bool is_store = (Op.getOpcode() == ISD::STORE); // @LOCALMOD
+  if (!FlagSfiStore || !is_store ) { // @LOCALMOD
+
   if (N.getOpcode() == ISD::MUL) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
       // X * [3,5,9] -> X + X * [2,4,8] etc.
@@ -240,6 +249,7 @@
       }
     }
   }
+  } // @LOCALMOD
 
   if (N.getOpcode() != ISD::ADD && N.getOpcode() != ISD::SUB) {
     Base = N;
@@ -257,7 +267,7 @@
   }
 
   // Match simple R +/- imm12 operands.
-  if (N.getOpcode() == ISD::ADD)
+  if (N.getOpcode() == ISD::ADD) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
       int RHSC = (int)RHS->getZExtValue();
       if ((RHSC >= 0 && RHSC < 0x1000) ||
@@ -280,6 +290,24 @@
         return true;
       }
     }
+  }
+
+  // @LOCALMOD-START
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    if (N.getOpcode() == ISD::FrameIndex) {
+      int FI = cast<FrameIndexSDNode>(N)->getIndex();
+      Base = CurDAG->getTargetFrameIndex(FI, TLI.getPointerTy());
+    } else if (N.getOpcode() == ARMISD::Wrapper) {
+      Base = N.getOperand(0);
+    }
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(ARM_AM::add, 0,
+                                                      ARM_AM::no_shift),
+                                    MVT::i32);
+    return true;
+  }
+  // @LOCALMOD-END
 
   // Otherwise this is R +/- [possibly shifted] R.
   ARM_AM::AddrOpc AddSub = N.getOpcode() == ISD::ADD ? ARM_AM::add:ARM_AM::sub;
@@ -320,9 +348,13 @@
 
   Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(AddSub, ShAmt, ShOpcVal),
                                   MVT::i32);
+
   return true;
 }
 
+// @@ For a load/store operation op
+// @@ and and address node n
+// @@ come up with a an Offset and Opc????
 bool ARMDAGToDAGISel::SelectAddrMode2Offset(SDValue Op, SDValue N,
                                             SDValue &Offset, SDValue &Opc) {
   unsigned Opcode = Op.getOpcode();
@@ -334,6 +366,7 @@
   if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(N)) {
     int Val = (int)C->getZExtValue();
     if (Val >= 0 && Val < 0x1000) { // 12 bits.
+      // Register 0 means no offset
       Offset = CurDAG->getRegister(0, MVT::i32);
       Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(AddSub, Val,
                                                         ARM_AM::no_shift),
@@ -342,13 +375,19 @@
     }
   }
 
+
+  const bool is_store = (Opcode == ISD::STORE); // @LOCALMOD
+
   Offset = N;
   ARM_AM::ShiftOpc ShOpcVal = ARM_AM::getShiftOpcForNode(N);
   unsigned ShAmt = 0;
+  // @@ CONVOLUTED LOGIC BELOW: REWRITE
   if (ShOpcVal != ARM_AM::no_shift) {
     // Check to see if the RHS of the shift is a constant, if not, we can't fold
     // it.
-    if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+    //if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+    ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1));
+    if ((!FlagSfiStore || !is_store) && Sh ) { // @LOCALMOD
       ShAmt = Sh->getZExtValue();
       Offset = N.getOperand(0);
     } else {
@@ -361,11 +400,17 @@
   return true;
 }
 
-
+// @@ Op: load store
+// @@N: node for computing the address
+//
+// @@ BASE, offset, Opc represent the new addreessing mode
 bool ARMDAGToDAGISel::SelectAddrMode3(SDValue Op, SDValue N,
                                       SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
-  if (N.getOpcode() == ISD::SUB) {
+
+  const bool is_store = (Op.getOpcode() == ISD::STORE); // @LOCALMOD
+
+  if ((!FlagSfiStore ||!is_store) && N.getOpcode() == ISD::SUB) {  // @LOCALMOD
     // X - C  is canonicalize to X + -C, no need to handle it here.
     Base = N.getOperand(0);
     Offset = N.getOperand(1);
@@ -405,6 +450,17 @@
       return true;
     }
   }
+
+
+  // @LOCALMOD-START
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM3Opc(ARM_AM::add, 0),MVT::i32);
+    return true;
+  }
+  // @LOCALMOD-END
+
 
   Base = N.getOperand(0);
   Offset = N.getOperand(1);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
@@ -42,9 +42,14 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include <sstream>
+
 using namespace llvm;
 
+#include "llvm/Support/CommandLine.h"
+
+
 static bool CC_ARM_APCS_Custom_f64(unsigned &ValNo, EVT &ValVT, EVT &LocVT,
+
                                    CCValAssign::LocInfo &LocInfo,
                                    ISD::ArgFlagsTy &ArgFlags,
                                    CCState &State);
@@ -61,8 +66,18 @@
                                        ISD::ArgFlagsTy &ArgFlags,
                                        CCState &State);
 
+
+// @LOCALMOD-START
+extern cl::opt<bool> FlagSfiStore;
+
+cl::opt<bool> FlagAeabiCalls("aeabi-calls",
+                        cl::desc("use AEABI calls"));
+// @LOCALMOD-END
+
+
 void ARMTargetLowering::addTypeForNEON(EVT VT, EVT PromotedLdStVT,
                                        EVT PromotedBitwiseVT) {
+
   if (VT != PromotedLdStVT) {
     setOperationAction(ISD::LOAD, VT.getSimpleVT(), Promote);
     AddPromotedToType (ISD::LOAD, VT.getSimpleVT(),
@@ -135,6 +150,41 @@
 ARMTargetLowering::ARMTargetLowering(TargetMachine &TM)
     : TargetLowering(TM, createTLOF(TM)) {
   Subtarget = &TM.getSubtarget<ARMSubtarget>();
+
+  // @LOCALMOD-START
+  // NOTE: THIS IS INCOMPLETE: IT LACKS COMPARISON, ETC
+  //  NOTE: also check thread stuff
+  //from:  http://gcc.gnu.org/ml/gcc-patches/2008-03/msg00141.html
+
+  if (FlagAeabiCalls) {
+
+    // Single-precision floating-point arithmetic.
+    setLibcallName(RTLIB::ADD_F32, "__aeabi_fadd");
+    setLibcallName(RTLIB::SUB_F32, "__aeabi_fsub");
+    setLibcallName(RTLIB::MUL_F32, "__aeabi_fmul");
+    setLibcallName(RTLIB::DIV_F32, "__aeabi_fdiv");
+
+    // Double-precision floating-point arithmetic.
+    setLibcallName(RTLIB::ADD_F64, "__aeabi_dadd");
+    setLibcallName(RTLIB::SUB_F64, "__aeabi_dsub");
+    setLibcallName(RTLIB::MUL_F64, "__aeabi_dmul");
+    setLibcallName(RTLIB::DIV_F64, "__aeabi_ddiv");
+
+
+    setLibcallName(RTLIB::FPTOSINT_F64_I32, "__aeabi_d2iz");
+    setLibcallName(RTLIB::FPTOUINT_F64_I32, "__aeabi_d2uiz");
+    setLibcallName(RTLIB::FPTOSINT_F32_I32, "__aeabi_f2iz");
+    setLibcallName(RTLIB::FPTOUINT_F32_I32, "__aeabi_f2uiz");
+
+    setLibcallName(RTLIB::FPROUND_F64_F32, "__aeabi_d2f");
+    setLibcallName(RTLIB::FPEXT_F32_F64,   "__aeabi_f2d");
+
+    setLibcallName(RTLIB::SINTTOFP_I32_F64, "__aeabi_i2d");
+    setLibcallName(RTLIB::UINTTOFP_I32_F64, "__aeabi_ui2d");
+    setLibcallName(RTLIB::SINTTOFP_I32_F32, "__aeabi_i2f");
+    setLibcallName(RTLIB::UINTTOFP_I32_F32, "__aeabi_ui2f");
+  }
+  // @LOCALMOD-END
 
   if (Subtarget->isTargetDarwin()) {
     // Uses VFP for Thumb libfuncs if available.
@@ -364,6 +414,11 @@
   setOperationAction(ISD::GLOBAL_OFFSET_TABLE, MVT::i32, Custom);
   setOperationAction(ISD::GlobalTLSAddress, MVT::i32, Custom);
   setOperationAction(ISD::BlockAddress, MVT::i32, Custom);
+  // @LOCALMOD-START
+  if (!Subtarget->useInlineJumpTables())
+    setOperationAction(ISD::JumpTable,     MVT::i32,   Custom);
+  // @LOCALMOD-END
+
 
   // Use the default implementation.
   setOperationAction(ISD::VASTART,            MVT::Other, Custom);
@@ -409,8 +464,11 @@
   setOperationAction(ISD::BR_CC,     MVT::i32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f64,   Custom);
-  setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
-
+  // @LOCALMOD-START
+  setOperationAction(ISD::BR_JT,     MVT::Other,
+		     Subtarget->useInlineJumpTables() ? Custom : Expand);
+  // @ORIGINAL setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
+  // @LOCALMOD-END
   // We don't support sin/cos/fmod/copysign/pow
   setOperationAction(ISD::FSIN,      MVT::f64, Expand);
   setOperationAction(ISD::FSIN,      MVT::f32, Expand);
@@ -1239,6 +1297,24 @@
   return DAG.getNode(ARMISD::PIC_ADD, DL, PtrVT, Result, PICLabel);
 }
 
+// @LOCALMOD-START
+SDValue ARMTargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) {
+  assert(!Subtarget->useInlineJumpTables() &&
+	 "inline jump tables not custom lowered");
+  const MVT PTy = getPointerTy();
+  const JumpTableSDNode *JT = cast<JumpTableSDNode>(Op);
+  const SDValue JTI = DAG.getTargetJumpTable(JT->getIndex(), PTy);
+  const DebugLoc dl = Op.getDebugLoc();
+
+  ARMConstantPoolValue *CPV = new ARMConstantPoolValue(*DAG.getContext(),
+						       JT->getIndex());
+  // TODO: factor this idiom to load a value from a CP into a new function
+  const SDValue PoolEntry = DAG.getTargetConstantPool(CPV, PTy, 4);
+  const SDValue PoolWrapper = DAG.getNode(ARMISD::Wrapper, dl, PTy, PoolEntry);
+  return DAG.getLoad(PTy, dl, DAG.getEntryNode(), PoolWrapper, NULL, 0);
+}
+// @LOCALMOD-END
+
 // Lower ISD::GlobalTLSAddress using the "general dynamic" model
 SDValue
 ARMTargetLowering::LowerToTLSGeneralDynamicModel(GlobalAddressSDNode *GA,
@@ -1360,6 +1436,8 @@
                            PseudoSourceValue::getGOT(), 0);
     return Result;
   } else {
+    // The address will be stored in the constant pool, so
+    // put it in the pool and load it from there to materialize it
     SDValue CPAddr = DAG.getTargetConstantPool(GV, PtrVT, 4);
     CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
     return DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), CPAddr,
@@ -1874,7 +1952,30 @@
   return Res;
 }
 
+
 SDValue ARMTargetLowering::LowerBR_JT(SDValue Op, SelectionDAG &DAG) {
+
+  //  The Jumptable idiom we are aiming for looks somthing like this:
+  //
+  //          .set PCRELV0, (.LJTI9_0_0-(.LPCRELL0+8))
+  //  .LPCRELL0:
+  //          add r3, pc, #PCRELV0
+  //          ldr pc, [r3, +r0, lsl #2]
+  //  .LJTI9_0_0:
+  //          .long    .LBB9_2
+  //          .long    .LBB9_5
+  //          .long    .LBB9_7
+  //          .long    .LBB9_4
+  //          .long    .LBB9_8
+  //
+  // In pic mode the table entries are relative to table beginning
+  // requiring and extra addition
+  //
+  // The code generation logic for ARMISD::BR_JT will also
+  // emit the table (c.f. ARMAsmPrinter::printJTBlockOperand())
+  // Also check  "def BR_JTm" in ARMInstrInfo.td
+
+  // allocate constant pool entry
   SDValue Chain = Op.getOperand(0);
   SDValue Table = Op.getOperand(1);
   SDValue Index = Op.getOperand(2);
@@ -2873,6 +2974,7 @@
   default: llvm_unreachable("Don't know how to custom lower this!");
   case ISD::ConstantPool:  return LowerConstantPool(Op, DAG);
   case ISD::BlockAddress:  return LowerBlockAddress(Op, DAG);
+  case ISD::JumpTable:     return LowerJumpTable(Op, DAG); // @LOCALMOD
   case ISD::GlobalAddress:
     return Subtarget->isTargetDarwin() ? LowerGlobalAddressDarwin(Op, DAG) :
       LowerGlobalAddressELF(Op, DAG);
@@ -3642,6 +3744,8 @@
 /// by AM is legal for this target, for a load/store of the specified type.
 bool ARMTargetLowering::isLegalAddressingMode(const AddrMode &AM,
                                               const Type *Ty) const {
+  // @DEBUG
+  //cout << "@@CHECK ADDRESSING MODE" << "\n";
   EVT VT = getValueType(Ty, true);
   if (!isLegalAddressImmediate(AM.BaseOffs, VT, Subtarget))
     return false;
@@ -3669,6 +3773,12 @@
       return isLegalT2ScaledAddressingMode(AM, VT);
 
     int Scale = AM.Scale;
+
+    // @LOCAMOD-START
+    // For simplicity do not allow scaling
+    if (FlagSfiStore && Scale != 0) return false;
+    // @LOCAMOD-END
+
     switch (VT.getSimpleVT().SimpleTy) {
     default: return false;
     case MVT::i1:
@@ -3806,6 +3916,14 @@
   if (Subtarget->isThumb1Only())
     return false;
 
+  // @LOCAMOD-START
+  // NOTE: disallow ...
+  // NOTE: THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCAMOD-END
+
   EVT VT;
   SDValue Ptr;
   bool isSEXTLoad = false;
@@ -3844,6 +3962,13 @@
                                                    SelectionDAG &DAG) const {
   if (Subtarget->isThumb1Only())
     return false;
+
+  // @LOCALMOD-START
+  // THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCALMOD-END
 
   EVT VT;
   SDValue Ptr;
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelLowering.h
--- a/llvm-trunk/lib/Target/ARM/ARMISelLowering.h
+++ b/llvm-trunk/lib/Target/ARM/ARMISelLowering.h
@@ -274,6 +274,7 @@
                              ISD::ArgFlagsTy Flags);
     SDValue LowerINTRINSIC_W_CHAIN(SDValue Op, SelectionDAG &DAG);
     SDValue LowerINTRINSIC_WO_CHAIN(SDValue Op, SelectionDAG &DAG);
+    SDValue LowerJumpTable(SDValue Op, SelectionDAG &DAG); // @LOCALMOD
     SDValue LowerBlockAddress(SDValue Op, SelectionDAG &DAG);
     SDValue LowerGlobalAddressDarwin(SDValue Op, SelectionDAG &DAG);
     SDValue LowerGlobalAddressELF(SDValue Op, SelectionDAG &DAG);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
@@ -22,7 +22,22 @@
 #include "llvm/CodeGen/MachineInstrBuilder.h"
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
 using namespace llvm;
+
+
+// @LOCALMOD-START
+cl::opt<bool> FlagSfiStore("sfi-store",
+                       cl::desc("SFI store"));
+
+cl::opt<bool> FlagSfiStack("sfi-stack",
+                       cl::desc("SFI stack"));
+
+cl::opt<bool> FlagSfiBranch("sfi-branch",
+                        cl::desc("SFI branch"));
+
+// @LOCALMOD-END
+
 
 ARMInstrInfo::ARMInstrInfo(const ARMSubtarget &STI)
   : ARMBaseInstrInfo(STI), RI(*this, STI) {
@@ -102,4 +117,3 @@
 
   return ARMBaseInstrInfo::reMaterialize(MBB, I, DestReg, SubIdx, Orig);
 }
-
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
--- a/llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
@@ -321,6 +321,11 @@
   let MIOperandInfo = (ops GPR, i32imm);
 }
 
+
+// ======================================================================
+
+
+
 // addrmode4 := reg, <mode|W>
 //
 def addrmode4 : Operand<i32>,
@@ -568,6 +573,17 @@
            [(ARMcallseq_start timm:$amt)]>;
 }
 
+def SFISTRMASK :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary,
+           "sfi_store_preamble $dst, $p",
+           []>;
+
+let Defs = [CPSR] in
+def SFISTRTST :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a), NoItinerary,
+           "sfi_cstore_preamble $dst",
+           []>;
+
 def DWARF_LOC :
 PseudoInst<(outs), (ins i32imm:$line, i32imm:$col, i32imm:$file), NoItinerary,
            ".loc $file, $line, $col",
@@ -645,8 +661,12 @@
 //
 
 let isReturn = 1, isTerminator = 1, isBarrier = 1 in
+// @LOCALMOD-START
   def BX_RET : AI<(outs), (ins), BrMiscFrm, IIC_Br, 
-                  "bx", "\tlr", [(ARMretflag)]> {
+//                  "bx", "\tlr", [(ARMretflag)]> {
+                  "sfi_bx", "\tlr", [(ARMretflag)]> {
+
+// @LOCALMOD-END
   let Inst{7-4}   = 0b0001;
   let Inst{19-8}  = 0b111111111111;
   let Inst{27-20} = 0b00010010;
@@ -654,7 +674,11 @@
 
 // Indirect branches
 let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1 in {
-  def BRIND : AXI<(outs), (ins GPR:$dst), BrMiscFrm, IIC_Br, "bx\t$dst",
+    // @LOCALMOD-START
+    //  def BRIND : AXI<(outs), (ins GPR:$dst), BrMiscFrm, IIC_Br, "bx\t$dst",
+    def BRIND : AXI<(outs), (ins GPR:$dst), BrMiscFrm, IIC_Br,
+    "sfi_indirect_jump_preamble $dst\n\tbx\t$dst",
+    // @LOCALMOD-END
                   [(brind GPR:$dst)]> {
     let Inst{7-4}   = 0b0001;
     let Inst{19-8}  = 0b111111111111;
@@ -678,20 +702,29 @@
           D16, D17, D18, D19, D20, D21, D22, D23,
           D24, D25, D26, D27, D28, D29, D30, D31, CPSR, FPSCR] in {
   def BL  : ABXI<0b1011, (outs), (ins i32imm:$func, variable_ops),
-                IIC_Br, "bl\t${func:call}",
+// @LOCALMOD-START
+//                IIC_Br, "bl\t${func:call}",
+                IIC_Br, "sfi_call_preamble\n\tbl\t${func:call}",
+// @LOCALMOD-END
                 [(ARMcall tglobaladdr:$func)]>,
             Requires<[IsARM, IsNotDarwin]> {
     let Inst{31-28} = 0b1110;
   }
 
   def BL_pred : ABI<0b1011, (outs), (ins i32imm:$func, variable_ops),
-                   IIC_Br, "bl", "\t${func:call}",
+// @LOCALMOD-START
+//                   IIC_Br, "bl", "\t${func:call}",
+                   IIC_Br, "sfi_call_preamble\n\tbl", "\t${func:call}",
+// @LOCALMOD-END
                    [(ARMcall_pred tglobaladdr:$func)]>,
                 Requires<[IsARM, IsNotDarwin]>;
 
   // ARMv5T and above
   def BLX : AXI<(outs), (ins GPR:$func, variable_ops), BrMiscFrm,
-                IIC_Br, "blx\t$func",
+// @LOCALMOD-START
+//                IIC_Br, "blx\t$func",
+                IIC_Br, "sfi_indirect_call_preamble $func\n\tblx\t$func",
+// @LOCALMOD-END
                 [(ARMcall GPR:$func)]>,
             Requires<[IsARM, HasV5T, IsNotDarwin]> {
     let Inst{7-4}   = 0b0011;
@@ -701,7 +734,11 @@
 
   // ARMv4T
   def BX : ABXIx2<(outs), (ins GPR:$func, variable_ops),
-                  IIC_Br, "mov\tlr, pc\n\tbx\t$func",
+// @LOCALMOD-START
+//                  IIC_Br, "mov\tlr, pc\n\tbx\t$func",
+// TODO(robertm): this does not quite work
+                  IIC_Br, "sfi_call_preamble\n\tmov\tlr, pc\n\tbx\t$func",
+// @LOCALMOD-END
                   [(ARMcall_nolink GPR:$func)]>,
            Requires<[IsARM, IsNotDarwin]> {
     let Inst{7-4}   = 0b0001;
@@ -717,19 +754,28 @@
           D16, D17, D18, D19, D20, D21, D22, D23,
           D24, D25, D26, D27, D28, D29, D30, D31, CPSR, FPSCR] in {
   def BLr9  : ABXI<0b1011, (outs), (ins i32imm:$func, variable_ops),
-                IIC_Br, "bl\t${func:call}",
+// @LOCALMOD-START
+//                IIC_Br, "bl\t${func:call}",
+                IIC_Br, "sfi_call_preamble\n\tbl\t${func:call}",
+// @LOCALMOD-END
                 [(ARMcall tglobaladdr:$func)]>, Requires<[IsARM, IsDarwin]> {
     let Inst{31-28} = 0b1110;
   }
 
   def BLr9_pred : ABI<0b1011, (outs), (ins i32imm:$func, variable_ops),
-                   IIC_Br, "bl", "\t${func:call}",
+// @LOCALMOD-START
+//                   IIC_Br, "bl", "\t${func:call}",
+                   IIC_Br, "sfi_call_preamble\n\tbl", "\t${func:call}",
+// @LOCALMOD-END
                    [(ARMcall_pred tglobaladdr:$func)]>,
                   Requires<[IsARM, IsDarwin]>;
 
   // ARMv5T and above
   def BLXr9 : AXI<(outs), (ins GPR:$func, variable_ops), BrMiscFrm,
-                IIC_Br, "blx\t$func",
+// @LOCALMOD-START
+//                IIC_Br, "blx\t$func",
+                IIC_Br, "sfi_indirect_call_preamble $func\n\tblx\t$func",
+// @LOCALMOD-END
                 [(ARMcall GPR:$func)]>, Requires<[IsARM, HasV5T, IsDarwin]> {
     let Inst{7-4}   = 0b0011;
     let Inst{19-8}  = 0b111111111111;
@@ -738,7 +784,11 @@
 
   // ARMv4T
   def BXr9 : ABXIx2<(outs), (ins GPR:$func, variable_ops),
-                  IIC_Br, "mov\tlr, pc\n\tbx\t$func",
+// @LOCALMOD-START
+//                  IIC_Br, "mov\tlr, pc\n\tbx\t$func",
+// TODO(robertm): this does not quite work
+                  IIC_Br, "sfi_call_preamble\n\tmov\tlr, pc\n\tbx\t$func",
+// @LOCALMOD-END
                   [(ARMcall_nolink GPR:$func)]>, Requires<[IsARM, IsDarwin]> {
     let Inst{7-4}   = 0b0001;
     let Inst{19-8}  = 0b111111111111;
@@ -874,6 +924,7 @@
 
 // Store
 def STR  : AI2stw<(outs), (ins GPR:$src, addrmode2:$addr), StFrm, IIC_iStorer,
+// c.f.:  ARMAsmPrinter::printAddrMode2Operand
                "str", "\t$src, $addr",
                [(store GPR:$src, addrmode2:$addr)]>;
 
@@ -948,7 +999,11 @@
 let mayStore = 1, hasExtraSrcRegAllocReq = 1 in
 def STM : AXI4st<(outs),
                (ins addrmode4:$addr, pred:$p, reglist:$wb, variable_ops),
-               LdStMulFrm, IIC_iStorem, "stm${addr:submode}${p}\t$addr, $wb",
+// @LOCALMOD-START
+// c.f. ARMAsmPrinter::printAddrMode4Operand()
+//               LdStMulFrm, IIC_iStorem, "stm${addr:submode}${p}\t$addr, $wb",
+               LdStMulFrm, IIC_iStorem, "sfi_store_preamble ${addr:base}, ${p}\n\tstm${addr:submode}${p}\t$addr, $wb",
+// @LOCALMOD-END
                []>;
 
 //===----------------------------------------------------------------------===//
@@ -1087,6 +1142,17 @@
 defm SUB  : AsI1_bin_irs<0b0010, "sub",
                          BinOpFrag<(sub  node:$LHS, node:$RHS)>>;
 
+
+// @LOCALMOD-START
+defm STACK_ADD  : AsI1_bin_irs<0b0100, "sfi_add",
+                         BinOpFrag<(add  node:$LHS, node:$RHS)>, 1>;
+defm STACK_SUB  : AsI1_bin_irs<0b0010, "sfi_sub",
+                         BinOpFrag<(sub  node:$LHS, node:$RHS)>>;
+let neverHasSideEffects = 1 in
+def STACK_MOVr : AsI1<0b1101, (outs GPR:$dst), (ins GPR:$src), DPFrm, IIC_iMOVr,
+                 "sfi_mov", " $dst, $src", []>, UnaryDP;
+// @LOCALMOD-END
+
 // ADD and SUB with 's' bit set.
 defm ADDS : AI1_bin_s_irs<0b0100, "adds",
                           BinOpFrag<(addc node:$LHS, node:$RHS)>, 1>;
@@ -1551,7 +1617,10 @@
 let isCall = 1,
   Defs = [R0, R12, LR, CPSR] in {
   def TPsoft : ABXI<0b1011, (outs), (ins), IIC_Br,
-               "bl\t__aeabi_read_tp",
+// @LOCALMOD-START
+//               "bl\t__aeabi_read_tp",
+               "sfi_call_preamble\n\tbl\t__aeabi_read_tp",
+// @LOCALMOD-END
                [(set R0, ARMthread_pointer)]>;
 }
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
--- a/llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
@@ -91,6 +91,7 @@
   let Inst{20} = 1;
 }
 } // mayLoad, hasExtraDefRegAllocReq
+
 
 let mayStore = 1, hasExtraSrcRegAllocReq = 1 in {
 def VSTMD : AXDI5<(outs), (ins addrmode5:$addr, pred:$p, reglist:$wb,
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
@@ -38,6 +38,9 @@
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
 using namespace llvm;
+
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
 
 STATISTIC(NumLDMGened , "Number of ldm instructions generated");
 STATISTIC(NumSTMGened , "Number of stm instructions generated");
@@ -320,10 +323,12 @@
   unsigned MyPredReg = 0;
   if (!MI)
     return false;
+
   if (MI->getOpcode() != ARM::t2SUBri &&
       MI->getOpcode() != ARM::t2SUBrSPi &&
       MI->getOpcode() != ARM::t2SUBrSPi12 &&
       MI->getOpcode() != ARM::tSUBspi &&
+      MI->getOpcode() != ARM::STACK_SUBri && // @LOCALMOD
       MI->getOpcode() != ARM::SUBri)
     return false;
 
@@ -345,10 +350,12 @@
   unsigned MyPredReg = 0;
   if (!MI)
     return false;
+
   if (MI->getOpcode() != ARM::t2ADDri &&
       MI->getOpcode() != ARM::t2ADDrSPi &&
       MI->getOpcode() != ARM::t2ADDrSPi12 &&
       MI->getOpcode() != ARM::tADDspi &&
+      MI->getOpcode() != ARM::STACK_ADDri && // @LOCALMOD
       MI->getOpcode() != ARM::ADDri)
     return false;
 
@@ -404,6 +411,7 @@
 /// ldmia rn, <ra, rb, rc>
 /// =>
 /// ldmdb rn!, <ra, rb, rc>
+/// @LOCALMOD This is especially useful for rn == sp
 bool ARMLoadStoreOpt::MergeBaseUpdateLSMultiple(MachineBasicBlock &MBB,
                                                MachineBasicBlock::iterator MBBI,
                                                bool &Advance,
@@ -1040,6 +1048,7 @@
   return NumMerges > 0;
 }
 
+
 namespace {
   struct OffsetCompare {
     bool operator()(const MachineInstr *LHS, const MachineInstr *RHS) const {
@@ -1058,7 +1067,12 @@
 ///   bx lr
 /// =>
 ///   ldmfd sp!, {r7, pc}
+// @LOCALMOD for sfi we do not want this to happen
 bool ARMLoadStoreOpt::MergeReturnIntoLDM(MachineBasicBlock &MBB) {
+// @LOCALMOD-START
+  return false;
+  // @LOCALMOD-END
+
   if (MBB.empty()) return false;
 
   MachineBasicBlock::iterator MBBI = prior(MBB.end());
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSFIPlacement.cpp
--- /dev/null
+++ b/llvm-trunk/lib/Target/ARM/ARMSFIPlacement.cpp
@@ -0,0 +1,252 @@
+//===-- ARMSFIPlacement.cpp - Place SFI mask instructions ---------*- C++ -*-=//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a pass that places mask instructions ahead of all stores.
+// This must be run as late in the game as possible -- after all scheduling and
+// constant island placement.  (This is set up in ARMTargetMachine.cpp.)
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "arm-pseudo"
+#include "ARM.h"
+#include "ARMBaseInstrInfo.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+
+#include <set>
+#include <stdio.h>
+
+using namespace llvm;
+
+namespace {
+  class ARMSFIPlacement : public MachineFunctionPass {
+  public:
+    static char ID;
+    ARMSFIPlacement() : MachineFunctionPass(&ID) {}
+
+    const TargetInstrInfo *TII;
+
+    virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+    virtual bool runOnMachineFunction(MachineFunction &Fn);
+
+    virtual const char *getPassName() const {
+      return "ARM SFI mask placement";
+    }
+
+  private:
+    bool PlaceMBB(MachineBasicBlock &MBB);
+    void SandboxStore(MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator MBBI,
+                      MachineInstr &MI,
+                      int AddrIdx,
+                      bool CPSRLive);
+    bool TryPredicating(MachineInstr &MI, ARMCC::CondCodes);
+  };
+  char ARMSFIPlacement::ID = 0;
+}
+
+static ARMCC::CondCodes GetPredicate(MachineInstr &MI) {
+  int PIdx = MI.findFirstPredOperandIdx();
+  if (PIdx != -1) {
+    return (ARMCC::CondCodes)MI.getOperand(PIdx).getImm();
+  } else {
+    return ARMCC::AL;
+  }
+}
+
+void ARMSFIPlacement::getAnalysisUsage(AnalysisUsage &AU) const {
+  // Slight (possibly unnecessary) efficiency tweak:
+  // Promise not to modify the CFG.
+  AU.setPreservesCFG();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool ARMSFIPlacement::TryPredicating(MachineInstr &MI, ARMCC::CondCodes Pred) {
+  // Can't predicate if it's already predicated.
+  // TODO(cbiffle): actually we can, if the conditions match.
+  if (TII->isPredicated(&MI)) return false;
+
+  /*
+   * ARM predicate operands use two actual MachineOperands: an immediate
+   * holding the predicate condition, and a register referencing the flags.
+   */
+  SmallVector<MachineOperand, 2> PredOperands;
+  PredOperands.push_back(MachineOperand::CreateImm((int64_t) Pred));
+  PredOperands.push_back(MachineOperand::CreateReg(ARM::CPSR, false));
+
+  // This attempts to rewrite, but some instructions can't be predicated.
+  return TII->PredicateInstruction(&MI, PredOperands);
+}
+
+/*
+ * Sandboxes a store instruction by inserting an appropriate mask or check
+ * operation before it.
+ */
+void ARMSFIPlacement::SandboxStore(MachineBasicBlock &MBB,
+                                   MachineBasicBlock::iterator MBBI,
+                                   MachineInstr &MI,
+                                   int AddrIdx,
+                                   bool CPSRLive) {
+  MachineOperand &Addr = MI.getOperand(AddrIdx);
+
+  if (!CPSRLive && TryPredicating(MI, ARMCC::EQ)) {
+    /*
+     * For unconditional stores where CPSR is not in use, we can use a faster
+     * sandboxing sequence by predicating the store -- assuming we *can*
+     * predicate the store.
+     */
+
+    // Instruction can be predicated -- use the new sandbox.
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFISTRTST))
+      .addOperand(Addr)   // rD
+      .addReg(0);         // apparently unused source register?
+  } else {
+    // Use the older BIC sandbox, which is universal, but incurs a stall.
+    ARMCC::CondCodes Pred = GetPredicate(MI);
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFISTRMASK))
+      .addOperand(Addr)        // rD
+      .addReg(0)               // apparently unused source register?
+      .addImm((int64_t) Pred)  // predicate condition
+      .addReg(ARM::CPSR);      // predicate source register (CPSR)
+
+    /*
+     * This pseudo-instruction is intended to generate something resembling the
+     * following, but with alignment enforced.
+     * TODO(cbiffle): move alignment into this function, use the code below.
+     *
+     *  // bic<cc> Addr, Addr, #0xC0000000
+     *  BuildMI(MBB, MBBI, MI.getDebugLoc(),
+     *          TII->get(ARM::BICri))
+     *    .addOperand(Addr)        // rD
+     *    .addOperand(Addr)        // rN
+     *    .addImm(0xC0000000)      // imm
+     *    .addImm((int64_t) Pred)  // predicate condition
+     *    .addReg(ARM::CPSR)       // predicate source register (CPSR)
+     *    .addReg(0);              // flag output register (0 == no flags)
+     */
+  }
+}
+
+static bool IsDangerousStore(const MachineInstr &MI, int *AddrIdx) {
+  unsigned Opcode = MI.getOpcode();
+  switch (Opcode) {
+  default: return false;
+
+  // Instructions with base address register in position 0...
+  case ARM::VSTMD:
+  case ARM::VSTMS:
+    *AddrIdx = 0;
+    break;
+
+  // Instructions with base address register in position 1...
+  case ARM::STR:
+  case ARM::STRB:
+  case ARM::STRH:
+  case ARM::VSTRS:
+  case ARM::VSTRD:
+    *AddrIdx = 1;
+    break;
+
+  // Instructions with base address register in position 2...
+  case ARM::STR_PRE:
+  case ARM::STR_POST:
+  case ARM::STRB_PRE:
+  case ARM::STRB_POST:
+  case ARM::STRH_PRE:
+  case ARM::STRH_POST:
+  case ARM::STRD:
+    *AddrIdx = 2;
+    break;
+  }
+
+  if (MI.getOperand(*AddrIdx).getReg() == ARM::SP) {
+    // The contents of SP do not require masking.
+    return false;
+  }
+
+  return true;
+}
+
+static bool IsCPSRLiveOut(const MachineBasicBlock &MBB) {
+  // CPSR is live-out if any successor lists it as live-in.
+  for (MachineBasicBlock::const_succ_iterator SI = MBB.succ_begin(),
+                                              E = MBB.succ_end();
+       SI != E;
+       ++SI) {
+    const MachineBasicBlock *Succ = *SI;
+    if (Succ->isLiveIn(ARM::CPSR)) return true;
+  }
+  return false;
+}
+
+bool ARMSFIPlacement::PlaceMBB(MachineBasicBlock &MBB) {
+  /*
+   * This is a simple local reverse-dataflow analysis to determine where CPSR
+   * is live.  We cannot use the conditional store sequence anywhere that CPSR
+   * is live, or we'd affect correctness.  The existing liveness analysis passes
+   * barf when applied pre-emit, after allocation, so we must do it ourselves.
+   */ 
+
+  bool CPSRLive = IsCPSRLiveOut(MBB);
+
+  // Given that, record which instructions should not be altered to trash CPSR:
+  std::set<const MachineInstr *> InstrsWhereCPSRLives;
+  for (MachineBasicBlock::const_reverse_iterator MBBI = MBB.rbegin(),
+                                                 E = MBB.rend();
+       MBBI != E;
+       ++MBBI) {
+    const MachineInstr &MI = *MBBI;
+    // Check for kills first.
+    if (MI.modifiesRegister(ARM::CPSR)) CPSRLive = false;
+    // Then check for uses.
+    if (MI.readsRegister(ARM::CPSR)) CPSRLive = true;
+
+    if (CPSRLive) InstrsWhereCPSRLives.insert(&MI);
+  }
+
+  // Sanity check:
+  assert(CPSRLive == MBB.isLiveIn(ARM::CPSR)
+         && "CPSR Liveness analysis does not match cached live-in result.");
+
+  // Now: find and sandbox stores.
+  bool Modified = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+
+    int AddrIdx;
+    if (IsDangerousStore(MI, &AddrIdx)) {
+      bool CPSRLive =
+          (InstrsWhereCPSRLives.find(&MI) != InstrsWhereCPSRLives.end());
+      SandboxStore(MBB, MBBI, MI, AddrIdx, CPSRLive);
+      Modified = true;
+    }
+  }
+
+  return Modified;
+}
+
+bool ARMSFIPlacement::runOnMachineFunction(MachineFunction &MF) {
+  TII = MF.getTarget().getInstrInfo();
+
+  bool Modified = false;
+  for (MachineFunction::iterator MFI = MF.begin(), E = MF.end();
+       MFI != E;
+       ++MFI)
+    Modified |= PlaceMBB(*MFI);
+  return Modified;
+}
+
+/// createARMSFIPlacementPass - returns an instance of the SFI placement pass.
+FunctionPass *llvm::createARMSFIPlacementPass() {
+  return new ARMSFIPlacement();
+}
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
@@ -27,6 +27,17 @@
           cl::desc("Use NEON for single-precision FP"),
           cl::init(false), cl::Hidden);
 
+// @LOCALMOD-START
+// TODO: * This does not currently work as expected for PIC mode:
+//         It does work, but the table still ends up in the .text section.
+//       * JITing has not been tested at all
+//       * Thumb mode operation is also not clear: it seems jump tables
+//         for thumb are broken independent of this option
+static cl::opt<bool>
+NoInlineJumpTables("no-inline-jumptables",
+		  cl::desc("Do not place jump tables inline in the code"));
+// @LOCALMOD-END
+
 ARMSubtarget::ARMSubtarget(const std::string &TT, const std::string &FS,
                            bool isT)
   : ARMArchVersion(V4T)
@@ -36,6 +47,7 @@
   , ThumbMode(Thumb1)
   , PostRAScheduler(false)
   , IsR9Reserved(ReserveR9)
+  , UseInlineJumpTables(!NoInlineJumpTables) // @LOCAMOD
   , stackAlignment(4)
   , CPUString("generic")
   , TargetType(isELF) // Default to ELF unless otherwise specified.
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSubtarget.h
--- a/llvm-trunk/lib/Target/ARM/ARMSubtarget.h
+++ b/llvm-trunk/lib/Target/ARM/ARMSubtarget.h
@@ -61,6 +61,11 @@
 
   /// IsR9Reserved - True if R9 is a not available as general purpose register.
   bool IsR9Reserved;
+
+  // @LOCALMOD-START
+  /// UseInlineJumpTables - True if jump tables should be in-line in the code.
+  bool UseInlineJumpTables;
+  // @LOCALMOD-END
 
   /// stackAlignment - The minimum alignment known to hold of the stack frame on
   /// entry to the function and which must be maintained by every function.
@@ -144,6 +149,9 @@
   /// GVIsIndirectSymbol - true if the GV will be accessed via an indirect
   /// symbol.
   bool GVIsIndirectSymbol(GlobalValue *GV, Reloc::Model RelocM) const;
+
+  // @LOCALMOD
+  bool useInlineJumpTables() const {return UseInlineJumpTables;}
 };
 } // End llvm namespace
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
@@ -19,7 +19,11 @@
 #include "llvm/Support/FormattedStream.h"
 #include "llvm/Target/TargetOptions.h"
 #include "llvm/Target/TargetRegistry.h"
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
 using namespace llvm;
+
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
+
 
 static const MCAsmInfo *createMCAsmInfo(const Target &T, StringRef TT) {
   Triple TheTriple(TT);
@@ -128,6 +132,11 @@
   }
 
   PM.add(createARMConstantIslandPass());
+
+  if (FlagSfiStore) {
+    PM.add(createARMSFIPlacementPass());
+  }
+
   return true;
 }
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
@@ -51,7 +51,13 @@
 #include "llvm/Support/Mangler.h"
 #include "llvm/Support/MathExtras.h"
 #include <cctype>
+#include <sstream>
+
 using namespace llvm;
+
+// @LOCALMOD
+cl::opt<bool> FlagSfiZeroMask("sfi-zero-mask");
+// @LOCALMOD
 
 STATISTIC(EmittedInsts, "Number of machine instrs printed");
 
@@ -94,10 +100,12 @@
 
     void printOperand(const MachineInstr *MI, int OpNum,
                       const char *Modifier = 0);
+
     void printSOImmOperand(const MachineInstr *MI, int OpNum);
     void printSOImm2PartOperand(const MachineInstr *MI, int OpNum);
     void printSORegOperand(const MachineInstr *MI, int OpNum);
     void printAddrMode2Operand(const MachineInstr *MI, int OpNum);
+
     void printAddrMode2OffsetOperand(const MachineInstr *MI, int OpNum);
     void printAddrMode3Operand(const MachineInstr *MI, int OpNum);
     void printAddrMode3OffsetOperand(const MachineInstr *MI, int OpNum);
@@ -170,6 +178,7 @@
     /// EmitMachineConstantPoolValue - Print a machine constantpool value to
     /// the .s file.
     virtual void EmitMachineConstantPoolValue(MachineConstantPoolValue *MCPV) {
+      // NOTE: A lot of this code is replicated in  ARMConstantPoolValue::print
       printDataDirective(MCPV->getType());
 
       ARMConstantPoolValue *ACPV = static_cast<ARMConstantPoolValue*>(MCPV);
@@ -180,6 +189,10 @@
         raw_svector_ostream(LSDAName) << MAI->getPrivateGlobalPrefix() <<
           "_LSDA_" << getFunctionNumber();
         Name = LSDAName.str();
+      } else if (ACPV->isJumpTable()) {
+	Name = std::string(MAI->getPrivateGlobalPrefix()) + "JTI" +
+               utostr(getFunctionNumber()) + '_' +
+               utostr(*ACPV->getJumpTableIndex());
       } else if (ACPV->isBlockAddress()) {
         Name = GetBlockAddressSymbol(ACPV->getBlockAddress())->getName();
       } else if (ACPV->isGlobalValue()) {
@@ -231,6 +244,15 @@
   };
 } // end of anonymous namespace
 
+// @LOCALMOD-START
+extern cl::opt<bool> FlagSfiStore;
+extern cl::opt<bool> FlagSfiStack;
+extern cl::opt<bool> FlagSfiBranch;
+cl::opt<bool> FlagSfiData("sfi-data",
+                        cl::desc("use illegal at data bundle beginning"));
+// @LOCALMOD-END
+
+
 #include "ARMGenAsmWriter.inc"
 
 /// runOnMachineFunction - This uses the printInstruction()
@@ -247,7 +269,6 @@
 
   // NOTE: we don't print out constant pools here, they are handled as
   // instructions.
-
   O << '\n';
 
   // Print out labels for the function.
@@ -287,7 +308,15 @@
       O << "\t" << CurrentFnName;
     O << "\n";
   } else {
-    EmitAlignment(FnAlign, F);
+    // @LOCALMOD-START
+    // EmitAlignment(FnAlign, F);
+    // make sure function entry is aligned. We use  XmagicX as our basis
+    // for alignment decisions (c.f. assembler sfi macros)
+    int alignment = MF.getAlignment();
+    if (alignment < 4) alignment = 4;
+    EmitAlignment(alignment, F);
+    O << "\t.set XmagicX, .\n";
+    // @LOCALMOD-END
   }
 
   O << CurrentFnName << ":\n";
@@ -304,6 +333,46 @@
       O << "\tnop\n";
   }
 
+  // @LOCALMOD-START
+  // Make sure all jump targets are aligned
+  // and also all constant pools
+  if (FlagSfiBranch) {
+    // JUMP TABLE TARGETS
+    MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+    const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+    for (unsigned i=0; i < JT.size(); ++i) {
+      std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+      //cout << "JUMPTABLE "<< i << " " << MBBs.size() << "\n";
+      for (unsigned j=0; j < MBBs.size(); ++j) {
+	if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+	  continue;
+	}
+        MBBs[j]->setAlignment(16);
+      }
+    }
+
+    // FIRST ENTRY IN A ConstanPool
+    bool last_bb_was_constant_pool = false;
+    for (MachineFunction::iterator I = MF.begin(), E = MF.end();
+         I != E; ++I) {
+      if (I->isLandingPad()) {
+        I->setAlignment(16);
+      }
+
+      if (I->empty()) continue;
+
+      bool is_constant_pool = I->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY;
+
+      if (last_bb_was_constant_pool != is_constant_pool) {
+        I->setAlignment(16);
+      }
+
+      last_bb_was_constant_pool = is_constant_pool;
+    }
+  }
+  // @LOCALMOD-END
+
   // Print out code for the function.
   for (MachineFunction::const_iterator I = MF.begin(), E = MF.end();
        I != E; ++I) {
@@ -319,6 +388,12 @@
 
   if (MAI->hasDotTypeDotSizeDirective())
     O << "\t.size " << CurrentFnName << ", .-" << CurrentFnName << "\n";
+
+  // @LOCALMOD-START
+  // Print out jump tables referenced by the function.
+  if (!Subtarget->useInlineJumpTables())
+    EmitJumpTableInfo(MF.getJumpTableInfo(), MF);
+  // @LOCALMOD-END
 
   // Emit post-function debug information.
   DW->EndFunction(&MF);
@@ -588,6 +663,10 @@
     ARM_AM::AMSubMode Mode = ARM_AM::getAM4SubMode(MO2.getImm());
     if (Mode == ARM_AM::ia)
       O << ".w";
+  // @LOCALMOD-START
+  } else if (Modifier && strcmp(Modifier, "base") == 0) {
+    printOperand(MI, Op);
+    // @LOCALMOD-END
   } else {
     printOperand(MI, Op);
     if (ARM_AM::getAM4WBFlag(MO2.getImm()))
@@ -618,6 +697,14 @@
       O << "!";
     return;
   }
+  //@LOCALMOD-START
+   else if (Modifier && strcmp(Modifier, "basereg") == 0) {
+     O << getRegisterName(MO1.getReg());
+     return;
+   }
+  //@LOCALMOD-END
+
+
 
   O << "[" << getRegisterName(MO1.getReg());
 
@@ -880,7 +967,25 @@
   assert(Modifier && "This operand only works with a modifier!");
   // There are two aspects to a CONSTANTPOOL_ENTRY operand, the label and the
   // data itself.
+
   if (!strcmp(Modifier, "label")) {
+    // @LOCALMOD-START
+    // NOTE: we also should make sure that the first data item
+    // is not in a code bundle
+    // NOTE: there may be issues with alignment constraints
+    const unsigned size = MI->getOperand(2).getImm();
+    //assert(size == 4 || size == 8 && "Unsupported data item size");
+    if (size == 8) {
+      // we cannot generate a size 8 constant at offset 12 (mod 16)
+      O << "sfi_nop_if_at_bundle_end\n";
+    }
+
+    if (FlagSfiData) {
+      O << "sfi_illegal_if_at_bundle_begining  @ ========== SFI (" <<
+        size << ")\n";
+    }
+    // @LOCALMOD-END
+
     unsigned ID = MI->getOperand(OpNum).getImm();
     O << MAI->getPrivateGlobalPrefix() << "CPI" << getFunctionNumber()
       << '_' << ID << ":\n";
@@ -891,6 +996,9 @@
     const MachineConstantPoolEntry &MCPE = MCP->getConstants()[CPI];
 
     if (MCPE.isMachineConstantPoolEntry()) {
+      // @LOCALMOD-START
+      // O << "@ Const pool\n";
+      // @LOCALMOD-END
       EmitMachineConstantPoolValue(MCPE.Val.MachineCPVal);
     } else {
       EmitGlobalConstant(MCPE.Val.ConstVal);
@@ -915,6 +1023,7 @@
   const std::vector<MachineBasicBlock*> &JTBBs = JT[JTI].MBBs;
   bool UseSet= MAI->getSetDirective() && TM.getRelocationModel() == Reloc::PIC_;
   SmallPtrSet<MachineBasicBlock*, 8> JTSets;
+
   for (unsigned i = 0, e = JTBBs.size(); i != e; ++i) {
     MachineBasicBlock *MBB = JTBBs[i];
     bool isNew = JTSets.insert(MBB);
@@ -1158,6 +1267,271 @@
 
     // FIXME: Should we signal R9 usage?
   }
+
+  O << " @ ========================================\n";
+  O << "@ Branch: " << FlagSfiBranch << "\n";
+  O << "@ Stack: " << FlagSfiStack << "\n";
+  O << "@ Store: " << FlagSfiStore << "\n";
+  O << "@ Data: " << FlagSfiData << "\n";
+
+  O << " @ ========================================\n";
+  // NOTE: this macro does bundle alignment as follows
+  //       if current bundle pos is X emit pX data items of value "val"
+  // NOTE: that pos will be one of: 0,4,8,12
+  //
+  O <<
+    "\t.macro sfi_long_based_on_pos p0 p1 p2 p3 val\n"
+    "\t.set pos, (. - XmagicX) % 16\n"
+    "\t.fill  (((\\p3<<12)|(\\p2<<8)|(\\p1<<4)|\\p0)>>pos) & 15, 4, \\val\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_illegal_if_at_bundle_begining\n"
+    "\tsfi_long_based_on_pos 1 0 0 0 0xe1277777\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nop_if_at_bundle_end\n"
+    "\tsfi_long_based_on_pos 0 0 0 1 0xe1a00000\n"
+    "\t.endm\n"
+      "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot3\n"
+    "\tsfi_long_based_on_pos 3 2 1 0 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot2\n"
+    "\tsfi_long_based_on_pos 2 1 0 3 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot1\n"
+    "\tsfi_long_based_on_pos 1 0 3 2 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O << " @ ========================================\n";
+
+  if (FlagSfiZeroMask) {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+  } else {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_data_tst reg\n"
+      "\ttst \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0xc000000f\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  O << " @ ========================================\n";
+  if (FlagSfiBranch) {
+    O <<
+      "\t.macro sfi_call_preamble\n"
+      "\tsfi_nops_to_force_slot3\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_return_alignment_and_code_mask reg cond=\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_code_mask \\reg \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+  } else {
+    O <<
+      "\t.macro sfi_call_preamble\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_return_alignment_and_code_mask reg cond=\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+
+  if (FlagSfiStore) {
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_mask \\reg, \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_tst \\reg\n"
+      "\t.endm\n"
+      "\n\n";
+  } else {
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  const char* kPreds[] = {
+    "eq",
+    "ne",
+    "lt",
+    "le",
+    "ls",
+    "ge",
+    "gt",
+    "hs",
+    "hi",
+    "lo",
+    "mi",
+    "pl",
+    NULL,
+  };
+
+  if (FlagSfiStack) {
+
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_add rega regb imm rot=0\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tadd \\rega, \\regb, \\imm, \\rot\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++p) {
+      O <<
+        "\t.macro sfi_add" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tadd" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+    O << " @ ========================================\n";
+    O <<
+      "\t.macro sfi_sub rega regb imm rot=0\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsub \\rega, \\regb, \\imm, \\rot\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++ p) {
+      O <<
+        "\t.macro sfi_sub" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tsub" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_mov rega regb\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tmov \\rega, \\regb\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++ p) {
+      O <<
+        "\t.macro mov_sub" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tmov" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+  } // FlagSfiStack
+
+
+  O << " @ ========================================\n";
+
+  O <<
+    "\t.macro sfi_bx link\n"
+    "\tsfi_return_alignment_and_code_mask \\link\n"
+    "\tbx \\link\n"
+    "\t.endm\n"
+    "\n\n";
+
+
+  for (int p=0; kPreds[p] != NULL; ++ p) {
+    O <<
+      "\t.macro sfi_bx" << kPreds[p] << " link\n"
+      "\tsfi_return_alignment_and_code_mask \\link " << kPreds[p] << "\n"
+      "\tbx" << kPreds[p] << " \\link\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+
+  O << " @ ========================================\n";
+
+  // This is use just before "mov pc, rx"
+  // TODO: make it possible to turn this off
+  O <<
+    "\t.macro sfi_indirect_jump_preamble link\n"
+    "\tsfi_nop_if_at_bundle_end\n"
+    "\tsfi_code_mask \\link\n"
+    "\t.endm\n"
+    "\n\n";
+
+  // This is use just before "blx rx"
+  // TODO: make it possible to turn this off
+  O <<
+    "\t.macro sfi_indirect_call_preamble link\n"
+    "\tsfi_nops_to_force_slot2\n"
+    "\tsfi_code_mask \\link\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O << " @ ========================================\n";
+  O << "\t.text\n";
+
+  // @LOCALMOD-END
+
 }
 
 void ARMAsmPrinter::PrintGlobalVariable(const GlobalVariable* GVar) {
@@ -1345,6 +1719,7 @@
     // generates code that does this, it is always safe to set.
     OutStreamer.EmitAssemblerFlag(MCStreamer::SubsectionsViaSymbols);
   }
+
 }
 
 //===----------------------------------------------------------------------===//
@@ -1519,4 +1894,3 @@
   TargetRegistry::RegisterMCInstPrinter(TheARMTarget, createARMMCInstPrinter);
   TargetRegistry::RegisterMCInstPrinter(TheThumbTarget, createARMMCInstPrinter);
 }
-
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
@@ -18,8 +18,15 @@
 #include "llvm/MC/MCInst.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCExpr.h"
+#include "llvm/Support/CommandLine.h"  // @LOCALMOD
 #include "llvm/Support/raw_ostream.h"
+
 using namespace llvm;
+
+// @LOCALMOD
+extern cl::opt<bool> FlagSfiStore;
+// @LOCALMOD
+
 
 // Include the auto-generated portion of the assembly writer.
 #define MachineInstr MCInst
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
@@ -40,8 +40,19 @@
   
   void printSORegOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode2Operand(const MCInst *MI, unsigned OpNum);
+
+  // @LOCALMOD-START
+  // NOTE: UNTESTED - but also not used
+  void printAddrModeHelper(const MCInst *MI, int base, int pred, const char* Modifier);
+  void printAddrMode2Operand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode3Operand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode2OffsetOperand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode3OffsetOperand(const MCInst *MI, unsigned OpNum, const char* cp);
+  // @LOCALMOD-END
   void printAddrMode2OffsetOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode3Operand(const MCInst *MI, unsigned OpNum);
+
+
   void printAddrMode3OffsetOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode4Operand(const MCInst *MI, unsigned OpNum,
                              const char *Modifier = 0);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/CMakeLists.txt
--- a/llvm-trunk/lib/Target/ARM/CMakeLists.txt
+++ b/llvm-trunk/lib/Target/ARM/CMakeLists.txt
@@ -25,6 +25,7 @@
   ARMLoadStoreOptimizer.cpp
   ARMMCAsmInfo.cpp
   ARMRegisterInfo.cpp
+  ARMSFIPlacement.cpp
   ARMSubtarget.cpp
   ARMTargetMachine.cpp
   NEONMoveFix.cpp
