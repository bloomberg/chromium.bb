diff -r c06933b0e3e6 llvm-trunk/Makefile.rules
--- a/llvm-trunk/Makefile.rules
+++ b/llvm-trunk/Makefile.rules
@@ -569,8 +569,12 @@
 endif
 
 ifdef SHARED_LIBRARY
+ifneq ($(HOST_OS),Darwin)
+  LD.Flags += $(RPATH) -Wl,'$$ORIGIN'
+else
 ifneq ($(DARWIN_MAJVERS),4)
   LD.Flags += $(RPATH) -Wl,$(LibDir)
+endif
 endif
 endif
 
@@ -1062,7 +1066,12 @@
 uninstall-local::
 	$(Echo) Uninstall circumvented with NO_INSTALL
 else
+
+ifdef LOADABLE_MODULE
+DestSharedLib = $(PROJ_libdir)/$(LIBRARYNAME)$(SHLIBEXT)
+else
 DestSharedLib = $(PROJ_libdir)/lib$(LIBRARYNAME)$(SHLIBEXT)
+endif
 
 install-local:: $(DestSharedLib)
 
diff -r c06933b0e3e6 llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
--- a/llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
+++ b/llvm-trunk/include/llvm/CodeGen/MachineConstantPool.h
@@ -54,6 +54,17 @@
                                         unsigned Alignment) = 0;
 
   virtual void AddSelectionDAGCSEId(FoldingSetNodeID &ID) = 0;
+
+  // @LOCALMOD-START
+  /// getJumpTableIndex - Check if this is a reference to a jump table.
+  /// If so, return a pointer to the jump table index value that is stored
+  /// in the constant pool, else return 0.
+  /// The default behavior is to indicate that the value is not a jump table
+  /// index. This is used by BranchFolder::runOnMachineFunction() and only in
+  /// conjunction with ARM targets
+  /// TODO: this should be cleaned up as it does tripple duty: tester, setter, getter
+  virtual unsigned *getJumpTableIndex() { return 0; }
+  // @LOCALMOD-END
 
   /// print - Implement operator<<
   virtual void print(raw_ostream &O) const = 0;
diff -r c06933b0e3e6 llvm-trunk/lib/CodeGen/BranchFolding.cpp
--- a/llvm-trunk/lib/CodeGen/BranchFolding.cpp
+++ b/llvm-trunk/lib/CodeGen/BranchFolding.cpp
@@ -20,6 +20,7 @@
 #include "BranchFolding.h"
 #include "llvm/Function.h"
 #include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/MachineConstantPool.h" //  @LOCALMOD
 #include "llvm/CodeGen/MachineModuleInfo.h"
 #include "llvm/CodeGen/MachineFunctionPass.h"
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
@@ -241,6 +242,24 @@
           JTIsLive.set(NewIdx);
         }
     }
+
+    // @LOCALMOD-START
+    // This currently only used on ARM targets where the ConstantPool
+    // subclass is overloading getJumpTableIndex()
+    const std::vector<MachineConstantPoolEntry>& CPs =
+      MF.getConstantPool()->getConstants();
+    for (unsigned i = 0, e = CPs.size(); i != e; ++i) {
+      if (!CPs[i].isMachineConstantPoolEntry()) continue;
+      unsigned *JTIndex = CPs[i].Val.MachineCPVal->getJumpTableIndex();
+      if (!JTIndex) continue;
+      // Remap index.
+      const unsigned NewIdx = JTMapping[*JTIndex];
+      *JTIndex = NewIdx;
+      // Remember that this JT is live.
+      JTIsLive.set(NewIdx);
+    }
+    // @LOCALMOD-END
+
 
     // Finally, remove dead jump tables.  This happens either because the
     // indirect jump was unreachable (and thus deleted) or because the jump
diff -r c06933b0e3e6 llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
--- a/llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+++ b/llvm-trunk/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
@@ -5853,6 +5853,11 @@
   if (ConstantFPSDNode *TV = dyn_cast<ConstantFPSDNode>(N2))
     if (ConstantFPSDNode *FV = dyn_cast<ConstantFPSDNode>(N3)) {
       if (TLI.isTypeLegal(N2.getValueType()) &&
+          // @LOCALMOD-START
+          // when we combine two 8byte constants into a 16byte one
+          // we get constant pool entries which are too big
+          TLI.getTargetData()->getTypeAllocSize(FV->getConstantFPValue()->getType()) <= 4 &&
+          // @LOCALMOD-STOP
           (TLI.getOperationAction(ISD::ConstantFP, N2.getValueType()) !=
            TargetLowering::Legal) &&
           // If both constants have multiple uses, then we won't need to do an
diff -r c06933b0e3e6 llvm-trunk/lib/Linker/LinkArchives.cpp
--- a/llvm-trunk/lib/Linker/LinkArchives.cpp
+++ b/llvm-trunk/lib/Linker/LinkArchives.cpp
@@ -18,9 +18,22 @@
 #include "llvm/ADT/SetOperations.h"
 #include "llvm/Bitcode/Archive.h"
 #include "llvm/Config/config.h"
+
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+
 #include <memory>
 #include <set>
+
 using namespace llvm;
+
+// @LOCALMOD-START
+static cl::list<std::string>
+ UndefList("referenced-list", cl::value_desc("list"),
+         cl::desc("A list of symbols assumed to be referenced"),
+         cl::CommaSeparated);
+
+// @LOCALMOD-END
+
 
 /// GetAllUndefinedSymbols - calculates the set of undefined symbols that still
 /// exist in an LLVM module. This is a bit tricky because there may be two
@@ -39,6 +52,8 @@
   std::set<std::string> DefinedSymbols;
   UndefinedSymbols.clear();
 
+  UndefinedSymbols.insert(UndefList.begin(), UndefList.end()); // @LOCALMOD
+  
   // If the program doesn't define a main, try pulling one in from a .a file.
   // This is needed for programs where the main function is defined in an
   // archive, such f2c'd programs.
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARM.h
--- a/llvm-trunk/lib/Target/ARM/ARM.h
+++ b/llvm-trunk/lib/Target/ARM/ARM.h
@@ -105,6 +105,7 @@
 FunctionPass *createARMLoadStoreOptimizationPass(bool PreAlloc = false);
 FunctionPass *createARMExpandPseudoPass();
 FunctionPass *createARMConstantIslandPass();
+FunctionPass *createARMSFIPlacementPass();
 FunctionPass *createNEONPreAllocPass();
 FunctionPass *createNEONMoveFixPass();
 FunctionPass *createThumb2ITBlockPass();
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMBaseInstrInfo.cpp
@@ -34,6 +34,9 @@
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 using namespace llvm;
+
+
+extern cl::opt<bool> FlagSfiStack; // @LOCALMOD
 
 static cl::opt<bool>
 EnableARM3Addr("enable-arm-3-addr-conv", cl::Hidden,
@@ -411,6 +414,9 @@
   return JT[JTI].MBBs.size();
 }
 
+// @LOCALMOD-START
+// @NOTE: this needs to be fixe to make the constand island estimates better
+// @LOCALMOD-END
 /// GetInstSize - Return the size of the specified MachineInstr.
 ///
 unsigned ARMBaseInstrInfo::GetInstSizeInBytes(const MachineInstr *MI) const {
@@ -634,8 +640,17 @@
   }
 
   if (DestRC == ARM::GPRRegisterClass) {
+    // @LOCALMOD-START
+    // NOTE: rename stack loads/moves so we have an sfi hook
+    if (FlagSfiStack && DestReg == ARM::SP ) {
+      AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::STACK_MOVr),
+					  DestReg).addReg(SrcReg)));
+    } else {
+      // ORGIGNAL
     AddDefaultCC(AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::MOVr),
                                         DestReg).addReg(SrcReg)));
+    }
+    // @LOCALMOD-END
   } else if (DestRC == ARM::SPRRegisterClass) {
     AddDefaultPred(BuildMI(MBB, I, DL, get(ARM::VMOVS), DestReg)
                    .addReg(SrcReg));
@@ -1047,7 +1062,17 @@
     assert(ARM_AM::getSOImmVal(ThisVal) != -1 && "Bit extraction didn't work?");
 
     // Build the new ADD / SUB.
-    unsigned Opc = isSub ? ARM::SUBri : ARM::ADDri;
+    // @LOCALMOD-START
+    // NOTE: add stackhook
+    unsigned Opc;
+    if (FlagSfiStack && DestReg == ARM::SP ) {
+      Opc = isSub ? ARM::STACK_SUBri : ARM::STACK_ADDri;
+
+      /* assert(BaseReg == DestReg); */
+    } else {
+      Opc = isSub ? ARM::SUBri : ARM::ADDri;
+    }
+    // @LOCALMOD-END
     BuildMI(MBB, MBBI, dl, TII.get(Opc), DestReg)
       .addReg(BaseReg, RegState::Kill).addImm(ThisVal)
       .addImm((unsigned)Pred).addReg(PredReg).addReg(0);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMBaseRegisterInfo.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMBaseRegisterInfo.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMBaseRegisterInfo.cpp
@@ -143,10 +143,21 @@
     FramePtr((STI.isTargetDarwin() || STI.isThumb()) ? ARM::R7 : ARM::R11) {
 }
 
+extern cl::opt<bool> ReserveR9; // @LOCALMOD
+
 const unsigned*
 ARMBaseRegisterInfo::getCalleeSavedRegs(const MachineFunction *MF) const {
   static const unsigned CalleeSavedRegs[] = {
     ARM::LR, ARM::R11, ARM::R10, ARM::R9, ARM::R8,
+    ARM::R7, ARM::R6,  ARM::R5,  ARM::R4,
+
+    ARM::D15, ARM::D14, ARM::D13, ARM::D12,
+    ARM::D11, ARM::D10, ARM::D9,  ARM::D8,
+    0
+  };
+
+  static const unsigned CalleeSavedRegsNoR9[] = { // @LOCALMOD
+    ARM::LR, ARM::R11, ARM::R10, ARM::R8,
     ARM::R7, ARM::R6,  ARM::R5,  ARM::R4,
 
     ARM::D15, ARM::D14, ARM::D13, ARM::D12,
@@ -164,14 +175,30 @@
     ARM::D11, ARM::D10, ARM::D9,  ARM::D8,
     0
   };
-  return STI.isTargetDarwin() ? DarwinCalleeSavedRegs : CalleeSavedRegs;
+  if (STI.isTargetDarwin())
+    return DarwinCalleeSavedRegs;
+  if (ReserveR9)
+    return CalleeSavedRegsNoR9; // @LOCALMOD
+  return CalleeSavedRegs;
 }
 
 const TargetRegisterClass* const *
 ARMBaseRegisterInfo::getCalleeSavedRegClasses(const MachineFunction *MF) const {
+  // FIXME: Having both getCalleeSavedRegClasses and getCalleeSavedRegs is
+  // error prone.
   static const TargetRegisterClass * const CalleeSavedRegClasses[] = {
     &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
     &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
+    &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
+
+    &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass,
+    &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass,
+    0
+  };
+
+  static const TargetRegisterClass * const CalleeSavedRegClassesNoR9[] = { // @LOCALMOD
+    &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
+                       &ARM::GPRRegClass, &ARM::GPRRegClass,
     &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
 
     &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass,
@@ -189,6 +216,16 @@
     0
   };
 
+  static const TargetRegisterClass * const ThumbCalleeSavedRegClassesNoR9[] = { // @LOCALMOD
+    &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
+                       &ARM::GPRRegClass, &ARM::tGPRRegClass,
+    &ARM::tGPRRegClass,&ARM::tGPRRegClass,&ARM::tGPRRegClass,
+
+    &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass,
+    &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass, &ARM::DPRRegClass,
+    0
+  };
+
   static const TargetRegisterClass * const DarwinCalleeSavedRegClasses[] = {
     &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
     &ARM::GPRRegClass, &ARM::GPRRegClass, &ARM::GPRRegClass,
@@ -209,12 +246,14 @@
     0
   };
 
-  if (STI.isThumb1Only()) {
-    return STI.isTargetDarwin()
-      ? DarwinThumbCalleeSavedRegClasses : ThumbCalleeSavedRegClasses;
-  }
-  return STI.isTargetDarwin()
-    ? DarwinCalleeSavedRegClasses : CalleeSavedRegClasses;
+  if (STI.isTargetDarwin())
+    return STI.isThumb1Only() ? DarwinThumbCalleeSavedRegClasses :
+      DarwinCalleeSavedRegClasses;
+  if (ReserveR9)
+    return STI.isThumb1Only() ? ThumbCalleeSavedRegClassesNoR9
+      : CalleeSavedRegClassesNoR9;
+  return STI.isThumb1Only() ? ThumbCalleeSavedRegClasses
+    : CalleeSavedRegClasses;
 }
 
 BitVector ARMBaseRegisterInfo::getReservedRegs(const MachineFunction &MF) const {
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantIslandPass.cpp
@@ -35,6 +35,15 @@
 #include <algorithm>
 using namespace llvm;
 
+// @LOCALMOD-START
+#include "llvm/Support/CommandLine.h"
+
+cl::opt<bool> FlagSfiCpFudge("sfi-cp-fudge");
+cl::opt<int> FlagSfiCpFudgePercent("sfi-cp-fudge-percent", cl::init(85));
+extern cl::opt<bool> FlagSfiBranch;
+// @LOCALMOD-END
+
+
 STATISTIC(NumCPEs,       "Number of constpool entries");
 STATISTIC(NumSplit,      "Number of uncond branches inserted");
 STATISTIC(NumCBrFixed,   "Number of cond branches fixed");
@@ -52,6 +61,7 @@
           cl::desc("Adjust basic block layout to better use TB[BH]"));
 
 namespace {
+
   /// ARMConstantIslands - Due to limited PC-relative displacements, ARM
   /// requires constant pool entries to be scattered among the instructions
   /// inside a function.  To do this, it completely ignores the normal LLVM
@@ -179,6 +189,13 @@
     }
 
   private:
+    // @LOCALMOD-BEGIN
+    unsigned GetFudge(const MachineInstr* I,
+                      unsigned  offset,
+                      bool is_start,
+                      bool is_end,
+                      bool is_jump_target) const;
+    // @LOCALMOD-END
     void DoInitialPlacement(MachineFunction &MF,
                             std::vector<MachineInstr*> &CPEMIs);
     CPEntry *findConstPoolEntry(unsigned CPI, const MachineInstr *CPEMI);
@@ -259,7 +276,6 @@
 
 bool ARMConstantIslands::runOnMachineFunction(MachineFunction &MF) {
   MachineConstantPool &MCP = *MF.getConstantPool();
-
   TII = MF.getTarget().getInstrInfo();
   AFI = MF.getInfo<ARMFunctionInfo>();
   STI = &MF.getTarget().getSubtarget<ARMSubtarget>();
@@ -467,11 +483,132 @@
   }
 }
 
+
+//@LOCALMOD-START
+// We try to account for extra sfi space overhead here
+// NOTE: This function needs to be updatd whenever changes
+//       to the sfi scheme are made
+// NOTE: this is very likely missing a few cases
+//       we will add those as neeeded and otherwise
+//       rely on artificially reducing the ldr offset range.
+// NOTE: one missing case: jump table targets are 16 bytes aligned
+unsigned ARMConstantIslands::GetFudge(const MachineInstr* I,
+                                      unsigned  offset,
+                                      bool is_start,
+                                      bool is_end,
+                                      bool is_jump_target) const {
+  if (!FlagSfiCpFudge) return 0;
+  const int kBundleSize = 16;
+  unsigned fudge = 0;
+  const int Opc = I->getOpcode();
+
+  if (is_jump_target && is_start) {
+    while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+  }
+
+  switch(Opc) {
+   case ARM::BL:
+   case ARM::BLX:
+   case ARM::BL_pred:
+   case ARM::BLr9:
+   case ARM::BLXr9:
+   case ARM::BLr9_pred:
+   case ARM::TPsoft:
+    // branches must be in the last slot
+    while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    break;
+
+   case ARM::CONSTPOOL_ENTRY:
+    if (is_start) {
+      while ( (offset + fudge) % kBundleSize != 0) fudge += 4;
+    }
+
+    if (is_end) {
+      while ( (offset + fudge) % kBundleSize != 0xc) fudge += 4;
+    }
+
+    {
+      const int size = TII->GetInstSizeInBytes(I);
+      assert (size == 4 || size == 8);
+      // we do not want the data to cross bundle boundaries
+      if (size == 8) {
+        if((offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      }
+    }
+    // illegal if at data bundle beginning
+    if ((offset + fudge) % kBundleSize == 0) fudge += 4;
+    break;
+
+   case ARM::STACK_ADDri:
+   case ARM::STACK_SUBri:
+   case ARM::STACK_MOVr:
+    // stack adjusts must not be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // add masking
+    fudge += 4;
+    break;
+
+   case ARM::STR:
+   case ARM::STRB:
+   case ARM::STRH:
+   case ARM::STRD:
+    // TODO: there are vfp stores missing
+   case ARM::VSTRS:
+   case ARM::VSTRD:
+
+    // case ARM::STM:// TODO: make this work
+    {
+    const MachineOperand &MO1 = I->getOperand(1);
+    if (MO1.getReg() != ARM::SP) {
+      // cannot be in the last slot
+      if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+      // one mask
+      fudge += 4;
+    }
+    break;
+    }
+  case ARM::BX:
+  case ARM::BXr9:
+  case ARM::BX_RET:
+  case ARM::BRIND:
+    // cannot be in the last slot
+    if ( (offset + fudge) % kBundleSize == 0xc) fudge += 4;
+    // one mask
+    fudge += 4;
+    break;
+  }
+
+  return fudge;
+}
+
+static void UpdateJumpTargetAlignment(MachineFunction &MF) {
+  if (!FlagSfiBranch) return;
+
+  // JUMP TABLE TARGETS
+  MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+  const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+  for (unsigned i=0; i < JT.size(); ++i) {
+    std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+    //cout << "JUMPTABLE "<< i << " " << MBBs.size() << "\n";
+    for (unsigned j=0; j < MBBs.size(); ++j) {
+      if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+        continue;
+      }
+      MBBs[j]->setAlignment(16);
+    }
+  }
+}
+
+//@LOCALMOD-END
+
 /// InitialFunctionScan - Do the initial scan of the function, building up
 /// information about the sizes of each block, the location of all the water,
 /// and finding all of the constant pool users.
 void ARMConstantIslands::InitialFunctionScan(MachineFunction &MF,
                                  const std::vector<MachineInstr*> &CPEMIs) {
+  UpdateJumpTargetAlignment(MF); // @LOCALMOD
+
   unsigned Offset = 0;
   for (MachineFunction::iterator MBBI = MF.begin(), E = MF.end();
        MBBI != E; ++MBBI) {
@@ -483,12 +620,24 @@
       WaterList.push_back(&MBB);
 
     unsigned MBBSize = 0;
+
     for (MachineBasicBlock::iterator I = MBB.begin(), E = MBB.end();
          I != E; ++I) {
+      //@LOCALMOD-START
+      // TODO: also account for jump_targets more
+      MBBSize += GetFudge(I,
+                          Offset,
+                          I == MBB.begin(),
+                          I == E,
+                          MF.begin() == MBBI || MBBI->getAlignment() == 16);
+      //@LOCALMOD-END
+
       // Add instruction size to MBBSize.
       MBBSize += TII->GetInstSizeInBytes(I);
 
       int Opc = I->getOpcode();
+
+
       if (I->getDesc().isBranch()) {
         bool isCond = false;
         unsigned Bits = 0;
@@ -545,6 +694,7 @@
 
       if (Opc == ARM::tPUSH || Opc == ARM::tPOP_RET)
         PushPopMIs.push_back(I);
+
 
       if (Opc == ARM::CONSTPOOL_ENTRY)
         continue;
@@ -611,6 +761,13 @@
           unsigned CPI = I->getOperand(op).getIndex();
           MachineInstr *CPEMI = CPEMIs[CPI];
           unsigned MaxOffs = ((1 << Bits)-1) * Scale;
+
+          // @LOCALMOD-BEGIN
+          if (FlagSfiCpFudge) {
+            MaxOffs *= FlagSfiCpFudgePercent;
+            MaxOffs /= 100;
+          }
+          // @LOCALMOD-END
           CPUsers.push_back(CPUser(I, CPEMI, MaxOffs, NegOk, IsSoImm));
 
           // Increment corresponding CPEntry reference count.
@@ -661,6 +818,11 @@
     assert(I != MBB->end() && "Didn't find MI in its own basic block?");
     if (&*I == MI) return Offset;
     Offset += TII->GetInstSizeInBytes(I);
+
+    // @LOCALMOD-START
+    // TODO: take jump targets into account
+    Offset += GetFudge(I, Offset, I ==  MBB->begin(), I == MBB->end(), 0);
+    // @LOCALMOD-END
   }
 }
 
@@ -763,9 +925,12 @@
   // contain a constpool_entry or tablejump.)
   unsigned NewBBSize = 0;
   for (MachineBasicBlock::iterator I = NewBB->begin(), E = NewBB->end();
-       I != E; ++I)
+       I != E; ++I) {
+    // @LOCALMOD-START
+    NewBBSize += GetFudge(I, NewBBSize, false, false, 0);
+    // @LOCALMOD-END
     NewBBSize += TII->GetInstSizeInBytes(I);
-
+  }
   unsigned OrigBBI = OrigBB->getNumber();
   unsigned NewBBI = NewBB->getNumber();
   // Set the size of NewBB in BBSizes.
@@ -882,9 +1047,19 @@
 void ARMConstantIslands::AdjustBBOffsetsAfter(MachineBasicBlock *BB,
                                               int delta) {
   MachineFunction::iterator MBBI = BB; MBBI = next(MBBI);
+  // @LOCALMOD-START
+#if 1
+  if (delta > 0) {
+    BBSizes[BB->getNumber()] += 4;  // @LOCALMOD
+    delta += 4;
+  }
+#endif
+  // @LOCALMOD-END
+
   for(unsigned i = BB->getNumber()+1, e = BB->getParent()->getNumBlockIDs();
       i < e; ++i) {
     BBOffsets[i] += delta;
+
     // If some existing blocks have padding, adjust the padding as needed, a
     // bit tricky.  delta can be negative so don't use % on that.
     if (!isThumb)
@@ -1122,7 +1297,15 @@
     // The 4 in the following is for the unconditional branch we'll be
     // inserting (allows for long branch on Thumb1).  Alignment of the
     // island is handled inside OffsetIsInRange.
-    unsigned BaseInsertOffset = UserOffset + U.MaxDisp -4;
+     // @LOCALMOD-START
+     unsigned BaseInsertOffset = UserOffset - 4;
+     if (FlagSfiCpFudge) {
+       BaseInsertOffset += U.MaxDisp * FlagSfiCpFudgePercent / 100;
+     } else {
+       BaseInsertOffset += U.MaxDisp;
+     }
+     // @LOCALMOD-END
+     
     // This could point off the end of the block if we've already got
     // constant pool entries following this block; only the last one is
     // in the water list.  Back past any possible branches (allow for a
@@ -1135,10 +1318,17 @@
     MachineBasicBlock::iterator MI = UserMI;
     ++MI;
     unsigned CPUIndex = CPUserIndex+1;
+    // @LOCALMOD: TODO: GetInstSizeInBytes() should be replaced with
+    //                  our estimator
     for (unsigned Offset = UserOffset+TII->GetInstSizeInBytes(UserMI);
          Offset < BaseInsertOffset;
-         Offset += TII->GetInstSizeInBytes(MI),
-            MI = next(MI)) {
+         // @LOCALMOD-START
+         Offset +=  GetFudge(MI, Offset, false, false, false) +
+                           TII->GetInstSizeInBytes(MI),
+        // @LOCALMOD-END 
+         MI = next(MI)) {
+      
+      assert(MI != UserMBB->end() && "getting out of range"); // @LOCALMOD
       if (CPUIndex < CPUsers.size() && CPUsers[CPUIndex].MI == MI) {
         CPUser &U = CPUsers[CPUIndex];
         if (!OffsetIsInRange(Offset, EndInsertOffset,
@@ -1354,6 +1544,9 @@
 /// Otherwise, add an intermediate branch instruction to a branch.
 bool
 ARMConstantIslands::FixUpUnconditionalBr(MachineFunction &MF, ImmBranch &Br) {
+  // @LOCALMOD-start
+  assert(0 && "fix up uncond br not implemented");
+  // @LOCALMOD-end
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *MBB = MI->getParent();
   if (!isThumb1)
@@ -1377,6 +1570,9 @@
 /// conditional branch + an unconditional branch to the destination.
 bool
 ARMConstantIslands::FixUpConditionalBr(MachineFunction &MF, ImmBranch &Br) {
+  // @LOCALMOD-start
+  assert(0 && "fix up cond br not implemented");
+  // @LOCALMOD-end
   MachineInstr *MI = Br.MI;
   MachineBasicBlock *DestBB = MI->getOperand(0).getMBB();
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.cpp
@@ -27,7 +27,8 @@
                                            const char *Modif,
                                            bool AddCA)
   : MachineConstantPoolValue((const Type*)cval->getType()),
-    CVal(cval), S(NULL), LabelId(id), Kind(K), PCAdjust(PCAdj),
+    // @LOCALMOD
+    CVal(cval), S(NULL),  JumpTableIndex(0), LabelId(id), Kind(K), PCAdjust(PCAdj),
     Modifier(Modif), AddCurrentAddress(AddCA) {}
 
 ARMConstantPoolValue::ARMConstantPoolValue(LLVMContext &C,
@@ -36,13 +37,24 @@
                                            const char *Modif,
                                            bool AddCA)
   : MachineConstantPoolValue((const Type*)Type::getInt32Ty(C)),
-    CVal(NULL), S(strdup(s)), LabelId(id), Kind(ARMCP::CPExtSymbol),
+    // @LOCALMOD
+    CVal(NULL), S(strdup(s)), JumpTableIndex(0), LabelId(id), Kind(ARMCP::CPExtSymbol),
     PCAdjust(PCAdj), Modifier(Modif), AddCurrentAddress(AddCA) {}
 
 ARMConstantPoolValue::ARMConstantPoolValue(GlobalValue *gv, const char *Modif)
   : MachineConstantPoolValue((const Type*)Type::getInt32Ty(gv->getContext())),
-    CVal(gv), S(NULL), LabelId(0), Kind(ARMCP::CPValue), PCAdjust(0),
+    // @LOCALMOD
+    CVal(gv), S(NULL), JumpTableIndex(0), LabelId(0), Kind(ARMCP::CPValue), PCAdjust(0),
     Modifier(Modif) {}
+
+// @LOCALMOD-START
+ARMConstantPoolValue::ARMConstantPoolValue(LLVMContext &C, unsigned jt)
+  : MachineConstantPoolValue((const Type*)Type::getInt32Ty(C)),
+      CVal(NULL), S(NULL), JumpTableIndex(jt), LabelId(0), Kind(ARMCP::CPJumpTable),
+    PCAdjust(0), Modifier(NULL) {}
+// @LOCALMOD-END
+
+
 
 GlobalValue *ARMConstantPoolValue::getGV() const {
   return dyn_cast_or_null<GlobalValue>(CVal);
@@ -63,6 +75,7 @@
         (ARMConstantPoolValue *)Constants[i].Val.MachineCPVal;
       if (CPV->CVal == CVal &&
           CPV->LabelId == LabelId &&
+          CPV->JumpTableIndex == JumpTableIndex && // @LOCALMOD
           CPV->PCAdjust == PCAdjust &&
           (CPV->S == S || strcmp(CPV->S, S) == 0) &&
           (CPV->Modifier == Modifier || strcmp(CPV->Modifier, Modifier) == 0))
@@ -81,6 +94,7 @@
 ARMConstantPoolValue::AddSelectionDAGCSEId(FoldingSetNodeID &ID) {
   ID.AddPointer(CVal);
   ID.AddPointer(S);
+  ID.AddInteger(JumpTableIndex);   // @LOCALMOD
   ID.AddInteger(LabelId);
   ID.AddInteger(PCAdjust);
 }
@@ -89,6 +103,7 @@
 ARMConstantPoolValue::hasSameValue(ARMConstantPoolValue *ACPV) {
   if (ACPV->Kind == Kind &&
       ACPV->CVal == CVal &&
+      ACPV->JumpTableIndex == JumpTableIndex && // @LOCALMOD
       ACPV->PCAdjust == PCAdjust &&
       (ACPV->S == S || strcmp(ACPV->S, S) == 0) &&
       (ACPV->Modifier == Modifier || strcmp(ACPV->Modifier, Modifier) == 0)) {
@@ -110,6 +125,10 @@
 void ARMConstantPoolValue::print(raw_ostream &O) const {
   if (CVal)
     O << CVal->getName();
+  // @LOCALMOD-START
+  else if (isJumpTable())
+    O << "jumptable_" << JumpTableIndex;
+  // @LOCALMOD-END
   else
     O << S;
   if (Modifier) O << "(" << Modifier << ")";
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
--- a/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
+++ b/llvm-trunk/lib/Target/ARM/ARMConstantPoolValue.h
@@ -28,7 +28,8 @@
     CPValue,
     CPExtSymbol,
     CPBlockAddress,
-    CPLSDA
+    CPLSDA,
+    CPJumpTable // @LOCALMOD
   };
 }
 
@@ -38,6 +39,7 @@
 class ARMConstantPoolValue : public MachineConstantPoolValue {
   Constant *CVal;          // Constant being loaded.
   const char *S;           // ExtSymbol being loaded.
+  unsigned JumpTableIndex; // Index of a jump table. // @LOCALMOD
   unsigned LabelId;        // Label id of the load.
   ARMCP::ARMCPKind Kind;   // Kind of constant.
   unsigned char PCAdjust;  // Extra adjustment if constantpool is pc-relative.
@@ -54,6 +56,7 @@
                        unsigned char PCAdj = 0, const char *Modifier = NULL,
                        bool AddCurrentAddress = false);
   ARMConstantPoolValue(GlobalValue *GV, const char *Modifier);
+  ARMConstantPoolValue(LLVMContext &C, unsigned jt); // @LOCALMOD
   ARMConstantPoolValue();
   ~ARMConstantPoolValue();
 
@@ -69,6 +72,13 @@
   bool isExtSymbol() const { return Kind == ARMCP::CPExtSymbol; }
   bool isBlockAddress() { return Kind == ARMCP::CPBlockAddress; }
   bool isLSDA() { return Kind == ARMCP::CPLSDA; }
+  // @LOCALMOD-START
+  bool isValue() const { return Kind == ARMCP::CPValue; }
+  bool isJumpTable() const { return Kind == ARMCP::CPJumpTable; }
+  virtual unsigned *getJumpTableIndex() {
+    return isJumpTable() ? &JumpTableIndex : 0;
+  }
+  // @LOCALMOD-END
 
   virtual unsigned getRelocationInfo() const {
     // FIXME: This is conservatively claiming that these entries require a
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMISelDAGToDAG.cpp
@@ -34,6 +34,10 @@
 #include "llvm/Support/raw_ostream.h"
 
 using namespace llvm;
+
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
+
 
 //===--------------------------------------------------------------------===//
 /// ARMDAGToDAGISel - ARM specific code to select ARM machine
@@ -218,6 +222,11 @@
 bool ARMDAGToDAGISel::SelectAddrMode2(SDValue Op, SDValue N,
                                       SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
+
+
+  const bool is_store = (Op.getOpcode() == ISD::STORE); // @LOCALMOD
+  if (!FlagSfiStore || !is_store ) { // @LOCALMOD
+
   if (N.getOpcode() == ISD::MUL) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
       // X * [3,5,9] -> X + X * [2,4,8] etc.
@@ -240,6 +249,7 @@
       }
     }
   }
+  } // @LOCALMOD
 
   if (N.getOpcode() != ISD::ADD && N.getOpcode() != ISD::SUB) {
     Base = N;
@@ -257,7 +267,7 @@
   }
 
   // Match simple R +/- imm12 operands.
-  if (N.getOpcode() == ISD::ADD)
+  if (N.getOpcode() == ISD::ADD) {
     if (ConstantSDNode *RHS = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
       int RHSC = (int)RHS->getZExtValue();
       if ((RHSC >= 0 && RHSC < 0x1000) ||
@@ -280,6 +290,24 @@
         return true;
       }
     }
+  }
+
+  // @LOCALMOD-START
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    if (N.getOpcode() == ISD::FrameIndex) {
+      int FI = cast<FrameIndexSDNode>(N)->getIndex();
+      Base = CurDAG->getTargetFrameIndex(FI, TLI.getPointerTy());
+    } else if (N.getOpcode() == ARMISD::Wrapper) {
+      Base = N.getOperand(0);
+    }
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(ARM_AM::add, 0,
+                                                      ARM_AM::no_shift),
+                                    MVT::i32);
+    return true;
+  }
+  // @LOCALMOD-END
 
   // Otherwise this is R +/- [possibly shifted] R.
   ARM_AM::AddrOpc AddSub = N.getOpcode() == ISD::ADD ? ARM_AM::add:ARM_AM::sub;
@@ -320,9 +348,13 @@
 
   Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(AddSub, ShAmt, ShOpcVal),
                                   MVT::i32);
+
   return true;
 }
 
+// @@ For a load/store operation op
+// @@ and and address node n
+// @@ come up with a an Offset and Opc????
 bool ARMDAGToDAGISel::SelectAddrMode2Offset(SDValue Op, SDValue N,
                                             SDValue &Offset, SDValue &Opc) {
   unsigned Opcode = Op.getOpcode();
@@ -334,6 +366,7 @@
   if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(N)) {
     int Val = (int)C->getZExtValue();
     if (Val >= 0 && Val < 0x1000) { // 12 bits.
+      // Register 0 means no offset
       Offset = CurDAG->getRegister(0, MVT::i32);
       Opc = CurDAG->getTargetConstant(ARM_AM::getAM2Opc(AddSub, Val,
                                                         ARM_AM::no_shift),
@@ -342,13 +375,19 @@
     }
   }
 
+
+  const bool is_store = (Opcode == ISD::STORE); // @LOCALMOD
+
   Offset = N;
   ARM_AM::ShiftOpc ShOpcVal = ARM_AM::getShiftOpcForNode(N);
   unsigned ShAmt = 0;
+  // @@ CONVOLUTED LOGIC BELOW: REWRITE
   if (ShOpcVal != ARM_AM::no_shift) {
     // Check to see if the RHS of the shift is a constant, if not, we can't fold
     // it.
-    if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+    //if (ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+    ConstantSDNode *Sh = dyn_cast<ConstantSDNode>(N.getOperand(1));
+    if ((!FlagSfiStore || !is_store) && Sh ) { // @LOCALMOD
       ShAmt = Sh->getZExtValue();
       Offset = N.getOperand(0);
     } else {
@@ -361,11 +400,17 @@
   return true;
 }
 
-
+// @@ Op: load store
+// @@N: node for computing the address
+//
+// @@ BASE, offset, Opc represent the new addreessing mode
 bool ARMDAGToDAGISel::SelectAddrMode3(SDValue Op, SDValue N,
                                       SDValue &Base, SDValue &Offset,
                                       SDValue &Opc) {
-  if (N.getOpcode() == ISD::SUB) {
+
+  const bool is_store = (Op.getOpcode() == ISD::STORE); // @LOCALMOD
+
+  if ((!FlagSfiStore ||!is_store) && N.getOpcode() == ISD::SUB) {  // @LOCALMOD
     // X - C  is canonicalize to X + -C, no need to handle it here.
     Base = N.getOperand(0);
     Offset = N.getOperand(1);
@@ -405,6 +450,17 @@
       return true;
     }
   }
+
+
+  // @LOCALMOD-START
+  if (FlagSfiStore && is_store) {
+    Base = N;
+    Offset = CurDAG->getRegister(0, MVT::i32);
+    Opc = CurDAG->getTargetConstant(ARM_AM::getAM3Opc(ARM_AM::add, 0),MVT::i32);
+    return true;
+  }
+  // @LOCALMOD-END
+
 
   Base = N.getOperand(0);
   Offset = N.getOperand(1);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMISelLowering.cpp
@@ -42,9 +42,14 @@
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include <sstream>
+
 using namespace llvm;
 
+#include "llvm/Support/CommandLine.h"
+
+
 static bool CC_ARM_APCS_Custom_f64(unsigned &ValNo, EVT &ValVT, EVT &LocVT,
+
                                    CCValAssign::LocInfo &LocInfo,
                                    ISD::ArgFlagsTy &ArgFlags,
                                    CCState &State);
@@ -61,8 +66,18 @@
                                        ISD::ArgFlagsTy &ArgFlags,
                                        CCState &State);
 
+
+// @LOCALMOD-START
+extern cl::opt<bool> FlagSfiStore;
+
+cl::opt<bool> FlagAeabiCalls("aeabi-calls",
+                        cl::desc("use AEABI calls"));
+// @LOCALMOD-END
+
+
 void ARMTargetLowering::addTypeForNEON(EVT VT, EVT PromotedLdStVT,
                                        EVT PromotedBitwiseVT) {
+
   if (VT != PromotedLdStVT) {
     setOperationAction(ISD::LOAD, VT.getSimpleVT(), Promote);
     AddPromotedToType (ISD::LOAD, VT.getSimpleVT(),
@@ -135,6 +150,41 @@
 ARMTargetLowering::ARMTargetLowering(TargetMachine &TM)
     : TargetLowering(TM, createTLOF(TM)) {
   Subtarget = &TM.getSubtarget<ARMSubtarget>();
+
+  // @LOCALMOD-START
+  // NOTE: THIS IS INCOMPLETE: IT LACKS COMPARISON, ETC
+  //  NOTE: also check thread stuff
+  //from:  http://gcc.gnu.org/ml/gcc-patches/2008-03/msg00141.html
+
+  if (FlagAeabiCalls) {
+
+    // Single-precision floating-point arithmetic.
+    setLibcallName(RTLIB::ADD_F32, "__aeabi_fadd");
+    setLibcallName(RTLIB::SUB_F32, "__aeabi_fsub");
+    setLibcallName(RTLIB::MUL_F32, "__aeabi_fmul");
+    setLibcallName(RTLIB::DIV_F32, "__aeabi_fdiv");
+
+    // Double-precision floating-point arithmetic.
+    setLibcallName(RTLIB::ADD_F64, "__aeabi_dadd");
+    setLibcallName(RTLIB::SUB_F64, "__aeabi_dsub");
+    setLibcallName(RTLIB::MUL_F64, "__aeabi_dmul");
+    setLibcallName(RTLIB::DIV_F64, "__aeabi_ddiv");
+
+
+    setLibcallName(RTLIB::FPTOSINT_F64_I32, "__aeabi_d2iz");
+    setLibcallName(RTLIB::FPTOUINT_F64_I32, "__aeabi_d2uiz");
+    setLibcallName(RTLIB::FPTOSINT_F32_I32, "__aeabi_f2iz");
+    setLibcallName(RTLIB::FPTOUINT_F32_I32, "__aeabi_f2uiz");
+
+    setLibcallName(RTLIB::FPROUND_F64_F32, "__aeabi_d2f");
+    setLibcallName(RTLIB::FPEXT_F32_F64,   "__aeabi_f2d");
+
+    setLibcallName(RTLIB::SINTTOFP_I32_F64, "__aeabi_i2d");
+    setLibcallName(RTLIB::UINTTOFP_I32_F64, "__aeabi_ui2d");
+    setLibcallName(RTLIB::SINTTOFP_I32_F32, "__aeabi_i2f");
+    setLibcallName(RTLIB::UINTTOFP_I32_F32, "__aeabi_ui2f");
+  }
+  // @LOCALMOD-END
 
   if (Subtarget->isTargetDarwin()) {
     // Uses VFP for Thumb libfuncs if available.
@@ -364,6 +414,11 @@
   setOperationAction(ISD::GLOBAL_OFFSET_TABLE, MVT::i32, Custom);
   setOperationAction(ISD::GlobalTLSAddress, MVT::i32, Custom);
   setOperationAction(ISD::BlockAddress, MVT::i32, Custom);
+  // @LOCALMOD-START
+  if (!Subtarget->useInlineJumpTables())
+    setOperationAction(ISD::JumpTable,     MVT::i32,   Custom);
+  // @LOCALMOD-END
+
 
   // Use the default implementation.
   setOperationAction(ISD::VASTART,            MVT::Other, Custom);
@@ -409,8 +464,11 @@
   setOperationAction(ISD::BR_CC,     MVT::i32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f32,   Custom);
   setOperationAction(ISD::BR_CC,     MVT::f64,   Custom);
-  setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
-
+  // @LOCALMOD-START
+  setOperationAction(ISD::BR_JT,     MVT::Other,
+		     Subtarget->useInlineJumpTables() ? Custom : Expand);
+  // @ORIGINAL setOperationAction(ISD::BR_JT,     MVT::Other, Custom);
+  // @LOCALMOD-END
   // We don't support sin/cos/fmod/copysign/pow
   setOperationAction(ISD::FSIN,      MVT::f64, Expand);
   setOperationAction(ISD::FSIN,      MVT::f32, Expand);
@@ -1239,6 +1297,24 @@
   return DAG.getNode(ARMISD::PIC_ADD, DL, PtrVT, Result, PICLabel);
 }
 
+// @LOCALMOD-START
+SDValue ARMTargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) {
+  assert(!Subtarget->useInlineJumpTables() &&
+	 "inline jump tables not custom lowered");
+  const MVT PTy = getPointerTy();
+  const JumpTableSDNode *JT = cast<JumpTableSDNode>(Op);
+  const SDValue JTI = DAG.getTargetJumpTable(JT->getIndex(), PTy);
+  const DebugLoc dl = Op.getDebugLoc();
+
+  ARMConstantPoolValue *CPV = new ARMConstantPoolValue(*DAG.getContext(),
+						       JT->getIndex());
+  // TODO: factor this idiom to load a value from a CP into a new function
+  const SDValue PoolEntry = DAG.getTargetConstantPool(CPV, PTy, 4);
+  const SDValue PoolWrapper = DAG.getNode(ARMISD::Wrapper, dl, PTy, PoolEntry);
+  return DAG.getLoad(PTy, dl, DAG.getEntryNode(), PoolWrapper, NULL, 0);
+}
+// @LOCALMOD-END
+
 // Lower ISD::GlobalTLSAddress using the "general dynamic" model
 SDValue
 ARMTargetLowering::LowerToTLSGeneralDynamicModel(GlobalAddressSDNode *GA,
@@ -1360,6 +1436,8 @@
                            PseudoSourceValue::getGOT(), 0);
     return Result;
   } else {
+    // The address will be stored in the constant pool, so
+    // put it in the pool and load it from there to materialize it
     SDValue CPAddr = DAG.getTargetConstantPool(GV, PtrVT, 4);
     CPAddr = DAG.getNode(ARMISD::Wrapper, dl, MVT::i32, CPAddr);
     return DAG.getLoad(PtrVT, dl, DAG.getEntryNode(), CPAddr,
@@ -1874,7 +1952,30 @@
   return Res;
 }
 
+
 SDValue ARMTargetLowering::LowerBR_JT(SDValue Op, SelectionDAG &DAG) {
+
+  //  The Jumptable idiom we are aiming for looks somthing like this:
+  //
+  //          .set PCRELV0, (.LJTI9_0_0-(.LPCRELL0+8))
+  //  .LPCRELL0:
+  //          add r3, pc, #PCRELV0
+  //          ldr pc, [r3, +r0, lsl #2]
+  //  .LJTI9_0_0:
+  //          .long    .LBB9_2
+  //          .long    .LBB9_5
+  //          .long    .LBB9_7
+  //          .long    .LBB9_4
+  //          .long    .LBB9_8
+  //
+  // In pic mode the table entries are relative to table beginning
+  // requiring and extra addition
+  //
+  // The code generation logic for ARMISD::BR_JT will also
+  // emit the table (c.f. ARMAsmPrinter::printJTBlockOperand())
+  // Also check  "def BR_JTm" in ARMInstrInfo.td
+
+  // allocate constant pool entry
   SDValue Chain = Op.getOperand(0);
   SDValue Table = Op.getOperand(1);
   SDValue Index = Op.getOperand(2);
@@ -2873,6 +2974,7 @@
   default: llvm_unreachable("Don't know how to custom lower this!");
   case ISD::ConstantPool:  return LowerConstantPool(Op, DAG);
   case ISD::BlockAddress:  return LowerBlockAddress(Op, DAG);
+  case ISD::JumpTable:     return LowerJumpTable(Op, DAG); // @LOCALMOD
   case ISD::GlobalAddress:
     return Subtarget->isTargetDarwin() ? LowerGlobalAddressDarwin(Op, DAG) :
       LowerGlobalAddressELF(Op, DAG);
@@ -3642,6 +3744,8 @@
 /// by AM is legal for this target, for a load/store of the specified type.
 bool ARMTargetLowering::isLegalAddressingMode(const AddrMode &AM,
                                               const Type *Ty) const {
+  // @DEBUG
+  //cout << "@@CHECK ADDRESSING MODE" << "\n";
   EVT VT = getValueType(Ty, true);
   if (!isLegalAddressImmediate(AM.BaseOffs, VT, Subtarget))
     return false;
@@ -3669,6 +3773,12 @@
       return isLegalT2ScaledAddressingMode(AM, VT);
 
     int Scale = AM.Scale;
+
+    // @LOCAMOD-START
+    // For simplicity do not allow scaling
+    if (FlagSfiStore && Scale != 0) return false;
+    // @LOCAMOD-END
+
     switch (VT.getSimpleVT().SimpleTy) {
     default: return false;
     case MVT::i1:
@@ -3806,6 +3916,14 @@
   if (Subtarget->isThumb1Only())
     return false;
 
+  // @LOCAMOD-START
+  // NOTE: disallow ...
+  // NOTE: THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCAMOD-END
+
   EVT VT;
   SDValue Ptr;
   bool isSEXTLoad = false;
@@ -3844,6 +3962,13 @@
                                                    SelectionDAG &DAG) const {
   if (Subtarget->isThumb1Only())
     return false;
+
+  // @LOCALMOD-START
+  // THIS IS A LITTLE DRASTIC
+  if (FlagSfiStore && N->getOpcode() == ISD::STORE) {
+    return false;
+  }
+  // @LOCALMOD-END
 
   EVT VT;
   SDValue Ptr;
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMISelLowering.h
--- a/llvm-trunk/lib/Target/ARM/ARMISelLowering.h
+++ b/llvm-trunk/lib/Target/ARM/ARMISelLowering.h
@@ -274,6 +274,7 @@
                              ISD::ArgFlagsTy Flags);
     SDValue LowerINTRINSIC_W_CHAIN(SDValue Op, SelectionDAG &DAG);
     SDValue LowerINTRINSIC_WO_CHAIN(SDValue Op, SelectionDAG &DAG);
+    SDValue LowerJumpTable(SDValue Op, SelectionDAG &DAG); // @LOCALMOD
     SDValue LowerBlockAddress(SDValue Op, SelectionDAG &DAG);
     SDValue LowerGlobalAddressDarwin(SDValue Op, SelectionDAG &DAG);
     SDValue LowerGlobalAddressELF(SDValue Op, SelectionDAG &DAG);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrInfo.cpp
@@ -23,6 +23,7 @@
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
 #include "llvm/MC/MCAsmInfo.h"
 using namespace llvm;
+
 
 ARMInstrInfo::ARMInstrInfo(const ARMSubtarget &STI)
   : ARMBaseInstrInfo(STI), RI(*this, STI) {
@@ -102,4 +103,3 @@
 
   return ARMBaseInstrInfo::reMaterialize(MBB, I, DestReg, SubIdx, Orig);
 }
-
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
--- a/llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrInfo.td
@@ -321,6 +321,11 @@
   let MIOperandInfo = (ops GPR, i32imm);
 }
 
+
+// ======================================================================
+
+
+
 // addrmode4 := reg, <mode|W>
 //
 def addrmode4 : Operand<i32>,
@@ -568,6 +573,40 @@
            [(ARMcallseq_start timm:$amt)]>;
 }
 
+// @LOCALMOD-START
+def SFI_GUARD_STORE :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary,
+           "sfi_store_preamble $dst, $p",
+           []>;
+
+let Defs = [CPSR] in
+def SFI_GUARD_STORE_TST :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a), NoItinerary,
+           "sfi_cstore_preamble $dst",
+           []>;
+
+def SFI_GUARD_INDIRECT_CALL :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary,
+           "sfi_indirect_call_preamble $dst, $p",
+           []>;
+
+def SFI_GUARD_INDIRECT_JMP :
+PseudoInst<(outs GPR:$dst), (ins GPR:$a, pred:$p), NoItinerary,
+           "sfi_indirect_jump_preamble $dst, $p",
+           []>;
+
+def SFI_GUARD_CALL :
+PseudoInst<(outs), (ins pred:$p), NoItinerary,
+           "sfi_call_preamble $p",
+           []>;
+
+// NOTE: the BX_RET instruction hardcodes lr as well
+def SFI_GUARD_RETURN :
+PseudoInst<(outs), (ins pred:$p), NoItinerary,
+           "sfi_return_preamble lr, $p",
+           []>;
+// @LOCALMOD-END
+
 def DWARF_LOC :
 PseudoInst<(outs), (ins i32imm:$line, i32imm:$col, i32imm:$file), NoItinerary,
            ".loc $file, $line, $col",
@@ -654,7 +693,7 @@
 
 // Indirect branches
 let isBranch = 1, isTerminator = 1, isBarrier = 1, isIndirectBranch = 1 in {
-  def BRIND : AXI<(outs), (ins GPR:$dst), BrMiscFrm, IIC_Br, "bx\t$dst",
+    def BRIND : AXI<(outs), (ins GPR:$dst), BrMiscFrm, IIC_Br, "bx\t$dst",
                   [(brind GPR:$dst)]> {
     let Inst{7-4}   = 0b0001;
     let Inst{19-8}  = 0b111111111111;
@@ -874,6 +913,7 @@
 
 // Store
 def STR  : AI2stw<(outs), (ins GPR:$src, addrmode2:$addr), StFrm, IIC_iStorer,
+// c.f.:  ARMAsmPrinter::printAddrMode2Operand
                "str", "\t$src, $addr",
                [(store GPR:$src, addrmode2:$addr)]>;
 
@@ -948,7 +988,11 @@
 let mayStore = 1, hasExtraSrcRegAllocReq = 1 in
 def STM : AXI4st<(outs),
                (ins addrmode4:$addr, pred:$p, reglist:$wb, variable_ops),
-               LdStMulFrm, IIC_iStorem, "stm${addr:submode}${p}\t$addr, $wb",
+// @LOCALMOD-START
+// c.f. ARMAsmPrinter::printAddrMode4Operand()
+//               LdStMulFrm, IIC_iStorem, "stm${addr:submode}${p}\t$addr, $wb",
+               LdStMulFrm, IIC_iStorem, "sfi_store_preamble ${addr:base}, ${p}\n\tstm${addr:submode}${p}\t$addr, $wb",
+// @LOCALMOD-END
                []>;
 
 //===----------------------------------------------------------------------===//
@@ -1087,6 +1131,17 @@
 defm SUB  : AsI1_bin_irs<0b0010, "sub",
                          BinOpFrag<(sub  node:$LHS, node:$RHS)>>;
 
+
+// @LOCALMOD-START
+defm STACK_ADD  : AsI1_bin_irs<0b0100, "sfi_add",
+                         BinOpFrag<(add  node:$LHS, node:$RHS)>, 1>;
+defm STACK_SUB  : AsI1_bin_irs<0b0010, "sfi_sub",
+                         BinOpFrag<(sub  node:$LHS, node:$RHS)>>;
+let neverHasSideEffects = 1 in
+def STACK_MOVr : AsI1<0b1101, (outs GPR:$dst), (ins GPR:$src), DPFrm, IIC_iMOVr,
+                 "sfi_mov", " $dst, $src", []>, UnaryDP;
+// @LOCALMOD-END
+
 // ADD and SUB with 's' bit set.
 defm ADDS : AI1_bin_s_irs<0b0100, "adds",
                           BinOpFrag<(addc node:$LHS, node:$RHS)>, 1>;
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
--- a/llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
+++ b/llvm-trunk/lib/Target/ARM/ARMInstrVFP.td
@@ -91,6 +91,7 @@
   let Inst{20} = 1;
 }
 } // mayLoad, hasExtraDefRegAllocReq
+
 
 let mayStore = 1, hasExtraSrcRegAllocReq = 1 in {
 def VSTMD : AXDI5<(outs), (ins addrmode5:$addr, pred:$p, reglist:$wb,
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMLoadStoreOptimizer.cpp
@@ -38,6 +38,9 @@
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
 using namespace llvm;
+
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
+extern cl::opt<bool> FlagSfiStore; // @LOCALMOD
 
 STATISTIC(NumLDMGened , "Number of ldm instructions generated");
 STATISTIC(NumSTMGened , "Number of stm instructions generated");
@@ -320,10 +323,12 @@
   unsigned MyPredReg = 0;
   if (!MI)
     return false;
+
   if (MI->getOpcode() != ARM::t2SUBri &&
       MI->getOpcode() != ARM::t2SUBrSPi &&
       MI->getOpcode() != ARM::t2SUBrSPi12 &&
       MI->getOpcode() != ARM::tSUBspi &&
+      MI->getOpcode() != ARM::STACK_SUBri && // @LOCALMOD
       MI->getOpcode() != ARM::SUBri)
     return false;
 
@@ -345,10 +350,12 @@
   unsigned MyPredReg = 0;
   if (!MI)
     return false;
+
   if (MI->getOpcode() != ARM::t2ADDri &&
       MI->getOpcode() != ARM::t2ADDrSPi &&
       MI->getOpcode() != ARM::t2ADDrSPi12 &&
       MI->getOpcode() != ARM::tADDspi &&
+      MI->getOpcode() != ARM::STACK_ADDri && // @LOCALMOD
       MI->getOpcode() != ARM::ADDri)
     return false;
 
@@ -404,6 +411,7 @@
 /// ldmia rn, <ra, rb, rc>
 /// =>
 /// ldmdb rn!, <ra, rb, rc>
+/// @LOCALMOD This is especially useful for rn == sp
 bool ARMLoadStoreOpt::MergeBaseUpdateLSMultiple(MachineBasicBlock &MBB,
                                                MachineBasicBlock::iterator MBBI,
                                                bool &Advance,
@@ -1040,6 +1048,7 @@
   return NumMerges > 0;
 }
 
+
 namespace {
   struct OffsetCompare {
     bool operator()(const MachineInstr *LHS, const MachineInstr *RHS) const {
@@ -1058,7 +1067,12 @@
 ///   bx lr
 /// =>
 ///   ldmfd sp!, {r7, pc}
+// @LOCALMOD for sfi we do not want this to happen
 bool ARMLoadStoreOpt::MergeReturnIntoLDM(MachineBasicBlock &MBB) {
+// @LOCALMOD-START
+  return false;
+  // @LOCALMOD-END
+
   if (MBB.empty()) return false;
 
   MachineBasicBlock::iterator MBBI = prior(MBB.end());
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSFIPlacement.cpp
--- /dev/null
+++ b/llvm-trunk/lib/Target/ARM/ARMSFIPlacement.cpp
@@ -0,0 +1,384 @@
+//===-- ARMSFIPlacement.cpp - Place SFI mask instructions ---------*- C++ -*-=//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a pass that places mask instructions ahead of all stores.
+// This must be run as late in the game as possible -- after all scheduling and
+// constant island placement.  (This is set up in ARMTargetMachine.cpp.)
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "arm-pseudo"
+#include "ARM.h"
+#include "ARMBaseInstrInfo.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/Support/CommandLine.h"
+
+#include <set>
+#include <stdio.h>
+
+using namespace llvm;
+
+cl::opt<bool> FlagSfiStore("sfi-store",
+                           cl::desc("enable sandboxing for stores"));
+
+cl::opt<bool> FlagSfiStack("sfi-stack",
+                           cl::desc("enable sandboxing for stack changes"));
+
+cl::opt<bool> FlagSfiBranch("sfi-branch",
+                            cl::desc("enable sandboxing for branches"));
+
+namespace {
+  class ARMSFIPlacement : public MachineFunctionPass {
+  public:
+    static char ID;
+    ARMSFIPlacement() : MachineFunctionPass(&ID) {}
+
+    const TargetInstrInfo *TII;
+
+    virtual void getAnalysisUsage(AnalysisUsage &AU) const;
+    virtual bool runOnMachineFunction(MachineFunction &Fn);
+
+    virtual const char *getPassName() const {
+      return "ARM SFI mask placement";
+    }
+
+  private:
+    bool PlaceMBB(MachineBasicBlock &MBB);
+    bool PassStoreSandboxing(MachineBasicBlock &MBB);
+    bool PassBranchSandboxing(MachineBasicBlock &MBB);
+    void SandboxStore(MachineBasicBlock &MBB,
+                      MachineBasicBlock::iterator MBBI,
+                      MachineInstr &MI,
+                      int AddrIdx,
+                      bool CPSRLive);
+    bool TryPredicating(MachineInstr &MI, ARMCC::CondCodes);
+  };
+  char ARMSFIPlacement::ID = 0;
+}
+
+static ARMCC::CondCodes GetPredicate(MachineInstr &MI) {
+  int PIdx = MI.findFirstPredOperandIdx();
+  if (PIdx != -1) {
+    return (ARMCC::CondCodes)MI.getOperand(PIdx).getImm();
+  } else {
+    return ARMCC::AL;
+  }
+}
+
+void ARMSFIPlacement::getAnalysisUsage(AnalysisUsage &AU) const {
+  // Slight (possibly unnecessary) efficiency tweak:
+  // Promise not to modify the CFG.
+  AU.setPreservesCFG();
+  MachineFunctionPass::getAnalysisUsage(AU);
+}
+
+bool ARMSFIPlacement::TryPredicating(MachineInstr &MI, ARMCC::CondCodes Pred) {
+  // Can't predicate if it's already predicated.
+  // TODO(cbiffle): actually we can, if the conditions match.
+  if (TII->isPredicated(&MI)) return false;
+
+  /*
+   * ARM predicate operands use two actual MachineOperands: an immediate
+   * holding the predicate condition, and a register referencing the flags.
+   */
+  SmallVector<MachineOperand, 2> PredOperands;
+  PredOperands.push_back(MachineOperand::CreateImm((int64_t) Pred));
+  PredOperands.push_back(MachineOperand::CreateReg(ARM::CPSR, false));
+
+  // This attempts to rewrite, but some instructions can't be predicated.
+  return TII->PredicateInstruction(&MI, PredOperands);
+}
+
+/*
+ * Sandboxes a store instruction by inserting an appropriate mask or check
+ * operation before it.
+ */
+void ARMSFIPlacement::SandboxStore(MachineBasicBlock &MBB,
+                                   MachineBasicBlock::iterator MBBI,
+                                   MachineInstr &MI,
+                                   int AddrIdx,
+                                   bool CPSRLive) {
+  MachineOperand &Addr = MI.getOperand(AddrIdx);
+
+  if (!CPSRLive && TryPredicating(MI, ARMCC::EQ)) {
+    /*
+     * For unconditional stores where CPSR is not in use, we can use a faster
+     * sandboxing sequence by predicating the store -- assuming we *can*
+     * predicate the store.
+     */
+
+    // Instruction can be predicated -- use the new sandbox.
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFI_GUARD_STORE_TST))
+      .addOperand(Addr)   // rD
+      .addReg(0);         // apparently unused source register?
+  } else {
+    // Use the older BIC sandbox, which is universal, but incurs a stall.
+    ARMCC::CondCodes Pred = GetPredicate(MI);
+    BuildMI(MBB, MBBI, MI.getDebugLoc(),
+            TII->get(ARM::SFI_GUARD_STORE))
+      .addOperand(Addr)        // rD
+      .addReg(0)               // apparently unused source register?
+      .addImm((int64_t) Pred)  // predicate condition
+      .addReg(ARM::CPSR);      // predicate source register (CPSR)
+
+    /*
+     * This pseudo-instruction is intended to generate something resembling the
+     * following, but with alignment enforced.
+     * TODO(cbiffle): move alignment into this function, use the code below.
+     *
+     *  // bic<cc> Addr, Addr, #0xC0000000
+     *  BuildMI(MBB, MBBI, MI.getDebugLoc(),
+     *          TII->get(ARM::BICri))
+     *    .addOperand(Addr)        // rD
+     *    .addOperand(Addr)        // rN
+     *    .addImm(0xC0000000)      // imm
+     *    .addImm((int64_t) Pred)  // predicate condition
+     *    .addReg(ARM::CPSR)       // predicate source register (CPSR)
+     *    .addReg(0);              // flag output register (0 == no flags)
+     */
+  }
+}
+
+
+static bool IsDirectCall(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   default: return false;
+
+   case ARM::BLr9:
+   case ARM::BLr9_pred:
+    assert(0 && "This should not have happend. We do not support usage of r9.");
+    return true;
+
+   case ARM::BL:
+   case ARM::BL_pred:
+   case ARM::TPsoft:
+    return true;
+  }
+}
+
+
+static bool IsIndirectCall(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   default: return false;
+
+   case ARM::BLXr9:
+    assert(0 && "we do not support usage of r9");
+
+   case ARM::BLX:
+    return true;
+  }
+}
+
+
+static bool IsIndirectJump(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   default: return false;
+
+   case ARM::BRIND:
+    return true;
+  }
+}
+
+
+static bool IsReturn(const MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+   default: return false;
+
+   case ARM::BX_RET:
+    return true;
+  }
+}
+
+
+
+
+static bool IsDangerousStore(const MachineInstr &MI, int *AddrIdx) {
+  unsigned Opcode = MI.getOpcode();
+  switch (Opcode) {
+  default: return false;
+
+  // Instructions with base address register in position 0...
+  case ARM::VSTMD:
+  case ARM::VSTMS:
+    *AddrIdx = 0;
+    break;
+
+  // Instructions with base address register in position 1...
+  case ARM::STR:
+  case ARM::STRB:
+  case ARM::STRH:
+  case ARM::VSTRS:
+  case ARM::VSTRD:
+    *AddrIdx = 1;
+    break;
+
+  // Instructions with base address register in position 2...
+  case ARM::STR_PRE:
+  case ARM::STR_POST:
+  case ARM::STRB_PRE:
+  case ARM::STRB_POST:
+  case ARM::STRH_PRE:
+  case ARM::STRH_POST:
+  case ARM::STRD:
+    *AddrIdx = 2;
+    break;
+  }
+
+  if (MI.getOperand(*AddrIdx).getReg() == ARM::SP) {
+    // The contents of SP do not require masking.
+    return false;
+  }
+
+  return true;
+}
+
+static bool IsCPSRLiveOut(const MachineBasicBlock &MBB) {
+  // CPSR is live-out if any successor lists it as live-in.
+  for (MachineBasicBlock::const_succ_iterator SI = MBB.succ_begin(),
+                                              E = MBB.succ_end();
+       SI != E;
+       ++SI) {
+    const MachineBasicBlock *Succ = *SI;
+    if (Succ->isLiveIn(ARM::CPSR)) return true;
+  }
+  return false;
+}
+
+bool ARMSFIPlacement::PassStoreSandboxing(MachineBasicBlock &MBB) {
+  /*
+   * This is a simple local reverse-dataflow analysis to determine where CPSR
+   * is live.  We cannot use the conditional store sequence anywhere that CPSR
+   * is live, or we'd affect correctness.  The existing liveness analysis passes
+   * barf when applied pre-emit, after allocation, so we must do it ourselves.
+   */
+
+  bool CPSRLive = IsCPSRLiveOut(MBB);
+
+  // Given that, record which instructions should not be altered to trash CPSR:
+  std::set<const MachineInstr *> InstrsWhereCPSRLives;
+  for (MachineBasicBlock::const_reverse_iterator MBBI = MBB.rbegin(),
+                                                 E = MBB.rend();
+       MBBI != E;
+       ++MBBI) {
+    const MachineInstr &MI = *MBBI;
+    // Check for kills first.
+    if (MI.modifiesRegister(ARM::CPSR)) CPSRLive = false;
+    // Then check for uses.
+    if (MI.readsRegister(ARM::CPSR)) CPSRLive = true;
+
+    if (CPSRLive) InstrsWhereCPSRLives.insert(&MI);
+  }
+
+  // Sanity check:
+  assert(CPSRLive == MBB.isLiveIn(ARM::CPSR)
+         && "CPSR Liveness analysis does not match cached live-in result.");
+
+  // Now: find and sandbox stores.
+  bool Modified = false;
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+    int AddrIdx;
+
+    if (IsDangerousStore(MI, &AddrIdx)) {
+      bool CPSRLive =
+        (InstrsWhereCPSRLives.find(&MI) != InstrsWhereCPSRLives.end());
+      SandboxStore(MBB, MBBI, MI, AddrIdx, CPSRLive);
+      Modified = true;
+    }
+  }
+
+  return Modified;
+}
+
+bool ARMSFIPlacement::PassBranchSandboxing(MachineBasicBlock &MBB) {
+  bool Modified = false;
+
+  for (MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+       MBBI != E;
+       ++MBBI) {
+    MachineInstr &MI = *MBBI;
+
+    if (IsReturn(MI)) {
+      ARMCC::CondCodes Pred = GetPredicate(MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_RETURN))
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsIndirectJump(MI)) {
+      MachineOperand &Addr = MI.getOperand(0);
+      ARMCC::CondCodes Pred = GetPredicate(MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_INDIRECT_JMP))
+        .addOperand(Addr)        // rD
+        .addReg(0)               // apparently unused source register?
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsDirectCall(MI)) {
+      ARMCC::CondCodes Pred = GetPredicate(MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_CALL))
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+      Modified = true;
+    }
+
+    if (IsIndirectCall(MI)) {
+      MachineOperand &Addr = MI.getOperand(0);
+      ARMCC::CondCodes Pred = GetPredicate(MI);
+      BuildMI(MBB, MBBI, MI.getDebugLoc(),
+              TII->get(ARM::SFI_GUARD_INDIRECT_CALL))
+        .addOperand(Addr)        // rD
+        .addReg(0)               // apparently unused source register?
+        .addImm((int64_t) Pred)  // predicate condition
+        .addReg(ARM::CPSR);      // predicate source register (CPSR)
+        Modified = true;
+    }
+  }
+
+  return Modified;
+}
+
+bool ARMSFIPlacement::PlaceMBB(MachineBasicBlock &MBB) {
+  bool Modified = false;
+  if (FlagSfiStore) {
+    Modified |= PassStoreSandboxing(MBB);
+  }
+
+  if (FlagSfiBranch) {
+    Modified |= PassBranchSandboxing(MBB);
+  }
+  return Modified;
+}
+
+
+bool ARMSFIPlacement::runOnMachineFunction(MachineFunction &MF) {
+  TII = MF.getTarget().getInstrInfo();
+
+  bool Modified = false;
+  for (MachineFunction::iterator MFI = MF.begin(), E = MF.end();
+       MFI != E;
+       ++MFI)
+    Modified |= PlaceMBB(*MFI);
+  return Modified;
+}
+
+/// createARMSFIPlacementPass - returns an instance of the SFI placement pass.
+FunctionPass *llvm::createARMSFIPlacementPass() {
+  return new ARMSFIPlacement();
+}
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMSubtarget.cpp
@@ -19,13 +19,24 @@
 #include "llvm/ADT/SmallVector.h"
 using namespace llvm;
 
-static cl::opt<bool>
+cl::opt<bool>  // @LOCALMOD
 ReserveR9("arm-reserve-r9", cl::Hidden,
           cl::desc("Reserve R9, making it unavailable as GPR"));
 static cl::opt<bool>
 UseNEONFP("arm-use-neon-fp",
           cl::desc("Use NEON for single-precision FP"),
           cl::init(false), cl::Hidden);
+
+// @LOCALMOD-START
+// TODO: * This does not currently work as expected for PIC mode:
+//         It does work, but the table still ends up in the .text section.
+//       * JITing has not been tested at all
+//       * Thumb mode operation is also not clear: it seems jump tables
+//         for thumb are broken independent of this option
+static cl::opt<bool>
+NoInlineJumpTables("no-inline-jumptables",
+		  cl::desc("Do not place jump tables inline in the code"));
+// @LOCALMOD-END
 
 ARMSubtarget::ARMSubtarget(const std::string &TT, const std::string &FS,
                            bool isT)
@@ -36,6 +47,7 @@
   , ThumbMode(Thumb1)
   , PostRAScheduler(false)
   , IsR9Reserved(ReserveR9)
+  , UseInlineJumpTables(!NoInlineJumpTables) // @LOCAMOD
   , stackAlignment(4)
   , CPUString("generic")
   , TargetType(isELF) // Default to ELF unless otherwise specified.
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMSubtarget.h
--- a/llvm-trunk/lib/Target/ARM/ARMSubtarget.h
+++ b/llvm-trunk/lib/Target/ARM/ARMSubtarget.h
@@ -61,6 +61,11 @@
 
   /// IsR9Reserved - True if R9 is a not available as general purpose register.
   bool IsR9Reserved;
+
+  // @LOCALMOD-START
+  /// UseInlineJumpTables - True if jump tables should be in-line in the code.
+  bool UseInlineJumpTables;
+  // @LOCALMOD-END
 
   /// stackAlignment - The minimum alignment known to hold of the stack frame on
   /// entry to the function and which must be maintained by every function.
@@ -144,6 +149,9 @@
   /// GVIsIndirectSymbol - true if the GV will be accessed via an indirect
   /// symbol.
   bool GVIsIndirectSymbol(GlobalValue *GV, Reloc::Model RelocM) const;
+
+  // @LOCALMOD
+  bool useInlineJumpTables() const {return UseInlineJumpTables;}
 };
 } // End llvm namespace
 
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
--- a/llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
+++ b/llvm-trunk/lib/Target/ARM/ARMTargetMachine.cpp
@@ -19,7 +19,13 @@
 #include "llvm/Support/FormattedStream.h"
 #include "llvm/Target/TargetOptions.h"
 #include "llvm/Target/TargetRegistry.h"
+#include "llvm/Support/CommandLine.h" // @LOCALMOD
 using namespace llvm;
+
+// @LOCALMOD-START
+extern cl::opt<bool> FlagSfiStore;
+extern cl::opt<bool> FlagSfiBranch;
+// @LOCALMOD-END
 
 static const MCAsmInfo *createMCAsmInfo(const Target &T, StringRef TT) {
   Triple TheTriple(TT);
@@ -128,6 +134,13 @@
   }
 
   PM.add(createARMConstantIslandPass());
+
+  // @LOCALMOD-START
+  if (FlagSfiStore || FlagSfiBranch) {
+    PM.add(createARMSFIPlacementPass());
+  }
+  // @LOCALMOD-END
+
   return true;
 }
 
@@ -190,4 +203,3 @@
   PM.add(createARMObjectCodeEmitterPass(*this, OCE));
   return false;
 }
-
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMAsmPrinter.cpp
@@ -51,7 +51,22 @@
 #include "llvm/Support/Mangler.h"
 #include "llvm/Support/MathExtras.h"
 #include <cctype>
+#include <sstream>
+
 using namespace llvm;
+
+// @LOCALMOD
+cl::opt<bool> FlagSfiZeroMask("sfi-zero-mask");
+
+extern cl::opt<bool> FlagSfiStore;
+extern cl::opt<bool> FlagSfiStack;
+extern cl::opt<bool> FlagSfiBranch;
+cl::opt<bool> FlagSfiData("sfi-data",
+                        cl::desc("use illegal at data bundle beginning"));
+// @LOCALMOD-END
+
+
+// @LOCALMOD
 
 STATISTIC(EmittedInsts, "Number of machine instrs printed");
 
@@ -94,10 +109,12 @@
 
     void printOperand(const MachineInstr *MI, int OpNum,
                       const char *Modifier = 0);
+
     void printSOImmOperand(const MachineInstr *MI, int OpNum);
     void printSOImm2PartOperand(const MachineInstr *MI, int OpNum);
     void printSORegOperand(const MachineInstr *MI, int OpNum);
     void printAddrMode2Operand(const MachineInstr *MI, int OpNum);
+
     void printAddrMode2OffsetOperand(const MachineInstr *MI, int OpNum);
     void printAddrMode3Operand(const MachineInstr *MI, int OpNum);
     void printAddrMode3OffsetOperand(const MachineInstr *MI, int OpNum);
@@ -170,6 +187,7 @@
     /// EmitMachineConstantPoolValue - Print a machine constantpool value to
     /// the .s file.
     virtual void EmitMachineConstantPoolValue(MachineConstantPoolValue *MCPV) {
+      // NOTE: A lot of this code is replicated in  ARMConstantPoolValue::print
       printDataDirective(MCPV->getType());
 
       ARMConstantPoolValue *ACPV = static_cast<ARMConstantPoolValue*>(MCPV);
@@ -180,6 +198,10 @@
         raw_svector_ostream(LSDAName) << MAI->getPrivateGlobalPrefix() <<
           "_LSDA_" << getFunctionNumber();
         Name = LSDAName.str();
+      } else if (ACPV->isJumpTable()) {
+	Name = std::string(MAI->getPrivateGlobalPrefix()) + "JTI" +
+               utostr(getFunctionNumber()) + '_' +
+               utostr(*ACPV->getJumpTableIndex());
       } else if (ACPV->isBlockAddress()) {
         Name = GetBlockAddressSymbol(ACPV->getBlockAddress())->getName();
       } else if (ACPV->isGlobalValue()) {
@@ -231,6 +253,7 @@
   };
 } // end of anonymous namespace
 
+
 #include "ARMGenAsmWriter.inc"
 
 /// runOnMachineFunction - This uses the printInstruction()
@@ -247,7 +270,6 @@
 
   // NOTE: we don't print out constant pools here, they are handled as
   // instructions.
-
   O << '\n';
 
   // Print out labels for the function.
@@ -287,7 +309,15 @@
       O << "\t" << CurrentFnName;
     O << "\n";
   } else {
-    EmitAlignment(FnAlign, F);
+    // @LOCALMOD-START
+    // EmitAlignment(FnAlign, F);
+    // make sure function entry is aligned. We use  XmagicX as our basis
+    // for alignment decisions (c.f. assembler sfi macros)
+    int alignment = MF.getAlignment();
+    if (alignment < 4) alignment = 4;
+    EmitAlignment(alignment, F);
+    O << "\t.set XmagicX, .\n";
+    // @LOCALMOD-END
   }
 
   O << CurrentFnName << ":\n";
@@ -304,6 +334,46 @@
       O << "\tnop\n";
   }
 
+  // @LOCALMOD-START
+  // Make sure all jump targets are aligned
+  // and also all constant pools
+  if (FlagSfiBranch) {
+    // JUMP TABLE TARGETS
+    MachineJumpTableInfo *jt_info = MF.getJumpTableInfo();
+    const std::vector<MachineJumpTableEntry> &JT = jt_info->getJumpTables();
+    for (unsigned i=0; i < JT.size(); ++i) {
+      std::vector<MachineBasicBlock*> MBBs = JT[i].MBBs;
+
+      //cout << "JUMPTABLE "<< i << " " << MBBs.size() << "\n";
+      for (unsigned j=0; j < MBBs.size(); ++j) {
+	if (MBBs[j]->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY) {
+	  continue;
+	}
+        MBBs[j]->setAlignment(16);
+      }
+    }
+
+    // FIRST ENTRY IN A ConstanPool
+    bool last_bb_was_constant_pool = false;
+    for (MachineFunction::iterator I = MF.begin(), E = MF.end();
+         I != E; ++I) {
+      if (I->isLandingPad()) {
+        I->setAlignment(16);
+      }
+
+      if (I->empty()) continue;
+
+      bool is_constant_pool = I->begin()->getOpcode() == ARM::CONSTPOOL_ENTRY;
+
+      if (last_bb_was_constant_pool != is_constant_pool) {
+        I->setAlignment(16);
+      }
+
+      last_bb_was_constant_pool = is_constant_pool;
+    }
+  }
+  // @LOCALMOD-END
+
   // Print out code for the function.
   for (MachineFunction::const_iterator I = MF.begin(), E = MF.end();
        I != E; ++I) {
@@ -319,6 +389,12 @@
 
   if (MAI->hasDotTypeDotSizeDirective())
     O << "\t.size " << CurrentFnName << ", .-" << CurrentFnName << "\n";
+
+  // @LOCALMOD-START
+  // Print out jump tables referenced by the function.
+  if (!Subtarget->useInlineJumpTables())
+    EmitJumpTableInfo(MF.getJumpTableInfo(), MF);
+  // @LOCALMOD-END
 
   // Emit post-function debug information.
   DW->EndFunction(&MF);
@@ -588,6 +664,10 @@
     ARM_AM::AMSubMode Mode = ARM_AM::getAM4SubMode(MO2.getImm());
     if (Mode == ARM_AM::ia)
       O << ".w";
+  // @LOCALMOD-START
+  } else if (Modifier && strcmp(Modifier, "base") == 0) {
+    printOperand(MI, Op);
+    // @LOCALMOD-END
   } else {
     printOperand(MI, Op);
     if (ARM_AM::getAM4WBFlag(MO2.getImm()))
@@ -618,6 +698,14 @@
       O << "!";
     return;
   }
+  //@LOCALMOD-START
+   else if (Modifier && strcmp(Modifier, "basereg") == 0) {
+     O << getRegisterName(MO1.getReg());
+     return;
+   }
+  //@LOCALMOD-END
+
+
 
   O << "[" << getRegisterName(MO1.getReg());
 
@@ -880,7 +968,25 @@
   assert(Modifier && "This operand only works with a modifier!");
   // There are two aspects to a CONSTANTPOOL_ENTRY operand, the label and the
   // data itself.
+
   if (!strcmp(Modifier, "label")) {
+    // @LOCALMOD-START
+    // NOTE: we also should make sure that the first data item
+    // is not in a code bundle
+    // NOTE: there may be issues with alignment constraints
+    const unsigned size = MI->getOperand(2).getImm();
+    //assert(size == 4 || size == 8 && "Unsupported data item size");
+    if (size == 8) {
+      // we cannot generate a size 8 constant at offset 12 (mod 16)
+      O << "sfi_nop_if_at_bundle_end\n";
+    }
+
+    if (FlagSfiData) {
+      O << "sfi_illegal_if_at_bundle_begining  @ ========== SFI (" <<
+        size << ")\n";
+    }
+    // @LOCALMOD-END
+
     unsigned ID = MI->getOperand(OpNum).getImm();
     O << MAI->getPrivateGlobalPrefix() << "CPI" << getFunctionNumber()
       << '_' << ID << ":\n";
@@ -891,6 +997,9 @@
     const MachineConstantPoolEntry &MCPE = MCP->getConstants()[CPI];
 
     if (MCPE.isMachineConstantPoolEntry()) {
+      // @LOCALMOD-START
+      // O << "@ Const pool\n";
+      // @LOCALMOD-END
       EmitMachineConstantPoolValue(MCPE.Val.MachineCPVal);
     } else {
       EmitGlobalConstant(MCPE.Val.ConstVal);
@@ -915,6 +1024,7 @@
   const std::vector<MachineBasicBlock*> &JTBBs = JT[JTI].MBBs;
   bool UseSet= MAI->getSetDirective() && TM.getRelocationModel() == Reloc::PIC_;
   SmallPtrSet<MachineBasicBlock*, 8> JTSets;
+
   for (unsigned i = 0, e = JTBBs.size(); i != e; ++i) {
     MachineBasicBlock *MBB = JTBBs[i];
     bool isNew = JTSets.insert(MBB);
@@ -1158,6 +1268,235 @@
 
     // FIXME: Should we signal R9 usage?
   }
+
+  O << " @ ========================================\n";
+  O << "@ Branch: " << FlagSfiBranch << "\n";
+  O << "@ Stack: " << FlagSfiStack << "\n";
+  O << "@ Store: " << FlagSfiStore << "\n";
+  O << "@ Data: " << FlagSfiData << "\n";
+
+  O << " @ ========================================\n";
+  // NOTE: this macro does bundle alignment as follows
+  //       if current bundle pos is X emit pX data items of value "val"
+  // NOTE: that pos will be one of: 0,4,8,12
+  //
+  O <<
+    "\t.macro sfi_long_based_on_pos p0 p1 p2 p3 val\n"
+    "\t.set pos, (. - XmagicX) % 16\n"
+    "\t.fill  (((\\p3<<12)|(\\p2<<8)|(\\p1<<4)|\\p0)>>pos) & 15, 4, \\val\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_illegal_if_at_bundle_begining\n"
+    "\tsfi_long_based_on_pos 1 0 0 0 0xe1277777\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nop_if_at_bundle_end\n"
+    "\tsfi_long_based_on_pos 0 0 0 1 0xe1a00000\n"
+    "\t.endm\n"
+      "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot3\n"
+    "\tsfi_long_based_on_pos 3 2 1 0 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot2\n"
+    "\tsfi_long_based_on_pos 2 1 0 3 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O <<
+    "\t.macro sfi_nops_to_force_slot1\n"
+    "\tsfi_long_based_on_pos 1 0 3 2 0xe1a00000\n"
+    "\t.endm\n"
+    "\n\n";
+
+  O << " @ ========================================\n";
+  if (FlagSfiZeroMask) {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0\n"
+      "\t.endm\n"
+      "\n\n";
+
+  } else {
+    O <<
+      "\t.macro sfi_data_mask reg cond\n"
+      "\tbic\\cond \\reg, \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_data_tst reg\n"
+      "\ttst \\reg, #0xc0000000\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_code_mask reg cond=\n"
+      "\tbic\\cond \\reg, \\reg, #0xc000000f\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  O << " @ ========================================\n";
+  if (FlagSfiBranch) {
+    O <<
+      "\t.macro sfi_call_preamble cond=\n"
+      "\tsfi_nops_to_force_slot3\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_return_preamble reg cond=\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_code_mask \\reg \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+    
+    // This is used just before "bx rx"
+    O <<
+      "\t.macro sfi_indirect_jump_preamble link cond=\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_code_mask \\link \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    // This is use just before "blx rx"
+    O <<
+      "\t.macro sfi_indirect_call_preamble link cond=\n"
+      "\tsfi_nops_to_force_slot2\n"
+      "\tsfi_code_mask \\link \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+  }
+
+  if (FlagSfiStore) {
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_mask \\reg, \\cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsfi_data_tst \\reg\n"
+      "\t.endm\n"
+      "\n\n";
+  } else {
+    O <<
+      "\t.macro sfi_store_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+
+    O <<
+      "\t.macro sfi_cstore_preamble reg cond\n"
+      "\t.endm\n"
+      "\n\n";
+  }
+
+  const char* kPreds[] = {
+    "eq",
+    "ne",
+    "lt",
+    "le",
+    "ls",
+    "ge",
+    "gt",
+    "hs",
+    "hi",
+    "lo",
+    "mi",
+    "pl",
+    NULL,
+  };
+
+  if (FlagSfiStack) {
+
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_add rega regb imm rot=0\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tadd \\rega, \\regb, \\imm, \\rot\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++p) {
+      O <<
+        "\t.macro sfi_add" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tadd" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+    O << " @ ========================================\n";
+    O <<
+      "\t.macro sfi_sub rega regb imm rot=0\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tsub \\rega, \\regb, \\imm, \\rot\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++ p) {
+      O <<
+        "\t.macro sfi_sub" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tsub" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+    O << " @ ========================================\n";
+
+    O <<
+      "\t.macro sfi_mov rega regb\n"
+      "\tsfi_nop_if_at_bundle_end\n"
+      "\tmov \\rega, \\regb\n"
+      "\tsfi_data_mask \\rega\n"
+      "\t.endm\n"
+      "\n\n";
+
+    for (int p=0; kPreds[p] != NULL; ++ p) {
+      O <<
+        "\t.macro mov_sub" << kPreds[p] << " rega regb imm rot=0\n"
+        "\tsfi_nop_if_at_bundle_end\n"
+        "\tmov" << kPreds[p] << " \\rega, \\regb, \\imm, \\rot\n"
+        "\tsfi_data_mask \\rega, " << kPreds[p] << "\n"
+        "\t.endm\n"
+        "\n\n";
+    }
+
+  } // FlagSfiStack
+  
+  O << " @ ========================================\n";
+  O << "\t.text\n";
+
+
+  // @LOCALMOD-END
+
 }
 
 void ARMAsmPrinter::PrintGlobalVariable(const GlobalVariable* GVar) {
@@ -1345,6 +1684,7 @@
     // generates code that does this, it is always safe to set.
     OutStreamer.EmitAssemblerFlag(MCStreamer::SubsectionsViaSymbols);
   }
+
 }
 
 //===----------------------------------------------------------------------===//
@@ -1519,4 +1859,3 @@
   TargetRegistry::RegisterMCInstPrinter(TheARMTarget, createARMMCInstPrinter);
   TargetRegistry::RegisterMCInstPrinter(TheThumbTarget, createARMMCInstPrinter);
 }
-
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.cpp
@@ -18,8 +18,15 @@
 #include "llvm/MC/MCInst.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCExpr.h"
+#include "llvm/Support/CommandLine.h"  // @LOCALMOD
 #include "llvm/Support/raw_ostream.h"
+
 using namespace llvm;
+
+// @LOCALMOD
+extern cl::opt<bool> FlagSfiStore;
+// @LOCALMOD
+
 
 // Include the auto-generated portion of the assembly writer.
 #define MachineInstr MCInst
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
--- a/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
+++ b/llvm-trunk/lib/Target/ARM/AsmPrinter/ARMInstPrinter.h
@@ -40,8 +40,19 @@
   
   void printSORegOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode2Operand(const MCInst *MI, unsigned OpNum);
+
+  // @LOCALMOD-START
+  // NOTE: UNTESTED - but also not used
+  void printAddrModeHelper(const MCInst *MI, int base, int pred, const char* Modifier);
+  void printAddrMode2Operand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode3Operand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode2OffsetOperand(const MCInst *MI, unsigned OpNum, const char* cp);
+  void printAddrMode3OffsetOperand(const MCInst *MI, unsigned OpNum, const char* cp);
+  // @LOCALMOD-END
   void printAddrMode2OffsetOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode3Operand(const MCInst *MI, unsigned OpNum);
+
+
   void printAddrMode3OffsetOperand(const MCInst *MI, unsigned OpNum);
   void printAddrMode4Operand(const MCInst *MI, unsigned OpNum,
                              const char *Modifier = 0);
diff -r c06933b0e3e6 llvm-trunk/lib/Target/ARM/CMakeLists.txt
--- a/llvm-trunk/lib/Target/ARM/CMakeLists.txt
+++ b/llvm-trunk/lib/Target/ARM/CMakeLists.txt
@@ -25,6 +25,7 @@
   ARMLoadStoreOptimizer.cpp
   ARMMCAsmInfo.cpp
   ARMRegisterInfo.cpp
+  ARMSFIPlacement.cpp
   ARMSubtarget.cpp
   ARMTargetMachine.cpp
   NEONMoveFix.cpp
diff -r c06933b0e3e6 llvm-trunk/tools/gold/Makefile
--- a/llvm-trunk/tools/gold/Makefile
+++ b/llvm-trunk/tools/gold/Makefile
@@ -8,7 +8,7 @@
 ##===----------------------------------------------------------------------===##
 
 LEVEL = ../..
-LIBRARYNAME = libLLVMgold
+LIBRARYNAME = LLVMgold
 
 # Include this here so we can get the configuration of the targets
 # that have been configured for construction. We have to do this 
diff -r c06933b0e3e6 llvm-trunk/tools/gold/gold-plugin.cpp
--- a/llvm-trunk/tools/gold/gold-plugin.cpp
+++ b/llvm-trunk/tools/gold/gold-plugin.cpp
@@ -53,45 +53,63 @@
   };
 
   lto_codegen_model output_type = LTO_CODEGEN_PIC_MODEL_STATIC;
+  std::string output_name = "";
   std::list<claimed_file> Modules;
   std::vector<sys::Path> Cleanup;
 }
 
 namespace options {
-  bool generate_api_file = false;
-  const char *as_path = NULL;
+  enum generate_bc { BC_NO, BC_ALSO, BC_ONLY };
+  static bool generate_api_file = false;
+  static generate_bc generate_bc_file = BC_NO;
+  static std::string bc_path;
+  static std::string as_path;
   // Additional options to pass into the code generator.
-  // Note: This array will contain all plugin options which are not claimed 
+  // Note: This array will contain all plugin options which are not claimed
   // as plugin exclusive to pass to the code generator.
-  // For example, "generate-api-file" and "as"options are for the plugin 
+  // For example, "generate-api-file" and "as"options are for the plugin
   // use only and will not be passed.
-  std::vector<std::string> extra;
+  static std::vector<std::string> extra;
 
-  void process_plugin_option(const char* opt)
+  static void process_plugin_option(const char* opt_)
   {
-    if (opt == NULL)
+    if (opt_ == NULL)
       return;
+    llvm::StringRef opt = opt_;
 
-    if (strcmp("generate-api-file", opt) == 0) {
+    if (opt == "generate-api-file") {
       generate_api_file = true;
-    } else if (strncmp("as=", opt, 3) == 0) {
-      if (as_path) {
+    } else if (opt.startswith("as=")) {
+      if (!as_path.empty()) {
         (*message)(LDPL_WARNING, "Path to as specified twice. "
-                   "Discarding %s", opt);
+                   "Discarding %s", opt_);
       } else {
-        as_path = strdup(opt + 3);
+        as_path = opt.substr(strlen("as="));
+      }
+    } else if (opt == "emit-llvm") {
+      generate_bc_file = BC_ONLY;
+    } else if (opt == "also-emit-llvm") {
+      generate_bc_file = BC_ALSO;
+    } else if (opt.startswith("also-emit-llvm=")) {
+      llvm::StringRef path = opt.substr(strlen("also-emit-llvm="));
+      generate_bc_file = BC_ALSO;
+      if (!bc_path.empty()) {
+        (*message)(LDPL_WARNING, "Path to the output IL file specified twice. "
+                   "Discarding %s", opt_);
+      } else {
+        bc_path = path;
       }
     } else {
       // Save this option to pass to the code generator.
-      extra.push_back(std::string(opt));
+      extra.push_back(opt);
     }
   }
 }
 
-ld_plugin_status claim_file_hook(const ld_plugin_input_file *file,
-                                 int *claimed);
-ld_plugin_status all_symbols_read_hook(void);
-ld_plugin_status cleanup_hook(void);
+static ld_plugin_status claim_file_hook(const ld_plugin_input_file *file,
+                                        int *claimed);
+static ld_plugin_status all_symbols_read_hook(void);
+static ld_plugin_status cleanup_hook(void);
 
 extern "C" ld_plugin_status onload(ld_plugin_tv *tv);
 ld_plugin_status onload(ld_plugin_tv *tv) {
@@ -112,6 +130,9 @@
         break;
       case LDPT_GOLD_VERSION:  // major * 100 + minor
         gold_version = tv->tv_u.tv_val;
+        break;
+      case LDPT_OUTPUT_NAME:
+        output_name = tv->tv_u.tv_string;
         break;
       case LDPT_LINKER_OUTPUT:
         switch (tv->tv_u.tv_val) {
@@ -192,15 +213,15 @@
 /// claim_file_hook - called by gold to see whether this file is one that
 /// our plugin can handle. We'll try to open it and register all the symbols
 /// with add_symbol if possible.
-ld_plugin_status claim_file_hook(const ld_plugin_input_file *file,
-                                 int *claimed) {
+static ld_plugin_status claim_file_hook(const ld_plugin_input_file *file,
+                                        int *claimed) {
   void *buf = NULL;
   if (file->offset) {
     // Gold has found what might be IR part-way inside of a file, such as
     // an .a archive.
     if (lseek(file->fd, file->offset, SEEK_SET) == -1) {
       (*message)(LDPL_ERROR,
-                 "Failed to seek to archive member of %s at offset %d: %s\n", 
+                 "Failed to seek to archive member of %s at offset %d: %s\n",
                  file->name,
                  file->offset, sys::StrError(errno).c_str());
       return LDPS_ERR;
@@ -208,7 +229,7 @@
     buf = malloc(file->filesize);
     if (!buf) {
       (*message)(LDPL_ERROR,
-                 "Failed to allocate buffer for archive member of size: %d\n", 
+                 "Failed to allocate buffer for archive member of size: %d\n",
                  file->filesize);
       return LDPS_ERR;
     }
@@ -316,7 +337,7 @@
 /// At this point, we use get_symbols to see if any of our definitions have
 /// been overridden by a native object file. Then, perform optimization and
 /// codegen.
-ld_plugin_status all_symbols_read_hook(void) {
+static ld_plugin_status all_symbols_read_hook(void) {
   lto_code_gen_t cg = lto_codegen_create();
 
   for (std::list<claimed_file>::iterator I = Modules.begin(),
@@ -334,21 +355,17 @@
 
   // If we don't preserve any symbols, libLTO will assume that all symbols are
   // needed. Keep all symbols unless we're producing a final executable.
-  if (output_type == LTO_CODEGEN_PIC_MODEL_STATIC) {
-    bool anySymbolsPreserved = false;
-    for (std::list<claimed_file>::iterator I = Modules.begin(),
+  bool anySymbolsPreserved = false;
+  for (std::list<claimed_file>::iterator I = Modules.begin(),
          E = Modules.end(); I != E; ++I) {
-      (*get_symbols)(I->handle, I->syms.size(), &I->syms[0]);
-      for (unsigned i = 0, e = I->syms.size(); i != e; i++) {
-        if (I->syms[i].resolution == LDPR_PREVAILING_DEF ||
-            (I->syms[i].def == LDPK_COMMON &&
-             I->syms[i].resolution == LDPR_RESOLVED_IR)) {
-          lto_codegen_add_must_preserve_symbol(cg, I->syms[i].name);
-          anySymbolsPreserved = true;
+    (*get_symbols)(I->handle, I->syms.size(), &I->syms[0]);
+    for (unsigned i = 0, e = I->syms.size(); i != e; i++) {
+      if (I->syms[i].resolution == LDPR_PREVAILING_DEF) {
+        lto_codegen_add_must_preserve_symbol(cg, I->syms[i].name);
+        anySymbolsPreserved = true;
 
-          if (options::generate_api_file)
-            api_file << I->syms[i].name << "\n";
-        }
+        if (options::generate_api_file)
+          api_file << I->syms[i].name << "\n";
       }
     }
 
@@ -364,7 +381,7 @@
 
   lto_codegen_set_pic_model(cg, output_type);
   lto_codegen_set_debug_model(cg, LTO_DEBUG_MODEL_DWARF);
-  if (options::as_path) {
+  if (!options::as_path.empty()) {
     sys::Path p = sys::Program::FindProgramByName(options::as_path);
     lto_codegen_set_assembler_path(cg, p.c_str());
   }
@@ -376,6 +393,21 @@
     }
   }
 
+
+  if (options::generate_bc_file != options::BC_NO) {
+    std::string path;
+    if (options::generate_bc_file == options::BC_ONLY)
+      path = output_name;
+    else if (!options::bc_path.empty())
+      path = options::bc_path;
+    else
+      path = output_name + ".bc";
+    bool err = lto_codegen_write_merged_modules(cg, path.c_str());
+    if (err)
+      (*message)(LDPL_FATAL, "Failed to write the output file.");
+    if (options::generate_bc_file == options::BC_ONLY)
+      exit(0);
+  }
   size_t bufsize = 0;
   const char *buffer = static_cast<const char *>(lto_codegen_compile(cg,
                                                                      &bufsize));
@@ -387,17 +419,15 @@
     (*message)(LDPL_ERROR, "%s", ErrMsg.c_str());
     return LDPS_ERR;
   }
-  raw_fd_ostream *objFile = 
-    new raw_fd_ostream(uniqueObjPath.c_str(), ErrMsg,
-                       raw_fd_ostream::F_Binary);
+  raw_fd_ostream objFile(uniqueObjPath.c_str(), ErrMsg,
+                         raw_fd_ostream::F_Binary);
   if (!ErrMsg.empty()) {
-    delete objFile;
     (*message)(LDPL_ERROR, "%s", ErrMsg.c_str());
     return LDPS_ERR;
   }
 
-  objFile->write(buffer, bufsize);
-  objFile->close();
+  objFile.write(buffer, bufsize);
+  objFile.close();
 
   lto_codegen_dispose(cg);
 
@@ -412,7 +442,7 @@
   return LDPS_OK;
 }
 
-ld_plugin_status cleanup_hook(void) {
+static ld_plugin_status cleanup_hook(void) {
   std::string ErrMsg;
 
   for (int i = 0, e = Cleanup.size(); i != e; ++i)
