# Copyright 2019 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Implements the interface of the results_processor module.

Provides functions to process intermediate results, and the entry point to
the standalone version of Results Processor.
"""

import json
import os

from core.results_processor import command_line
from core.results_processor import json3_output
from core.results_processor import histograms_output
from core.results_processor import html_output


HTML_TRACE_NAME = 'trace.html'
TELEMETRY_RESULTS = '_telemetry_results.jsonl'
FORMATTERS = {
    'json-test-results': json3_output,
    'histograms': histograms_output,
    'html': html_output,
}


def ProcessResults(options):
  """Process intermediate results and produce the requested outputs.

  This function takes the intermediate results generated by Telemetry after
  running benchmarks (including artifacts such as traces, etc.), and processes
  them as requested by the result processing options.

  Args:
    options: An options object with values parsed from the command line and
      after any adjustments from ProcessOptions were applied.
  """
  if not getattr(options, 'output_formats', None):
    return 0

  intermediate_results = _LoadIntermediateResults(
      os.path.join(options.intermediate_dir, TELEMETRY_RESULTS))

  _AggregateTraces(intermediate_results)

  _UploadArtifacts(intermediate_results, options.upload_bucket)

  for output_format in options.output_formats:
    if output_format not in FORMATTERS:
      raise NotImplementedError(output_format)

    formatter = FORMATTERS[output_format]
    formatter.Process(intermediate_results, options)


def _LoadIntermediateResults(intermediate_file):
  """Load intermediate results from a file into a single dict."""
  results = {'benchmarkRun': {}, 'testResults': []}
  with open(intermediate_file) as f:
    for line in f:
      record = json.loads(line)
      if 'benchmarkRun' in record:
        results['benchmarkRun'].update(record['benchmarkRun'])
      if 'testResult' in record:
        results['testResults'].append(record['testResult'])
  return results


def _AggregateTraces(intermediate_results):
  """Replace individual traces with an aggregate one for each test result.

  For each test run with traces, generates an aggregate HTML trace. Removes
  all entries for individual traces and adds one entry for aggregate one.
  """
  for result in intermediate_results['testResults']:
    artifacts = result.get('artifacts', {})
    traces = [name for name in artifacts if name.startswith('trace/')]
    if len(traces) > 0:
      # For now, the html trace is generated by Telemetry, so it should be there
      # already. All we need to do is remove individual traces from the dict.
      # TODO(crbug.com/981349): replace this with actual aggregation code.
      assert HTML_TRACE_NAME in artifacts
      for trace in traces:
        del artifacts[trace]


def _UploadArtifacts(intermediate_results, upload_bucket):
  """Upload all artifacts to cloud.

  For each test run, uploads all its artifacts to cloud and sets remoteUrl
  fields in intermediate_results.
  """
  if upload_bucket is not None:
    for result in intermediate_results['testResults']:
      artifacts = result.get('artifacts', {})
      for artifact in artifacts.values():
        # For now, the uploading is done by Telemetry, so we just check that
        # remoteUrls are set.
        # TODO(crbug.com/981349): replace this with actual uploading code
        assert 'remoteUrl' in artifact


def main(args=None):
  """Entry point for the standalone version of the results_processor script."""
  parser = command_line.ArgumentParser(standalone=True)
  options = parser.parse_args(args)
  command_line.ProcessOptions(options)
  return ProcessResults(options)
