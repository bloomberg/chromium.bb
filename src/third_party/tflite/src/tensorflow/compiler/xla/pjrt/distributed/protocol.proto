// Copyright 2020 The TensorFlow Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// ==============================================================================
//
// Distributed XLA service protocol.
//
// This is a minimal distributed protocol intended primarily for sharing NCCL
// communicator state between distributed hosts.
//
// The intention is to replace this with a more capable distributed runtime at
// some point in the near future, but this suffices for simple multihost GPU
// use cases.
//
// The intention is that a service is started during cluster initialization and
// persists for the lifetime of the cluster.
//
// TODO(phawkins): add a health-checking mechanism.

syntax = "proto3";

package xla;

// Describes a device local to a host.
message DeviceProto {
  int32 local_device_ordinal = 1;
  string name = 2;
  string vendor = 3;

  // The following fields are present in the GlobalTopologyProto message
  // returned by Connect() but not in the LocalTopologyProto messages passed to
  // Connect(). In other words, the master node determines the global device IDs
  // during Connect().
  int32 global_device_id = 4;  // Globally unique ID number.
}

// Describes the set of devices local to a host.
message LocalTopologyProto {
  // We assume that each node knows its globally-unique node ID, provided by
  // whatever mechanism launches the tasks. Node IDs should form a dense range
  // of integers [0, num_nodes).
  int32 node_id = 1;
  repeated DeviceProto devices = 2;
}

message GlobalTopologyProto {
  repeated LocalTopologyProto nodes = 1;
}

message ConnectRequest {
  int32 protocol_version = 1;  // Always 1 at present.

  LocalTopologyProto local_topology = 2;
}

message ConnectResponse {
  GlobalTopologyProto global_topology = 2;
}

message KeyValueGetRequest {
  bytes key = 1;
  int32 timeout_milliseconds = 2;
}

message KeyValueGetResponse {
  bool found = 1;
  bytes value = 2;
}

message KeyValueSetRequest {
  bytes key = 1;
  bytes value = 2;
}

message KeyValueSetResponse {}

service DistributedRuntimeService {
  // Connects a node to the distributed master node. Blocks until all workers
  // have connected. The service receives the number of nodes to expect as an
  // option passed to its constructor.
  rpc Connect(ConnectRequest) returns (ConnectResponse) {}

  // Simple key-value store used for sharing configuration data.
  // For example, when using NCCL to communicate between multiple GPUs,
  // the NCCL communicator IDs are stored here.

  // Looks up a key in the key-value service. Blocks until the key is present
  // or until `timeout` expires.
  rpc KeyValueGet(KeyValueGetRequest) returns (KeyValueGetResponse) {}

  // Updates the value associated with a key.
  rpc KeyValueSet(KeyValueSetRequest) returns (KeyValueSetResponse) {}
}
