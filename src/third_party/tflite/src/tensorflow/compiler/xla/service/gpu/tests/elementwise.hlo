// RUN: hlo_to_llvm_ir %s | FileCheck %s

// These CHECKs are tricky to generate. Here is how it's done:
// * Modify the input to surround the whole file content with "module {}"
// * Remove all global variables
// * Run mlir/utils/generate-test-checks.py
//
// It's a miracle that mlir/utils/generate-test-checks.py works with LLVM at
// all, because it assumes MLIR input. Oh well.

// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// CHECK: target datalayout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64"
// CHECK: target triple = "nvptx64-nvidia-cuda"

// CHECK: define void @r0(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r0.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r0.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.fabs.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.fabs.f32(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @llvm.fabs.f32(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @llvm.fabs.f32(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() #0
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() #0
// CHECK: ; Function Attrs: inaccessiblememonly nofree nosync nounwind willreturn
// CHECK: declare void @llvm.assume(i1 noundef) #1
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.fabs.f32(float) #2

// CHECK: define void @r1(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r1.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r1.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.round.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.round.f32(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @llvm.round.f32(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @llvm.round.f32(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.round.f32(float) #2

// CHECK: define void @r2(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r2.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r2.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.ceil.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.ceil.f32(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @llvm.ceil.f32(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @llvm.ceil.f32(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.ceil.f32(float) #2

// CHECK: define void @r3(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r3.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r3.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds i32, i32* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load i32, i32* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call i32 @llvm.ctlz.i32(i32 %[[VAL_34]], i1 false)
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds i32, i32* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store i32 %[[VAL_35]], i32* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i32, i32* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load i32, i32* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call i32 @llvm.ctlz.i32(i32 %[[VAL_40]], i1 false)
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i32, i32* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store i32 %[[VAL_41]], i32* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i32, i32* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load i32, i32* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call i32 @llvm.ctlz.i32(i32 %[[VAL_46]], i1 false)
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds i32, i32* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store i32 %[[VAL_47]], i32* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds i32, i32* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load i32, i32* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call i32 @llvm.ctlz.i32(i32 %[[VAL_52]], i1 false)
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds i32, i32* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store i32 %[[VAL_53]], i32* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare i32 @llvm.ctlz.i32(i32, i1 immarg) #2

// CHECK: define void @r4(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r4.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r4.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_34]], float* %[[VAL_36]], align 4
// CHECK:   %[[VAL_37:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_38:.*]] = getelementptr inbounds float, float* %[[VAL_37]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_39:.*]] = load float, float* %[[VAL_38]], align 4, !invariant.load !94
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds float, float* %[[VAL_40]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_39]], float* %[[VAL_41]], align 4
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_44:.*]] = load float, float* %[[VAL_43]], align 4, !invariant.load !94
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds float, float* %[[VAL_45]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_44]], float* %[[VAL_46]], align 4
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_49]], float* %[[VAL_51]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r5(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r5.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r5.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_34]], float* %[[VAL_36]], align 4
// CHECK:   %[[VAL_37:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_38:.*]] = getelementptr inbounds float, float* %[[VAL_37]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_39:.*]] = load float, float* %[[VAL_38]], align 4, !invariant.load !94
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds float, float* %[[VAL_40]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_39]], float* %[[VAL_41]], align 4
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_44:.*]] = load float, float* %[[VAL_43]], align 4, !invariant.load !94
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds float, float* %[[VAL_45]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_44]], float* %[[VAL_46]], align 4
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_49]], float* %[[VAL_51]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r7(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !95
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 1024
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 20480
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = udiv i32 %[[VAL_10]], 1
// CHECK:   %[[VAL_13:.*]] = urem i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_14:.*]] = udiv i32 %[[VAL_10]], 200
// CHECK:   %[[VAL_15:.*]] = icmp ult i32 %[[VAL_10]], 20000
// CHECK:   br i1 %[[VAL_15]], label %[[VAL_16:.*]], label %[[VAL_17:.*]]
// CHECK: r7.in_bounds-after:                               ; preds = %[[VAL_16]], %[[VAL_18:.*]]
// CHECK:   ret void
// CHECK: r7.in_bounds-true:                                ; preds = %[[VAL_18]]
// CHECK:   %[[VAL_19:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_20:.*]] = getelementptr inbounds float, float* %[[VAL_19]], i32 %[[VAL_10]]
// CHECK:   %[[VAL_21:.*]] = load float, float* %[[VAL_20]], align 4, !invariant.load !94
// CHECK:   %[[VAL_22:.*]] = call float @__nv_cosf(float %[[VAL_21]])
// CHECK:   %[[VAL_23:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_24:.*]] = getelementptr inbounds float, float* %[[VAL_23]], i32 %[[VAL_10]]
// CHECK:   store float %[[VAL_22]], float* %[[VAL_24]], align 4
// CHECK:   br label %[[VAL_17]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_cosf(float) #0

// CHECK: define void @r8(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r8.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r8.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_expf(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_expf(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_expf(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_expf(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_expf(float) #0

// CHECK: define void @r9(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r9.in_bounds-after:                               ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r9.in_bounds-true:                                ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_expm1f(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_expm1f(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_expm1f(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_expm1f(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_expm1f(float) #0

// CHECK: define void @r10(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r10.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r10.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.floor.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.floor.f32(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @llvm.floor.f32(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @llvm.floor.f32(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.floor.f32(float) #2

// CHECK: define void @r11(i8* noalias align 16 dereferenceable(160000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x %[[VAL_5:.*]]]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_9:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_10:.*]] = mul nuw nsw i32 %[[VAL_8]], 256
// CHECK:   %[[VAL_11:.*]] = add nuw nsw i32 %[[VAL_10]], %[[VAL_9]]
// CHECK:   %[[VAL_12:.*]] = icmp ult i32 %[[VAL_11]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_12]])
// CHECK:   %[[VAL_13:.*]] = mul nuw nsw i32 %[[VAL_11]], 4
// CHECK:   %[[VAL_14:.*]] = udiv i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_15:.*]] = urem i32 %[[VAL_14]], 200
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_17:.*]] = add nuw nsw i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_17]], 1
// CHECK:   %[[VAL_19:.*]] = urem i32 %[[VAL_18]], 200
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_21:.*]] = add nuw nsw i32 %[[VAL_13]], 2
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_21]], 1
// CHECK:   %[[VAL_23:.*]] = urem i32 %[[VAL_22]], 200
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_25:.*]] = add nuw nsw i32 %[[VAL_13]], 3
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_25]], 1
// CHECK:   %[[VAL_27:.*]] = urem i32 %[[VAL_26]], 200
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_29:.*]] = icmp ult i32 %[[VAL_13]], 20000
// CHECK:   br i1 %[[VAL_29]], label %[[VAL_30:.*]], label %[[VAL_31:.*]]
// CHECK: r11.in_bounds-after:                              ; preds = %[[VAL_30]], %[[VAL_32:.*]]
// CHECK:   ret void
// CHECK: r11.in_bounds-true:                               ; preds = %[[VAL_32]]
// CHECK:   %[[VAL_33:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_34:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_33]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_35:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_34]], align 1, !invariant.load !94
// CHECK:   %[[VAL_36:.*]] = extractvalue %[[VAL_5]] %[[VAL_35]], 1
// CHECK:   %[[VAL_37:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_38:.*]] = getelementptr inbounds float, float* %[[VAL_37]], i32 %[[VAL_13]]
// CHECK:   store float %[[VAL_36]], float* %[[VAL_38]], align 4
// CHECK:   %[[VAL_39:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_40:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_39]], i32 %[[VAL_17]]
// CHECK:   %[[VAL_41:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_40]], align 1, !invariant.load !94
// CHECK:   %[[VAL_42:.*]] = extractvalue %[[VAL_5]] %[[VAL_41]], 1
// CHECK:   %[[VAL_43:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_44:.*]] = getelementptr inbounds float, float* %[[VAL_43]], i32 %[[VAL_17]]
// CHECK:   store float %[[VAL_42]], float* %[[VAL_44]], align 4
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_45]], i32 %[[VAL_21]]
// CHECK:   %[[VAL_47:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_46]], align 1, !invariant.load !94
// CHECK:   %[[VAL_48:.*]] = extractvalue %[[VAL_5]] %[[VAL_47]], 1
// CHECK:   %[[VAL_49:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_50:.*]] = getelementptr inbounds float, float* %[[VAL_49]], i32 %[[VAL_21]]
// CHECK:   store float %[[VAL_48]], float* %[[VAL_50]], align 4
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_51]], i32 %[[VAL_25]]
// CHECK:   %[[VAL_53:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_52]], align 1, !invariant.load !94
// CHECK:   %[[VAL_54:.*]] = extractvalue %[[VAL_5]] %[[VAL_53]], 1
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds float, float* %[[VAL_55]], i32 %[[VAL_25]]
// CHECK:   store float %[[VAL_54]], float* %[[VAL_56]], align 4
// CHECK:   br label %[[VAL_31]]
// CHECK: }

// CHECK: define void @r12(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r12.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r12.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.fabs.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = fcmp one float %[[VAL_35]], 0x7FF0000000000000
// CHECK:   %[[VAL_37:.*]] = zext i1 %[[VAL_36]] to i8
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_12]]
// CHECK:   store i8 %[[VAL_37]], i8* %[[VAL_39]], align 1
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds float, float* %[[VAL_40]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_42:.*]] = load float, float* %[[VAL_41]], align 4, !invariant.load !94
// CHECK:   %[[VAL_43:.*]] = call float @llvm.fabs.f32(float %[[VAL_42]])
// CHECK:   %[[VAL_44:.*]] = fcmp one float %[[VAL_43]], 0x7FF0000000000000
// CHECK:   %[[VAL_45:.*]] = zext i1 %[[VAL_44]] to i8
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds i8, i8* %[[VAL_46]], i32 %[[VAL_16]]
// CHECK:   store i8 %[[VAL_45]], i8* %[[VAL_47]], align 1
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_50:.*]] = load float, float* %[[VAL_49]], align 4, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = call float @llvm.fabs.f32(float %[[VAL_50]])
// CHECK:   %[[VAL_52:.*]] = fcmp one float %[[VAL_51]], 0x7FF0000000000000
// CHECK:   %[[VAL_53:.*]] = zext i1 %[[VAL_52]] to i8
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds i8, i8* %[[VAL_54]], i32 %[[VAL_20]]
// CHECK:   store i8 %[[VAL_53]], i8* %[[VAL_55]], align 1
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = call float @llvm.fabs.f32(float %[[VAL_58]])
// CHECK:   %[[VAL_60:.*]] = fcmp one float %[[VAL_59]], 0x7FF0000000000000
// CHECK:   %[[VAL_61:.*]] = zext i1 %[[VAL_60]] to i8
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_24]]
// CHECK:   store i8 %[[VAL_61]], i8* %[[VAL_63]], align 1
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r13(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r13.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r13.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_logf(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_logf(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_logf(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_logf(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_logf(float) #0

// CHECK: define void @r14(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r14.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r14.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_log1pf(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_log1pf(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_log1pf(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_log1pf(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_log1pf(float) #0

// CHECK: define void @r15(i8* noalias align 16 dereferenceable(20000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r15.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r15.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_4]] to i8*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds i8, i8* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load i8, i8* %[[VAL_33]], align 1, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = trunc i8 %[[VAL_34]] to i1
// CHECK:   %[[VAL_36:.*]] = xor i1 %[[VAL_35]], true
// CHECK:   %[[VAL_37:.*]] = zext i1 %[[VAL_36]] to i8
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_12]]
// CHECK:   store i8 %[[VAL_37]], i8* %[[VAL_39]], align 1
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_4]] to i8*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds i8, i8* %[[VAL_40]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_42:.*]] = load i8, i8* %[[VAL_41]], align 1, !invariant.load !94
// CHECK:   %[[VAL_43:.*]] = trunc i8 %[[VAL_42]] to i1
// CHECK:   %[[VAL_44:.*]] = xor i1 %[[VAL_43]], true
// CHECK:   %[[VAL_45:.*]] = zext i1 %[[VAL_44]] to i8
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds i8, i8* %[[VAL_46]], i32 %[[VAL_16]]
// CHECK:   store i8 %[[VAL_45]], i8* %[[VAL_47]], align 1
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_4]] to i8*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds i8, i8* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_50:.*]] = load i8, i8* %[[VAL_49]], align 1, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = trunc i8 %[[VAL_50]] to i1
// CHECK:   %[[VAL_52:.*]] = xor i1 %[[VAL_51]], true
// CHECK:   %[[VAL_53:.*]] = zext i1 %[[VAL_52]] to i8
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds i8, i8* %[[VAL_54]], i32 %[[VAL_20]]
// CHECK:   store i8 %[[VAL_53]], i8* %[[VAL_55]], align 1
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_4]] to i8*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds i8, i8* %[[VAL_56]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_58:.*]] = load i8, i8* %[[VAL_57]], align 1, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = trunc i8 %[[VAL_58]] to i1
// CHECK:   %[[VAL_60:.*]] = xor i1 %[[VAL_59]], true
// CHECK:   %[[VAL_61:.*]] = zext i1 %[[VAL_60]] to i8
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_24]]
// CHECK:   store i8 %[[VAL_61]], i8* %[[VAL_63]], align 1
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r16(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r16.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r16.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = fneg float %[[VAL_34]]
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fneg float %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = fneg float %[[VAL_46]]
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = fneg float %[[VAL_52]]
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r17(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r17.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r17.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds i32, i32* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load i32, i32* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call i32 @llvm.ctpop.i32(i32 %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds i32, i32* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store i32 %[[VAL_35]], i32* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i32, i32* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load i32, i32* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call i32 @llvm.ctpop.i32(i32 %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i32, i32* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store i32 %[[VAL_41]], i32* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i32, i32* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load i32, i32* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call i32 @llvm.ctpop.i32(i32 %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds i32, i32* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store i32 %[[VAL_47]], i32* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_4]] to i32*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds i32, i32* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load i32, i32* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call i32 @llvm.ctpop.i32(i32 %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_6]] to i32*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds i32, i32* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store i32 %[[VAL_53]], i32* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare i32 @llvm.ctpop.i32(i32) #2

// CHECK: define void @r18(i8* noalias align 16 dereferenceable(160000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x %[[VAL_5:.*]]]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_9:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_10:.*]] = mul nuw nsw i32 %[[VAL_8]], 256
// CHECK:   %[[VAL_11:.*]] = add nuw nsw i32 %[[VAL_10]], %[[VAL_9]]
// CHECK:   %[[VAL_12:.*]] = icmp ult i32 %[[VAL_11]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_12]])
// CHECK:   %[[VAL_13:.*]] = mul nuw nsw i32 %[[VAL_11]], 4
// CHECK:   %[[VAL_14:.*]] = udiv i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_15:.*]] = urem i32 %[[VAL_14]], 200
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_17:.*]] = add nuw nsw i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_17]], 1
// CHECK:   %[[VAL_19:.*]] = urem i32 %[[VAL_18]], 200
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_21:.*]] = add nuw nsw i32 %[[VAL_13]], 2
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_21]], 1
// CHECK:   %[[VAL_23:.*]] = urem i32 %[[VAL_22]], 200
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_25:.*]] = add nuw nsw i32 %[[VAL_13]], 3
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_25]], 1
// CHECK:   %[[VAL_27:.*]] = urem i32 %[[VAL_26]], 200
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_29:.*]] = icmp ult i32 %[[VAL_13]], 20000
// CHECK:   br i1 %[[VAL_29]], label %[[VAL_30:.*]], label %[[VAL_31:.*]]
// CHECK: r18.in_bounds-after:                              ; preds = %[[VAL_30]], %[[VAL_32:.*]]
// CHECK:   ret void
// CHECK: r18.in_bounds-true:                               ; preds = %[[VAL_32]]
// CHECK:   %[[VAL_33:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_34:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_33]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_35:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_34]], align 1, !invariant.load !94
// CHECK:   %[[VAL_36:.*]] = extractvalue %[[VAL_5]] %[[VAL_35]], 0
// CHECK:   %[[VAL_37:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_38:.*]] = getelementptr inbounds float, float* %[[VAL_37]], i32 %[[VAL_13]]
// CHECK:   store float %[[VAL_36]], float* %[[VAL_38]], align 4
// CHECK:   %[[VAL_39:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_40:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_39]], i32 %[[VAL_17]]
// CHECK:   %[[VAL_41:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_40]], align 1, !invariant.load !94
// CHECK:   %[[VAL_42:.*]] = extractvalue %[[VAL_5]] %[[VAL_41]], 0
// CHECK:   %[[VAL_43:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_44:.*]] = getelementptr inbounds float, float* %[[VAL_43]], i32 %[[VAL_17]]
// CHECK:   store float %[[VAL_42]], float* %[[VAL_44]], align 4
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_45]], i32 %[[VAL_21]]
// CHECK:   %[[VAL_47:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_46]], align 1, !invariant.load !94
// CHECK:   %[[VAL_48:.*]] = extractvalue %[[VAL_5]] %[[VAL_47]], 0
// CHECK:   %[[VAL_49:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_50:.*]] = getelementptr inbounds float, float* %[[VAL_49]], i32 %[[VAL_21]]
// CHECK:   store float %[[VAL_48]], float* %[[VAL_50]], align 4
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x %[[VAL_5]]]]* %[[VAL_4]] to %[[VAL_5]]*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds %[[VAL_5]], %[[VAL_5]]* %[[VAL_51]], i32 %[[VAL_25]]
// CHECK:   %[[VAL_53:.*]] = load %[[VAL_5]], %[[VAL_5]]* %[[VAL_52]], align 1, !invariant.load !94
// CHECK:   %[[VAL_54:.*]] = extractvalue %[[VAL_5]] %[[VAL_53]], 0
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds float, float* %[[VAL_55]], i32 %[[VAL_25]]
// CHECK:   store float %[[VAL_54]], float* %[[VAL_56]], align 4
// CHECK:   br label %[[VAL_31]]
// CHECK: }

// CHECK: define void @r19(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r19.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r19.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = bitcast float %[[VAL_34]] to i32
// CHECK:   %[[VAL_36:.*]] = and i32 %[[VAL_35]], 2147483647
// CHECK:   %[[VAL_37:.*]] = icmp ugt i32 %[[VAL_36]], 2139095040
// CHECK:   %[[VAL_38:.*]] = and i32 %[[VAL_35]], 2048
// CHECK:   %[[VAL_39:.*]] = lshr i32 %[[VAL_38]], 11
// CHECK:   %[[VAL_40:.*]] = add i32 %[[VAL_39]], 1023
// CHECK:   %[[VAL_41:.*]] = add i32 %[[VAL_35]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = and i32 %[[VAL_41]], -2048
// CHECK:   %[[VAL_43:.*]] = and i32 %[[VAL_42]], 2139095040
// CHECK:   %[[VAL_44:.*]] = icmp ugt i32 %[[VAL_43]], 1191182336
// CHECK:   %[[VAL_45:.*]] = icmp ule i32 %[[VAL_43]], 939524096
// CHECK:   %[[VAL_46:.*]] = and i32 %[[VAL_42]], -2147483648
// CHECK:   %[[VAL_47:.*]] = or i32 %[[VAL_46]], 2139095040
// CHECK:   %[[VAL_48:.*]] = select i1 %[[VAL_44]], i32 %[[VAL_47]], i32 %[[VAL_42]]
// CHECK:   %[[VAL_49:.*]] = select i1 %[[VAL_45]], i32 %[[VAL_46]], i32 %[[VAL_48]]
// CHECK:   %[[VAL_50:.*]] = bitcast i32 %[[VAL_49]] to float
// CHECK:   %[[VAL_51:.*]] = select i1 %[[VAL_37]], float %[[VAL_34]], float %[[VAL_50]]
// CHECK:   %[[VAL_52:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_53:.*]] = getelementptr inbounds float, float* %[[VAL_52]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_51]], float* %[[VAL_53]], align 4
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_56:.*]] = load float, float* %[[VAL_55]], align 4, !invariant.load !94
// CHECK:   %[[VAL_57:.*]] = bitcast float %[[VAL_56]] to i32
// CHECK:   %[[VAL_58:.*]] = and i32 %[[VAL_57]], 2147483647
// CHECK:   %[[VAL_59:.*]] = icmp ugt i32 %[[VAL_58]], 2139095040
// CHECK:   %[[VAL_60:.*]] = and i32 %[[VAL_57]], 2048
// CHECK:   %[[VAL_61:.*]] = lshr i32 %[[VAL_60]], 11
// CHECK:   %[[VAL_62:.*]] = add i32 %[[VAL_61]], 1023
// CHECK:   %[[VAL_63:.*]] = add i32 %[[VAL_57]], %[[VAL_62]]
// CHECK:   %[[VAL_64:.*]] = and i32 %[[VAL_63]], -2048
// CHECK:   %[[VAL_65:.*]] = and i32 %[[VAL_64]], 2139095040
// CHECK:   %[[VAL_66:.*]] = icmp ugt i32 %[[VAL_65]], 1191182336
// CHECK:   %[[VAL_67:.*]] = icmp ule i32 %[[VAL_65]], 939524096
// CHECK:   %[[VAL_68:.*]] = and i32 %[[VAL_64]], -2147483648
// CHECK:   %[[VAL_69:.*]] = or i32 %[[VAL_68]], 2139095040
// CHECK:   %[[VAL_70:.*]] = select i1 %[[VAL_66]], i32 %[[VAL_69]], i32 %[[VAL_64]]
// CHECK:   %[[VAL_71:.*]] = select i1 %[[VAL_67]], i32 %[[VAL_68]], i32 %[[VAL_70]]
// CHECK:   %[[VAL_72:.*]] = bitcast i32 %[[VAL_71]] to float
// CHECK:   %[[VAL_73:.*]] = select i1 %[[VAL_59]], float %[[VAL_56]], float %[[VAL_72]]
// CHECK:   %[[VAL_74:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_75:.*]] = getelementptr inbounds float, float* %[[VAL_74]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_73]], float* %[[VAL_75]], align 4
// CHECK:   %[[VAL_76:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_77:.*]] = getelementptr inbounds float, float* %[[VAL_76]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_78:.*]] = load float, float* %[[VAL_77]], align 4, !invariant.load !94
// CHECK:   %[[VAL_79:.*]] = bitcast float %[[VAL_78]] to i32
// CHECK:   %[[VAL_80:.*]] = and i32 %[[VAL_79]], 2147483647
// CHECK:   %[[VAL_81:.*]] = icmp ugt i32 %[[VAL_80]], 2139095040
// CHECK:   %[[VAL_82:.*]] = and i32 %[[VAL_79]], 2048
// CHECK:   %[[VAL_83:.*]] = lshr i32 %[[VAL_82]], 11
// CHECK:   %[[VAL_84:.*]] = add i32 %[[VAL_83]], 1023
// CHECK:   %[[VAL_85:.*]] = add i32 %[[VAL_79]], %[[VAL_84]]
// CHECK:   %[[VAL_86:.*]] = and i32 %[[VAL_85]], -2048
// CHECK:   %[[VAL_87:.*]] = and i32 %[[VAL_86]], 2139095040
// CHECK:   %[[VAL_88:.*]] = icmp ugt i32 %[[VAL_87]], 1191182336
// CHECK:   %[[VAL_89:.*]] = icmp ule i32 %[[VAL_87]], 939524096
// CHECK:   %[[VAL_90:.*]] = and i32 %[[VAL_86]], -2147483648
// CHECK:   %[[VAL_91:.*]] = or i32 %[[VAL_90]], 2139095040
// CHECK:   %[[VAL_92:.*]] = select i1 %[[VAL_88]], i32 %[[VAL_91]], i32 %[[VAL_86]]
// CHECK:   %[[VAL_93:.*]] = select i1 %[[VAL_89]], i32 %[[VAL_90]], i32 %[[VAL_92]]
// CHECK:   %[[VAL_94:.*]] = bitcast i32 %[[VAL_93]] to float
// CHECK:   %[[VAL_95:.*]] = select i1 %[[VAL_81]], float %[[VAL_78]], float %[[VAL_94]]
// CHECK:   %[[VAL_96:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_97:.*]] = getelementptr inbounds float, float* %[[VAL_96]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_95]], float* %[[VAL_97]], align 4
// CHECK:   %[[VAL_98:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_99:.*]] = getelementptr inbounds float, float* %[[VAL_98]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_100:.*]] = load float, float* %[[VAL_99]], align 4, !invariant.load !94
// CHECK:   %[[VAL_101:.*]] = bitcast float %[[VAL_100]] to i32
// CHECK:   %[[VAL_102:.*]] = and i32 %[[VAL_101]], 2147483647
// CHECK:   %[[VAL_103:.*]] = icmp ugt i32 %[[VAL_102]], 2139095040
// CHECK:   %[[VAL_104:.*]] = and i32 %[[VAL_101]], 2048
// CHECK:   %[[VAL_105:.*]] = lshr i32 %[[VAL_104]], 11
// CHECK:   %[[VAL_106:.*]] = add i32 %[[VAL_105]], 1023
// CHECK:   %[[VAL_107:.*]] = add i32 %[[VAL_101]], %[[VAL_106]]
// CHECK:   %[[VAL_108:.*]] = and i32 %[[VAL_107]], -2048
// CHECK:   %[[VAL_109:.*]] = and i32 %[[VAL_108]], 2139095040
// CHECK:   %[[VAL_110:.*]] = icmp ugt i32 %[[VAL_109]], 1191182336
// CHECK:   %[[VAL_111:.*]] = icmp ule i32 %[[VAL_109]], 939524096
// CHECK:   %[[VAL_112:.*]] = and i32 %[[VAL_108]], -2147483648
// CHECK:   %[[VAL_113:.*]] = or i32 %[[VAL_112]], 2139095040
// CHECK:   %[[VAL_114:.*]] = select i1 %[[VAL_110]], i32 %[[VAL_113]], i32 %[[VAL_108]]
// CHECK:   %[[VAL_115:.*]] = select i1 %[[VAL_111]], i32 %[[VAL_112]], i32 %[[VAL_114]]
// CHECK:   %[[VAL_116:.*]] = bitcast i32 %[[VAL_115]] to float
// CHECK:   %[[VAL_117:.*]] = select i1 %[[VAL_103]], float %[[VAL_100]], float %[[VAL_116]]
// CHECK:   %[[VAL_118:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_119:.*]] = getelementptr inbounds float, float* %[[VAL_118]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_117]], float* %[[VAL_119]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r20(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r20.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r20.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_rsqrtf(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_rsqrtf(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_rsqrtf(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_rsqrtf(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_rsqrtf(float) #0

// CHECK: define void @r22(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r22.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r22.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = fcmp one float %[[VAL_34]], 0.000000e+00
// CHECK:   %[[VAL_36:.*]] = uitofp i1 %[[VAL_35]] to float
// CHECK:   %[[VAL_37:.*]] = call float @llvm.copysign.f32(float %[[VAL_36]], float %[[VAL_34]])
// CHECK:   %[[VAL_38:.*]] = fcmp uno float %[[VAL_34]], %[[VAL_34]]
// CHECK:   %[[VAL_39:.*]] = select i1 %[[VAL_38]], float %[[VAL_34]], float %[[VAL_37]]
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds float, float* %[[VAL_40]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_39]], float* %[[VAL_41]], align 4
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_44:.*]] = load float, float* %[[VAL_43]], align 4, !invariant.load !94
// CHECK:   %[[VAL_45:.*]] = fcmp one float %[[VAL_44]], 0.000000e+00
// CHECK:   %[[VAL_46:.*]] = uitofp i1 %[[VAL_45]] to float
// CHECK:   %[[VAL_47:.*]] = call float @llvm.copysign.f32(float %[[VAL_46]], float %[[VAL_44]])
// CHECK:   %[[VAL_48:.*]] = fcmp uno float %[[VAL_44]], %[[VAL_44]]
// CHECK:   %[[VAL_49:.*]] = select i1 %[[VAL_48]], float %[[VAL_44]], float %[[VAL_47]]
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_49]], float* %[[VAL_51]], align 4
// CHECK:   %[[VAL_52:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_53:.*]] = getelementptr inbounds float, float* %[[VAL_52]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_54:.*]] = load float, float* %[[VAL_53]], align 4, !invariant.load !94
// CHECK:   %[[VAL_55:.*]] = fcmp one float %[[VAL_54]], 0.000000e+00
// CHECK:   %[[VAL_56:.*]] = uitofp i1 %[[VAL_55]] to float
// CHECK:   %[[VAL_57:.*]] = call float @llvm.copysign.f32(float %[[VAL_56]], float %[[VAL_54]])
// CHECK:   %[[VAL_58:.*]] = fcmp uno float %[[VAL_54]], %[[VAL_54]]
// CHECK:   %[[VAL_59:.*]] = select i1 %[[VAL_58]], float %[[VAL_54]], float %[[VAL_57]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = fcmp one float %[[VAL_64]], 0.000000e+00
// CHECK:   %[[VAL_66:.*]] = uitofp i1 %[[VAL_65]] to float
// CHECK:   %[[VAL_67:.*]] = call float @llvm.copysign.f32(float %[[VAL_66]], float %[[VAL_64]])
// CHECK:   %[[VAL_68:.*]] = fcmp uno float %[[VAL_64]], %[[VAL_64]]
// CHECK:   %[[VAL_69:.*]] = select i1 %[[VAL_68]], float %[[VAL_64]], float %[[VAL_67]]
// CHECK:   %[[VAL_70:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_71:.*]] = getelementptr inbounds float, float* %[[VAL_70]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_69]], float* %[[VAL_71]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.copysign.f32(float, float) #2

// CHECK: define void @r23(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !95
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 1024
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 20480
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = udiv i32 %[[VAL_10]], 1
// CHECK:   %[[VAL_13:.*]] = urem i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_14:.*]] = udiv i32 %[[VAL_10]], 200
// CHECK:   %[[VAL_15:.*]] = icmp ult i32 %[[VAL_10]], 20000
// CHECK:   br i1 %[[VAL_15]], label %[[VAL_16:.*]], label %[[VAL_17:.*]]
// CHECK: r23.in_bounds-after:                              ; preds = %[[VAL_16]], %[[VAL_18:.*]]
// CHECK:   ret void
// CHECK: r23.in_bounds-true:                               ; preds = %[[VAL_18]]
// CHECK:   %[[VAL_19:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_20:.*]] = getelementptr inbounds float, float* %[[VAL_19]], i32 %[[VAL_10]]
// CHECK:   %[[VAL_21:.*]] = load float, float* %[[VAL_20]], align 4, !invariant.load !94
// CHECK:   %[[VAL_22:.*]] = call float @__nv_sinf(float %[[VAL_21]])
// CHECK:   %[[VAL_23:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_24:.*]] = getelementptr inbounds float, float* %[[VAL_23]], i32 %[[VAL_10]]
// CHECK:   store float %[[VAL_22]], float* %[[VAL_24]], align 4
// CHECK:   br label %[[VAL_17]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_sinf(float) #0

// CHECK: define void @r24(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r24.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r24.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @__nv_sqrtf(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_37:.*]] = getelementptr inbounds float, float* %[[VAL_36]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_35]], float* %[[VAL_37]], align 4
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_sqrtf(float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = call float @__nv_sqrtf(float %[[VAL_46]])
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_47]], float* %[[VAL_49]], align 4
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   %[[VAL_53:.*]] = call float @__nv_sqrtf(float %[[VAL_52]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_sqrtf(float) #0

// CHECK: define void @r25(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r25.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r25.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.fabs.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = call float @__nv_powf(float %[[VAL_35]], float 0x3FD5555560000000)
// CHECK:   %[[VAL_37:.*]] = call float @llvm.copysign.f32(float %[[VAL_36]], float %[[VAL_34]])
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_37]], float* %[[VAL_39]], align 4
// CHECK:   %[[VAL_40:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_41:.*]] = getelementptr inbounds float, float* %[[VAL_40]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_42:.*]] = load float, float* %[[VAL_41]], align 4, !invariant.load !94
// CHECK:   %[[VAL_43:.*]] = call float @llvm.fabs.f32(float %[[VAL_42]])
// CHECK:   %[[VAL_44:.*]] = call float @__nv_powf(float %[[VAL_43]], float 0x3FD5555560000000)
// CHECK:   %[[VAL_45:.*]] = call float @llvm.copysign.f32(float %[[VAL_44]], float %[[VAL_42]])
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds float, float* %[[VAL_46]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_45]], float* %[[VAL_47]], align 4
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_50:.*]] = load float, float* %[[VAL_49]], align 4, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = call float @llvm.fabs.f32(float %[[VAL_50]])
// CHECK:   %[[VAL_52:.*]] = call float @__nv_powf(float %[[VAL_51]], float 0x3FD5555560000000)
// CHECK:   %[[VAL_53:.*]] = call float @llvm.copysign.f32(float %[[VAL_52]], float %[[VAL_50]])
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = call float @llvm.fabs.f32(float %[[VAL_58]])
// CHECK:   %[[VAL_60:.*]] = call float @__nv_powf(float %[[VAL_59]], float 0x3FD5555560000000)
// CHECK:   %[[VAL_61:.*]] = call float @llvm.copysign.f32(float %[[VAL_60]], float %[[VAL_58]])
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_61]], float* %[[VAL_63]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_powf(float, float) #0

// CHECK: define void @r26(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_8:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_9:.*]] = mul nuw nsw i32 %[[VAL_7]], 256
// CHECK:   %[[VAL_10:.*]] = add nuw nsw i32 %[[VAL_9]], %[[VAL_8]]
// CHECK:   %[[VAL_11:.*]] = icmp ult i32 %[[VAL_10]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_11]])
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 4
// CHECK:   %[[VAL_13:.*]] = udiv i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_14:.*]] = urem i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_12]], 200
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_12]], 1
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_16]], 1
// CHECK:   %[[VAL_18:.*]] = urem i32 %[[VAL_17]], 200
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_20:.*]] = add nuw nsw i32 %[[VAL_12]], 2
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_20]], 1
// CHECK:   %[[VAL_22:.*]] = urem i32 %[[VAL_21]], 200
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_24:.*]] = add nuw nsw i32 %[[VAL_12]], 3
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_24]], 1
// CHECK:   %[[VAL_26:.*]] = urem i32 %[[VAL_25]], 200
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_28:.*]] = icmp ult i32 %[[VAL_12]], 20000
// CHECK:   br i1 %[[VAL_28]], label %[[VAL_29:.*]], label %[[VAL_30:.*]]
// CHECK: r26.in_bounds-after:                              ; preds = %[[VAL_29]], %[[VAL_31:.*]]
// CHECK:   ret void
// CHECK: r26.in_bounds-true:                               ; preds = %[[VAL_31]]
// CHECK:   %[[VAL_32:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_33:.*]] = getelementptr inbounds float, float* %[[VAL_32]], i32 %[[VAL_12]]
// CHECK:   %[[VAL_34:.*]] = load float, float* %[[VAL_33]], align 4, !invariant.load !94
// CHECK:   %[[VAL_35:.*]] = call float @llvm.fabs.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_36:.*]] = call float @llvm.fabs.f32(float %[[VAL_34]])
// CHECK:   %[[VAL_37:.*]] = fcmp olt float %[[VAL_36]], 0x3F3A36E2E0000000
// CHECK:   %[[VAL_38:.*]] = fcmp uge float %[[VAL_34]], -9.000000e+00
// CHECK:   %[[VAL_39:.*]] = select i1 %[[VAL_38]], float %[[VAL_34]], float -9.000000e+00
// CHECK:   %[[VAL_40:.*]] = fcmp ule float %[[VAL_39]], 9.000000e+00
// CHECK:   %[[VAL_41:.*]] = select i1 %[[VAL_40]], float %[[VAL_39]], float 9.000000e+00
// CHECK:   %[[VAL_42:.*]] = fmul float %[[VAL_41]], %[[VAL_41]]
// CHECK:   %[[VAL_43:.*]] = fmul float %[[VAL_42]], 0xBCB3E4B800000000
// CHECK:   %[[VAL_44:.*]] = fadd float %[[VAL_43]], 0x3D4C266FC0000000
// CHECK:   %[[VAL_45:.*]] = fmul float %[[VAL_42]], %[[VAL_44]]
// CHECK:   %[[VAL_46:.*]] = fadd float %[[VAL_45]], 0xBDD7A6FFE0000000
// CHECK:   %[[VAL_47:.*]] = fmul float %[[VAL_42]], %[[VAL_46]]
// CHECK:   %[[VAL_48:.*]] = fadd float %[[VAL_47]], 0x3E6B800820000000
// CHECK:   %[[VAL_49:.*]] = fmul float %[[VAL_42]], %[[VAL_48]]
// CHECK:   %[[VAL_50:.*]] = fadd float %[[VAL_49]], 0x3EEF286940000000
// CHECK:   %[[VAL_51:.*]] = fmul float %[[VAL_42]], %[[VAL_50]]
// CHECK:   %[[VAL_52:.*]] = fadd float %[[VAL_51]], 0x3F44E1BDA0000000
// CHECK:   %[[VAL_53:.*]] = fmul float %[[VAL_42]], %[[VAL_52]]
// CHECK:   %[[VAL_54:.*]] = fadd float %[[VAL_53]], 0x3F740B3B80000000
// CHECK:   %[[VAL_55:.*]] = fmul float %[[VAL_41]], %[[VAL_54]]
// CHECK:   %[[VAL_56:.*]] = fmul float %[[VAL_42]], 0x3EB41A7B00000000
// CHECK:   %[[VAL_57:.*]] = fadd float %[[VAL_56]], 0x3F1F12BAC0000000
// CHECK:   %[[VAL_58:.*]] = fmul float %[[VAL_42]], %[[VAL_57]]
// CHECK:   %[[VAL_59:.*]] = fadd float %[[VAL_58]], 0x3F629540A0000000
// CHECK:   %[[VAL_60:.*]] = fmul float %[[VAL_42]], %[[VAL_59]]
// CHECK:   %[[VAL_61:.*]] = fadd float %[[VAL_60]], 0x3F740B3BA0000000
// CHECK:   %[[VAL_62:.*]] = fdiv float %[[VAL_55]], %[[VAL_61]]
// CHECK:   %[[VAL_63:.*]] = select i1 %[[VAL_37]], float %[[VAL_34]], float %[[VAL_62]]
// CHECK:   %[[VAL_64:.*]] = call float @llvm.copysign.f32(float 1.000000e+00, float %[[VAL_34]])
// CHECK:   %[[VAL_65:.*]] = fcmp ult float %[[VAL_35]], 2.000000e+01
// CHECK:   %[[VAL_66:.*]] = select i1 %[[VAL_65]], float %[[VAL_63]], float %[[VAL_64]]
// CHECK:   %[[VAL_67:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_68:.*]] = getelementptr inbounds float, float* %[[VAL_67]], i32 %[[VAL_12]]
// CHECK:   store float %[[VAL_66]], float* %[[VAL_68]], align 4
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_16]]
// CHECK:   %[[VAL_71:.*]] = load float, float* %[[VAL_70]], align 4, !invariant.load !94
// CHECK:   %[[VAL_72:.*]] = call float @llvm.fabs.f32(float %[[VAL_71]])
// CHECK:   %[[VAL_73:.*]] = call float @llvm.fabs.f32(float %[[VAL_71]])
// CHECK:   %[[VAL_74:.*]] = fcmp olt float %[[VAL_73]], 0x3F3A36E2E0000000
// CHECK:   %[[VAL_75:.*]] = fcmp uge float %[[VAL_71]], -9.000000e+00
// CHECK:   %[[VAL_76:.*]] = select i1 %[[VAL_75]], float %[[VAL_71]], float -9.000000e+00
// CHECK:   %[[VAL_77:.*]] = fcmp ule float %[[VAL_76]], 9.000000e+00
// CHECK:   %[[VAL_78:.*]] = select i1 %[[VAL_77]], float %[[VAL_76]], float 9.000000e+00
// CHECK:   %[[VAL_79:.*]] = fmul float %[[VAL_78]], %[[VAL_78]]
// CHECK:   %[[VAL_80:.*]] = fmul float %[[VAL_79]], 0xBCB3E4B800000000
// CHECK:   %[[VAL_81:.*]] = fadd float %[[VAL_80]], 0x3D4C266FC0000000
// CHECK:   %[[VAL_82:.*]] = fmul float %[[VAL_79]], %[[VAL_81]]
// CHECK:   %[[VAL_83:.*]] = fadd float %[[VAL_82]], 0xBDD7A6FFE0000000
// CHECK:   %[[VAL_84:.*]] = fmul float %[[VAL_79]], %[[VAL_83]]
// CHECK:   %[[VAL_85:.*]] = fadd float %[[VAL_84]], 0x3E6B800820000000
// CHECK:   %[[VAL_86:.*]] = fmul float %[[VAL_79]], %[[VAL_85]]
// CHECK:   %[[VAL_87:.*]] = fadd float %[[VAL_86]], 0x3EEF286940000000
// CHECK:   %[[VAL_88:.*]] = fmul float %[[VAL_79]], %[[VAL_87]]
// CHECK:   %[[VAL_89:.*]] = fadd float %[[VAL_88]], 0x3F44E1BDA0000000
// CHECK:   %[[VAL_90:.*]] = fmul float %[[VAL_79]], %[[VAL_89]]
// CHECK:   %[[VAL_91:.*]] = fadd float %[[VAL_90]], 0x3F740B3B80000000
// CHECK:   %[[VAL_92:.*]] = fmul float %[[VAL_78]], %[[VAL_91]]
// CHECK:   %[[VAL_93:.*]] = fmul float %[[VAL_79]], 0x3EB41A7B00000000
// CHECK:   %[[VAL_94:.*]] = fadd float %[[VAL_93]], 0x3F1F12BAC0000000
// CHECK:   %[[VAL_95:.*]] = fmul float %[[VAL_79]], %[[VAL_94]]
// CHECK:   %[[VAL_96:.*]] = fadd float %[[VAL_95]], 0x3F629540A0000000
// CHECK:   %[[VAL_97:.*]] = fmul float %[[VAL_79]], %[[VAL_96]]
// CHECK:   %[[VAL_98:.*]] = fadd float %[[VAL_97]], 0x3F740B3BA0000000
// CHECK:   %[[VAL_99:.*]] = fdiv float %[[VAL_92]], %[[VAL_98]]
// CHECK:   %[[VAL_100:.*]] = select i1 %[[VAL_74]], float %[[VAL_71]], float %[[VAL_99]]
// CHECK:   %[[VAL_101:.*]] = call float @llvm.copysign.f32(float 1.000000e+00, float %[[VAL_71]])
// CHECK:   %[[VAL_102:.*]] = fcmp ult float %[[VAL_72]], 2.000000e+01
// CHECK:   %[[VAL_103:.*]] = select i1 %[[VAL_102]], float %[[VAL_100]], float %[[VAL_101]]
// CHECK:   %[[VAL_104:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_105:.*]] = getelementptr inbounds float, float* %[[VAL_104]], i32 %[[VAL_16]]
// CHECK:   store float %[[VAL_103]], float* %[[VAL_105]], align 4
// CHECK:   %[[VAL_106:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_107:.*]] = getelementptr inbounds float, float* %[[VAL_106]], i32 %[[VAL_20]]
// CHECK:   %[[VAL_108:.*]] = load float, float* %[[VAL_107]], align 4, !invariant.load !94
// CHECK:   %[[VAL_109:.*]] = call float @llvm.fabs.f32(float %[[VAL_108]])
// CHECK:   %[[VAL_110:.*]] = call float @llvm.fabs.f32(float %[[VAL_108]])
// CHECK:   %[[VAL_111:.*]] = fcmp olt float %[[VAL_110]], 0x3F3A36E2E0000000
// CHECK:   %[[VAL_112:.*]] = fcmp uge float %[[VAL_108]], -9.000000e+00
// CHECK:   %[[VAL_113:.*]] = select i1 %[[VAL_112]], float %[[VAL_108]], float -9.000000e+00
// CHECK:   %[[VAL_114:.*]] = fcmp ule float %[[VAL_113]], 9.000000e+00
// CHECK:   %[[VAL_115:.*]] = select i1 %[[VAL_114]], float %[[VAL_113]], float 9.000000e+00
// CHECK:   %[[VAL_116:.*]] = fmul float %[[VAL_115]], %[[VAL_115]]
// CHECK:   %[[VAL_117:.*]] = fmul float %[[VAL_116]], 0xBCB3E4B800000000
// CHECK:   %[[VAL_118:.*]] = fadd float %[[VAL_117]], 0x3D4C266FC0000000
// CHECK:   %[[VAL_119:.*]] = fmul float %[[VAL_116]], %[[VAL_118]]
// CHECK:   %[[VAL_120:.*]] = fadd float %[[VAL_119]], 0xBDD7A6FFE0000000
// CHECK:   %[[VAL_121:.*]] = fmul float %[[VAL_116]], %[[VAL_120]]
// CHECK:   %[[VAL_122:.*]] = fadd float %[[VAL_121]], 0x3E6B800820000000
// CHECK:   %[[VAL_123:.*]] = fmul float %[[VAL_116]], %[[VAL_122]]
// CHECK:   %[[VAL_124:.*]] = fadd float %[[VAL_123]], 0x3EEF286940000000
// CHECK:   %[[VAL_125:.*]] = fmul float %[[VAL_116]], %[[VAL_124]]
// CHECK:   %[[VAL_126:.*]] = fadd float %[[VAL_125]], 0x3F44E1BDA0000000
// CHECK:   %[[VAL_127:.*]] = fmul float %[[VAL_116]], %[[VAL_126]]
// CHECK:   %[[VAL_128:.*]] = fadd float %[[VAL_127]], 0x3F740B3B80000000
// CHECK:   %[[VAL_129:.*]] = fmul float %[[VAL_115]], %[[VAL_128]]
// CHECK:   %[[VAL_130:.*]] = fmul float %[[VAL_116]], 0x3EB41A7B00000000
// CHECK:   %[[VAL_131:.*]] = fadd float %[[VAL_130]], 0x3F1F12BAC0000000
// CHECK:   %[[VAL_132:.*]] = fmul float %[[VAL_116]], %[[VAL_131]]
// CHECK:   %[[VAL_133:.*]] = fadd float %[[VAL_132]], 0x3F629540A0000000
// CHECK:   %[[VAL_134:.*]] = fmul float %[[VAL_116]], %[[VAL_133]]
// CHECK:   %[[VAL_135:.*]] = fadd float %[[VAL_134]], 0x3F740B3BA0000000
// CHECK:   %[[VAL_136:.*]] = fdiv float %[[VAL_129]], %[[VAL_135]]
// CHECK:   %[[VAL_137:.*]] = select i1 %[[VAL_111]], float %[[VAL_108]], float %[[VAL_136]]
// CHECK:   %[[VAL_138:.*]] = call float @llvm.copysign.f32(float 1.000000e+00, float %[[VAL_108]])
// CHECK:   %[[VAL_139:.*]] = fcmp ult float %[[VAL_109]], 2.000000e+01
// CHECK:   %[[VAL_140:.*]] = select i1 %[[VAL_139]], float %[[VAL_137]], float %[[VAL_138]]
// CHECK:   %[[VAL_141:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_142:.*]] = getelementptr inbounds float, float* %[[VAL_141]], i32 %[[VAL_20]]
// CHECK:   store float %[[VAL_140]], float* %[[VAL_142]], align 4
// CHECK:   %[[VAL_143:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_144:.*]] = getelementptr inbounds float, float* %[[VAL_143]], i32 %[[VAL_24]]
// CHECK:   %[[VAL_145:.*]] = load float, float* %[[VAL_144]], align 4, !invariant.load !94
// CHECK:   %[[VAL_146:.*]] = call float @llvm.fabs.f32(float %[[VAL_145]])
// CHECK:   %[[VAL_147:.*]] = call float @llvm.fabs.f32(float %[[VAL_145]])
// CHECK:   %[[VAL_148:.*]] = fcmp olt float %[[VAL_147]], 0x3F3A36E2E0000000
// CHECK:   %[[VAL_149:.*]] = fcmp uge float %[[VAL_145]], -9.000000e+00
// CHECK:   %[[VAL_150:.*]] = select i1 %[[VAL_149]], float %[[VAL_145]], float -9.000000e+00
// CHECK:   %[[VAL_151:.*]] = fcmp ule float %[[VAL_150]], 9.000000e+00
// CHECK:   %[[VAL_152:.*]] = select i1 %[[VAL_151]], float %[[VAL_150]], float 9.000000e+00
// CHECK:   %[[VAL_153:.*]] = fmul float %[[VAL_152]], %[[VAL_152]]
// CHECK:   %[[VAL_154:.*]] = fmul float %[[VAL_153]], 0xBCB3E4B800000000
// CHECK:   %[[VAL_155:.*]] = fadd float %[[VAL_154]], 0x3D4C266FC0000000
// CHECK:   %[[VAL_156:.*]] = fmul float %[[VAL_153]], %[[VAL_155]]
// CHECK:   %[[VAL_157:.*]] = fadd float %[[VAL_156]], 0xBDD7A6FFE0000000
// CHECK:   %[[VAL_158:.*]] = fmul float %[[VAL_153]], %[[VAL_157]]
// CHECK:   %[[VAL_159:.*]] = fadd float %[[VAL_158]], 0x3E6B800820000000
// CHECK:   %[[VAL_160:.*]] = fmul float %[[VAL_153]], %[[VAL_159]]
// CHECK:   %[[VAL_161:.*]] = fadd float %[[VAL_160]], 0x3EEF286940000000
// CHECK:   %[[VAL_162:.*]] = fmul float %[[VAL_153]], %[[VAL_161]]
// CHECK:   %[[VAL_163:.*]] = fadd float %[[VAL_162]], 0x3F44E1BDA0000000
// CHECK:   %[[VAL_164:.*]] = fmul float %[[VAL_153]], %[[VAL_163]]
// CHECK:   %[[VAL_165:.*]] = fadd float %[[VAL_164]], 0x3F740B3B80000000
// CHECK:   %[[VAL_166:.*]] = fmul float %[[VAL_152]], %[[VAL_165]]
// CHECK:   %[[VAL_167:.*]] = fmul float %[[VAL_153]], 0x3EB41A7B00000000
// CHECK:   %[[VAL_168:.*]] = fadd float %[[VAL_167]], 0x3F1F12BAC0000000
// CHECK:   %[[VAL_169:.*]] = fmul float %[[VAL_153]], %[[VAL_168]]
// CHECK:   %[[VAL_170:.*]] = fadd float %[[VAL_169]], 0x3F629540A0000000
// CHECK:   %[[VAL_171:.*]] = fmul float %[[VAL_153]], %[[VAL_170]]
// CHECK:   %[[VAL_172:.*]] = fadd float %[[VAL_171]], 0x3F740B3BA0000000
// CHECK:   %[[VAL_173:.*]] = fdiv float %[[VAL_166]], %[[VAL_172]]
// CHECK:   %[[VAL_174:.*]] = select i1 %[[VAL_148]], float %[[VAL_145]], float %[[VAL_173]]
// CHECK:   %[[VAL_175:.*]] = call float @llvm.copysign.f32(float 1.000000e+00, float %[[VAL_145]])
// CHECK:   %[[VAL_176:.*]] = fcmp ult float %[[VAL_146]], 2.000000e+01
// CHECK:   %[[VAL_177:.*]] = select i1 %[[VAL_176]], float %[[VAL_174]], float %[[VAL_175]]
// CHECK:   %[[VAL_178:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_179:.*]] = getelementptr inbounds float, float* %[[VAL_178]], i32 %[[VAL_24]]
// CHECK:   store float %[[VAL_177]], float* %[[VAL_179]], align 4
// CHECK:   br label %[[VAL_30]]
// CHECK: }

// CHECK: define void @r27(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r27.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r27.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fadd float %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = fadd float %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = fadd float %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = fadd float %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r28(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !95
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 1024
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 20480
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_16:.*]] = urem i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_18:.*]] = icmp ult i32 %[[VAL_13]], 20000
// CHECK:   br i1 %[[VAL_18]], label %[[VAL_19:.*]], label %[[VAL_20:.*]]
// CHECK: r28.in_bounds-after:                              ; preds = %[[VAL_19]], %[[VAL_21:.*]]
// CHECK:   ret void
// CHECK: r28.in_bounds-true:                               ; preds = %[[VAL_21]]
// CHECK:   %[[VAL_22:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_23:.*]] = getelementptr inbounds float, float* %[[VAL_22]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_24:.*]] = load float, float* %[[VAL_23]], align 4, !invariant.load !94
// CHECK:   %[[VAL_25:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_26:.*]] = getelementptr inbounds float, float* %[[VAL_25]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_27:.*]] = load float, float* %[[VAL_26]], align 4, !invariant.load !94
// CHECK:   %[[VAL_28:.*]] = call float @__nv_atan2f(float %[[VAL_24]], float %[[VAL_27]])
// CHECK:   %[[VAL_29:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_30:.*]] = getelementptr inbounds float, float* %[[VAL_29]], i32 %[[VAL_13]]
// CHECK:   store float %[[VAL_28]], float* %[[VAL_30]], align 4
// CHECK:   br label %[[VAL_20]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_atan2f(float, float) #0

// CHECK: define void @r29(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r29.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r29.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fcmp oeq float %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = zext i1 %[[VAL_41]] to i8
// CHECK:   %[[VAL_43:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_44:.*]] = getelementptr inbounds i8, i8* %[[VAL_43]], i32 %[[VAL_15]]
// CHECK:   store i8 %[[VAL_42]], i8* %[[VAL_44]], align 1
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds float, float* %[[VAL_45]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_47:.*]] = load float, float* %[[VAL_46]], align 4, !invariant.load !94
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_50:.*]] = load float, float* %[[VAL_49]], align 4, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = fcmp oeq float %[[VAL_47]], %[[VAL_50]]
// CHECK:   %[[VAL_52:.*]] = zext i1 %[[VAL_51]] to i8
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds i8, i8* %[[VAL_53]], i32 %[[VAL_19]]
// CHECK:   store i8 %[[VAL_52]], i8* %[[VAL_54]], align 1
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds float, float* %[[VAL_55]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_57:.*]] = load float, float* %[[VAL_56]], align 4, !invariant.load !94
// CHECK:   %[[VAL_58:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_59:.*]] = getelementptr inbounds float, float* %[[VAL_58]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_60:.*]] = load float, float* %[[VAL_59]], align 4, !invariant.load !94
// CHECK:   %[[VAL_61:.*]] = fcmp oeq float %[[VAL_57]], %[[VAL_60]]
// CHECK:   %[[VAL_62:.*]] = zext i1 %[[VAL_61]] to i8
// CHECK:   %[[VAL_63:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_64:.*]] = getelementptr inbounds i8, i8* %[[VAL_63]], i32 %[[VAL_23]]
// CHECK:   store i8 %[[VAL_62]], i8* %[[VAL_64]], align 1
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds float, float* %[[VAL_68]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_70:.*]] = load float, float* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   %[[VAL_71:.*]] = fcmp oeq float %[[VAL_67]], %[[VAL_70]]
// CHECK:   %[[VAL_72:.*]] = zext i1 %[[VAL_71]] to i8
// CHECK:   %[[VAL_73:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_74:.*]] = getelementptr inbounds i8, i8* %[[VAL_73]], i32 %[[VAL_27]]
// CHECK:   store i8 %[[VAL_72]], i8* %[[VAL_74]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r30(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to [100 x [200 x %[[VAL_9:.*]]]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r30.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r30.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = insertvalue %[[VAL_9]] zeroinitializer, float %[[VAL_37]], 0
// CHECK:   %[[VAL_42:.*]] = insertvalue %[[VAL_9]] %[[VAL_41]], float %[[VAL_40]], 1
// CHECK:   %[[VAL_43:.*]] = bitcast [100 x [200 x %[[VAL_9]]]]* %[[VAL_8]] to %[[VAL_9]]*
// CHECK:   %[[VAL_44:.*]] = getelementptr inbounds %[[VAL_9]], %[[VAL_9]]* %[[VAL_43]], i32 %[[VAL_15]]
// CHECK:   store %[[VAL_9]] %[[VAL_42]], %[[VAL_9]]* %[[VAL_44]], align 1
// CHECK:   %[[VAL_45:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_46:.*]] = getelementptr inbounds float, float* %[[VAL_45]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_47:.*]] = load float, float* %[[VAL_46]], align 4, !invariant.load !94
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds float, float* %[[VAL_48]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_50:.*]] = load float, float* %[[VAL_49]], align 4, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = insertvalue %[[VAL_9]] zeroinitializer, float %[[VAL_47]], 0
// CHECK:   %[[VAL_52:.*]] = insertvalue %[[VAL_9]] %[[VAL_51]], float %[[VAL_50]], 1
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x %[[VAL_9]]]]* %[[VAL_8]] to %[[VAL_9]]*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds %[[VAL_9]], %[[VAL_9]]* %[[VAL_53]], i32 %[[VAL_19]]
// CHECK:   store %[[VAL_9]] %[[VAL_52]], %[[VAL_9]]* %[[VAL_54]], align 1
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds float, float* %[[VAL_55]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_57:.*]] = load float, float* %[[VAL_56]], align 4, !invariant.load !94
// CHECK:   %[[VAL_58:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_59:.*]] = getelementptr inbounds float, float* %[[VAL_58]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_60:.*]] = load float, float* %[[VAL_59]], align 4, !invariant.load !94
// CHECK:   %[[VAL_61:.*]] = insertvalue %[[VAL_9]] zeroinitializer, float %[[VAL_57]], 0
// CHECK:   %[[VAL_62:.*]] = insertvalue %[[VAL_9]] %[[VAL_61]], float %[[VAL_60]], 1
// CHECK:   %[[VAL_63:.*]] = bitcast [100 x [200 x %[[VAL_9]]]]* %[[VAL_8]] to %[[VAL_9]]*
// CHECK:   %[[VAL_64:.*]] = getelementptr inbounds %[[VAL_9]], %[[VAL_9]]* %[[VAL_63]], i32 %[[VAL_23]]
// CHECK:   store %[[VAL_9]] %[[VAL_62]], %[[VAL_9]]* %[[VAL_64]], align 1
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_4]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds float, float* %[[VAL_68]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_70:.*]] = load float, float* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   %[[VAL_71:.*]] = insertvalue %[[VAL_9]] zeroinitializer, float %[[VAL_67]], 0
// CHECK:   %[[VAL_72:.*]] = insertvalue %[[VAL_9]] %[[VAL_71]], float %[[VAL_70]], 1
// CHECK:   %[[VAL_73:.*]] = bitcast [100 x [200 x %[[VAL_9]]]]* %[[VAL_8]] to %[[VAL_9]]*
// CHECK:   %[[VAL_74:.*]] = getelementptr inbounds %[[VAL_9]], %[[VAL_9]]* %[[VAL_73]], i32 %[[VAL_27]]
// CHECK:   store %[[VAL_9]] %[[VAL_72]], %[[VAL_9]]* %[[VAL_74]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r31(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r31.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r31.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fdiv float %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = fdiv float %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = fdiv float %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = fdiv float %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r32(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r32.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r32.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.maxnum.f32(float %[[VAL_37]], float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = call float @llvm.maxnum.f32(float %[[VAL_46]], float %[[VAL_49]])
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = call float @llvm.maxnum.f32(float %[[VAL_55]], float %[[VAL_58]])
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = call float @llvm.maxnum.f32(float %[[VAL_64]], float %[[VAL_67]])
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.maxnum.f32(float, float) #2

// CHECK: define void @r33(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r33.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r33.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @llvm.minnum.f32(float %[[VAL_37]], float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = call float @llvm.minnum.f32(float %[[VAL_46]], float %[[VAL_49]])
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = call float @llvm.minnum.f32(float %[[VAL_55]], float %[[VAL_58]])
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = call float @llvm.minnum.f32(float %[[VAL_64]], float %[[VAL_67]])
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }
// CHECK: ; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
// CHECK: declare float @llvm.minnum.f32(float, float) #2

// CHECK: define void @r34(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r34.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r34.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fmul float %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = fmul float %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = fmul float %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = fmul float %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r35(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !95
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 1024
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 20480
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = udiv i32 %[[VAL_13]], 1
// CHECK:   %[[VAL_16:.*]] = urem i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_17:.*]] = udiv i32 %[[VAL_13]], 200
// CHECK:   %[[VAL_18:.*]] = icmp ult i32 %[[VAL_13]], 20000
// CHECK:   br i1 %[[VAL_18]], label %[[VAL_19:.*]], label %[[VAL_20:.*]]
// CHECK: r35.in_bounds-after:                              ; preds = %[[VAL_19]], %[[VAL_21:.*]]
// CHECK:   ret void
// CHECK: r35.in_bounds-true:                               ; preds = %[[VAL_21]]
// CHECK:   %[[VAL_22:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_23:.*]] = getelementptr inbounds float, float* %[[VAL_22]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_24:.*]] = load float, float* %[[VAL_23]], align 4, !invariant.load !94
// CHECK:   %[[VAL_25:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_26:.*]] = getelementptr inbounds float, float* %[[VAL_25]], i32 %[[VAL_13]]
// CHECK:   %[[VAL_27:.*]] = load float, float* %[[VAL_26]], align 4, !invariant.load !94
// CHECK:   %[[VAL_28:.*]] = call float @__nv_powf(float %[[VAL_24]], float %[[VAL_27]])
// CHECK:   %[[VAL_29:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_30:.*]] = getelementptr inbounds float, float* %[[VAL_29]], i32 %[[VAL_13]]
// CHECK:   store float %[[VAL_28]], float* %[[VAL_30]], align 4
// CHECK:   br label %[[VAL_20]]
// CHECK: }

// CHECK: define void @r36(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r36.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r36.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = call float @__nv_fmodf(float %[[VAL_37]], float %[[VAL_40]])
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = call float @__nv_fmodf(float %[[VAL_46]], float %[[VAL_49]])
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = call float @__nv_fmodf(float %[[VAL_55]], float %[[VAL_58]])
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = call float @__nv_fmodf(float %[[VAL_64]], float %[[VAL_67]])
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }
// CHECK: ; Function Attrs: nounwind readnone
// CHECK: declare float @__nv_fmodf(float, float) #0

// CHECK: define void @r37(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r37.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r37.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds float, float* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load float, float* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = fsub float %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds float, float* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store float %[[VAL_41]], float* %[[VAL_43]], align 4
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = fsub float %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = fsub float %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds float, float* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store float %[[VAL_59]], float* %[[VAL_61]], align 4
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_5]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load float, float* %[[VAL_63]], align 4, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_7]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = fsub float %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x float]]* %[[VAL_9]] to float*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds float, float* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_68]], float* %[[VAL_70]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r38(i8* noalias align 16 dereferenceable(20000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(20000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r38.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r38.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i8, i8* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i8, i8* %[[VAL_36]], align 1, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i8, i8* %[[VAL_39]], align 1, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = and i8 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i8, i8* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store i8 %[[VAL_41]], i8* %[[VAL_43]], align 1
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i8, i8* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load i8, i8* %[[VAL_45]], align 1, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds i8, i8* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load i8, i8* %[[VAL_48]], align 1, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = and i8 %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i8, i8* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store i8 %[[VAL_50]], i8* %[[VAL_52]], align 1
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds i8, i8* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load i8, i8* %[[VAL_54]], align 1, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds i8, i8* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load i8, i8* %[[VAL_57]], align 1, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = and i8 %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i8, i8* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store i8 %[[VAL_59]], i8* %[[VAL_61]], align 1
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load i8, i8* %[[VAL_63]], align 1, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds i8, i8* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load i8, i8* %[[VAL_66]], align 1, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = and i8 %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds i8, i8* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store i8 %[[VAL_68]], i8* %[[VAL_70]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r39(i8* noalias align 16 dereferenceable(20000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(20000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r39.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r39.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i8, i8* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i8, i8* %[[VAL_36]], align 1, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i8, i8* %[[VAL_39]], align 1, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = or i8 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i8, i8* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store i8 %[[VAL_41]], i8* %[[VAL_43]], align 1
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i8, i8* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load i8, i8* %[[VAL_45]], align 1, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds i8, i8* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load i8, i8* %[[VAL_48]], align 1, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = or i8 %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i8, i8* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store i8 %[[VAL_50]], i8* %[[VAL_52]], align 1
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds i8, i8* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load i8, i8* %[[VAL_54]], align 1, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds i8, i8* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load i8, i8* %[[VAL_57]], align 1, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = or i8 %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i8, i8* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store i8 %[[VAL_59]], i8* %[[VAL_61]], align 1
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load i8, i8* %[[VAL_63]], align 1, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds i8, i8* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load i8, i8* %[[VAL_66]], align 1, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = or i8 %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds i8, i8* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store i8 %[[VAL_68]], i8* %[[VAL_70]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r40(i8* noalias align 16 dereferenceable(20000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(20000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r40.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r40.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i8, i8* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i8, i8* %[[VAL_36]], align 1, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i8, i8* %[[VAL_39]], align 1, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = xor i8 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i8, i8* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store i8 %[[VAL_41]], i8* %[[VAL_43]], align 1
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i8, i8* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load i8, i8* %[[VAL_45]], align 1, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds i8, i8* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load i8, i8* %[[VAL_48]], align 1, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = xor i8 %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i8, i8* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store i8 %[[VAL_50]], i8* %[[VAL_52]], align 1
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds i8, i8* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load i8, i8* %[[VAL_54]], align 1, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds i8, i8* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load i8, i8* %[[VAL_57]], align 1, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = xor i8 %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i8, i8* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store i8 %[[VAL_59]], i8* %[[VAL_61]], align 1
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load i8, i8* %[[VAL_63]], align 1, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds i8, i8* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load i8, i8* %[[VAL_66]], align 1, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = xor i8 %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds i8, i8* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store i8 %[[VAL_68]], i8* %[[VAL_70]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r41(i8* noalias align 16 dereferenceable(20000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(20000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r41.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r41.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i8, i8* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i8, i8* %[[VAL_36]], align 1, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i8, i8* %[[VAL_39]], align 1, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = or i8 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_43:.*]] = getelementptr inbounds i8, i8* %[[VAL_42]], i32 %[[VAL_15]]
// CHECK:   store i8 %[[VAL_41]], i8* %[[VAL_43]], align 1
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i8, i8* %[[VAL_44]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_46:.*]] = load i8, i8* %[[VAL_45]], align 1, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds i8, i8* %[[VAL_47]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_49:.*]] = load i8, i8* %[[VAL_48]], align 1, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = or i8 %[[VAL_46]], %[[VAL_49]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i8, i8* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   store i8 %[[VAL_50]], i8* %[[VAL_52]], align 1
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds i8, i8* %[[VAL_53]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_55:.*]] = load i8, i8* %[[VAL_54]], align 1, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds i8, i8* %[[VAL_56]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_58:.*]] = load i8, i8* %[[VAL_57]], align 1, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = or i8 %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i8, i8* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   store i8 %[[VAL_59]], i8* %[[VAL_61]], align 1
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_5]] to i8*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds i8, i8* %[[VAL_62]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_64:.*]] = load i8, i8* %[[VAL_63]], align 1, !invariant.load !94
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_7]] to i8*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds i8, i8* %[[VAL_65]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_67:.*]] = load i8, i8* %[[VAL_66]], align 1, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = or i8 %[[VAL_64]], %[[VAL_67]]
// CHECK:   %[[VAL_69:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_9]] to i8*
// CHECK:   %[[VAL_70:.*]] = getelementptr inbounds i8, i8* %[[VAL_69]], i32 %[[VAL_27]]
// CHECK:   store i8 %[[VAL_68]], i8* %[[VAL_70]], align 1
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r42(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r42.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r42.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i32, i32* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i32, i32* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i32, i32* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i32, i32* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = shl i32 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = icmp ult i32 %[[VAL_40]], 32
// CHECK:   %[[VAL_43:.*]] = select i1 %[[VAL_42]], i32 %[[VAL_41]], i32 0
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i32, i32* %[[VAL_44]], i32 %[[VAL_15]]
// CHECK:   store i32 %[[VAL_43]], i32* %[[VAL_45]], align 4
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds i32, i32* %[[VAL_46]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_48:.*]] = load i32, i32* %[[VAL_47]], align 4, !invariant.load !94
// CHECK:   %[[VAL_49:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_50:.*]] = getelementptr inbounds i32, i32* %[[VAL_49]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_51:.*]] = load i32, i32* %[[VAL_50]], align 4, !invariant.load !94
// CHECK:   %[[VAL_52:.*]] = shl i32 %[[VAL_48]], %[[VAL_51]]
// CHECK:   %[[VAL_53:.*]] = icmp ult i32 %[[VAL_51]], 32
// CHECK:   %[[VAL_54:.*]] = select i1 %[[VAL_53]], i32 %[[VAL_52]], i32 0
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds i32, i32* %[[VAL_55]], i32 %[[VAL_19]]
// CHECK:   store i32 %[[VAL_54]], i32* %[[VAL_56]], align 4
// CHECK:   %[[VAL_57:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_58:.*]] = getelementptr inbounds i32, i32* %[[VAL_57]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_59:.*]] = load i32, i32* %[[VAL_58]], align 4, !invariant.load !94
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i32, i32* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_62:.*]] = load i32, i32* %[[VAL_61]], align 4, !invariant.load !94
// CHECK:   %[[VAL_63:.*]] = shl i32 %[[VAL_59]], %[[VAL_62]]
// CHECK:   %[[VAL_64:.*]] = icmp ult i32 %[[VAL_62]], 32
// CHECK:   %[[VAL_65:.*]] = select i1 %[[VAL_64]], i32 %[[VAL_63]], i32 0
// CHECK:   %[[VAL_66:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_67:.*]] = getelementptr inbounds i32, i32* %[[VAL_66]], i32 %[[VAL_23]]
// CHECK:   store i32 %[[VAL_65]], i32* %[[VAL_67]], align 4
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds i32, i32* %[[VAL_68]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_70:.*]] = load i32, i32* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   %[[VAL_71:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_72:.*]] = getelementptr inbounds i32, i32* %[[VAL_71]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_73:.*]] = load i32, i32* %[[VAL_72]], align 4, !invariant.load !94
// CHECK:   %[[VAL_74:.*]] = shl i32 %[[VAL_70]], %[[VAL_73]]
// CHECK:   %[[VAL_75:.*]] = icmp ult i32 %[[VAL_73]], 32
// CHECK:   %[[VAL_76:.*]] = select i1 %[[VAL_75]], i32 %[[VAL_74]], i32 0
// CHECK:   %[[VAL_77:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_78:.*]] = getelementptr inbounds i32, i32* %[[VAL_77]], i32 %[[VAL_27]]
// CHECK:   store i32 %[[VAL_76]], i32* %[[VAL_78]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r43(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r43.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r43.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i32, i32* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i32, i32* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i32, i32* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i32, i32* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = ashr i32 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = icmp slt i32 %[[VAL_37]], 0
// CHECK:   %[[VAL_43:.*]] = select i1 %[[VAL_42]], i32 -1, i32 0
// CHECK:   %[[VAL_44:.*]] = icmp ult i32 %[[VAL_40]], 32
// CHECK:   %[[VAL_45:.*]] = select i1 %[[VAL_44]], i32 %[[VAL_41]], i32 %[[VAL_43]]
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds i32, i32* %[[VAL_46]], i32 %[[VAL_15]]
// CHECK:   store i32 %[[VAL_45]], i32* %[[VAL_47]], align 4
// CHECK:   %[[VAL_48:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_49:.*]] = getelementptr inbounds i32, i32* %[[VAL_48]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_50:.*]] = load i32, i32* %[[VAL_49]], align 4, !invariant.load !94
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i32, i32* %[[VAL_51]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_53:.*]] = load i32, i32* %[[VAL_52]], align 4, !invariant.load !94
// CHECK:   %[[VAL_54:.*]] = ashr i32 %[[VAL_50]], %[[VAL_53]]
// CHECK:   %[[VAL_55:.*]] = icmp slt i32 %[[VAL_50]], 0
// CHECK:   %[[VAL_56:.*]] = select i1 %[[VAL_55]], i32 -1, i32 0
// CHECK:   %[[VAL_57:.*]] = icmp ult i32 %[[VAL_53]], 32
// CHECK:   %[[VAL_58:.*]] = select i1 %[[VAL_57]], i32 %[[VAL_54]], i32 %[[VAL_56]]
// CHECK:   %[[VAL_59:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_60:.*]] = getelementptr inbounds i32, i32* %[[VAL_59]], i32 %[[VAL_19]]
// CHECK:   store i32 %[[VAL_58]], i32* %[[VAL_60]], align 4
// CHECK:   %[[VAL_61:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_62:.*]] = getelementptr inbounds i32, i32* %[[VAL_61]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_63:.*]] = load i32, i32* %[[VAL_62]], align 4, !invariant.load !94
// CHECK:   %[[VAL_64:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_65:.*]] = getelementptr inbounds i32, i32* %[[VAL_64]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_66:.*]] = load i32, i32* %[[VAL_65]], align 4, !invariant.load !94
// CHECK:   %[[VAL_67:.*]] = ashr i32 %[[VAL_63]], %[[VAL_66]]
// CHECK:   %[[VAL_68:.*]] = icmp slt i32 %[[VAL_63]], 0
// CHECK:   %[[VAL_69:.*]] = select i1 %[[VAL_68]], i32 -1, i32 0
// CHECK:   %[[VAL_70:.*]] = icmp ult i32 %[[VAL_66]], 32
// CHECK:   %[[VAL_71:.*]] = select i1 %[[VAL_70]], i32 %[[VAL_67]], i32 %[[VAL_69]]
// CHECK:   %[[VAL_72:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_73:.*]] = getelementptr inbounds i32, i32* %[[VAL_72]], i32 %[[VAL_23]]
// CHECK:   store i32 %[[VAL_71]], i32* %[[VAL_73]], align 4
// CHECK:   %[[VAL_74:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_75:.*]] = getelementptr inbounds i32, i32* %[[VAL_74]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_76:.*]] = load i32, i32* %[[VAL_75]], align 4, !invariant.load !94
// CHECK:   %[[VAL_77:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_78:.*]] = getelementptr inbounds i32, i32* %[[VAL_77]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_79:.*]] = load i32, i32* %[[VAL_78]], align 4, !invariant.load !94
// CHECK:   %[[VAL_80:.*]] = ashr i32 %[[VAL_76]], %[[VAL_79]]
// CHECK:   %[[VAL_81:.*]] = icmp slt i32 %[[VAL_76]], 0
// CHECK:   %[[VAL_82:.*]] = select i1 %[[VAL_81]], i32 -1, i32 0
// CHECK:   %[[VAL_83:.*]] = icmp ult i32 %[[VAL_79]], 32
// CHECK:   %[[VAL_84:.*]] = select i1 %[[VAL_83]], i32 %[[VAL_80]], i32 %[[VAL_82]]
// CHECK:   %[[VAL_85:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_86:.*]] = getelementptr inbounds i32, i32* %[[VAL_85]], i32 %[[VAL_27]]
// CHECK:   store i32 %[[VAL_84]], i32* %[[VAL_86]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r44(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_8:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to [100 x [200 x i32]]*
// CHECK:   %[[VAL_10:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_11:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_12:.*]] = mul nuw nsw i32 %[[VAL_10]], 256
// CHECK:   %[[VAL_13:.*]] = add nuw nsw i32 %[[VAL_12]], %[[VAL_11]]
// CHECK:   %[[VAL_14:.*]] = icmp ult i32 %[[VAL_13]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_14]])
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 4
// CHECK:   %[[VAL_16:.*]] = udiv i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_17:.*]] = urem i32 %[[VAL_16]], 200
// CHECK:   %[[VAL_18:.*]] = udiv i32 %[[VAL_15]], 200
// CHECK:   %[[VAL_19:.*]] = add nuw nsw i32 %[[VAL_15]], 1
// CHECK:   %[[VAL_20:.*]] = udiv i32 %[[VAL_19]], 1
// CHECK:   %[[VAL_21:.*]] = urem i32 %[[VAL_20]], 200
// CHECK:   %[[VAL_22:.*]] = udiv i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_23:.*]] = add nuw nsw i32 %[[VAL_15]], 2
// CHECK:   %[[VAL_24:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:   %[[VAL_25:.*]] = urem i32 %[[VAL_24]], 200
// CHECK:   %[[VAL_26:.*]] = udiv i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_15]], 3
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = icmp ult i32 %[[VAL_15]], 20000
// CHECK:   br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK: r44.in_bounds-after:                              ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:   ret void
// CHECK: r44.in_bounds-true:                               ; preds = %[[VAL_34]]
// CHECK:   %[[VAL_35:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_36:.*]] = getelementptr inbounds i32, i32* %[[VAL_35]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_37:.*]] = load i32, i32* %[[VAL_36]], align 4, !invariant.load !94
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i32, i32* %[[VAL_38]], i32 %[[VAL_15]]
// CHECK:   %[[VAL_40:.*]] = load i32, i32* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = lshr i32 %[[VAL_37]], %[[VAL_40]]
// CHECK:   %[[VAL_42:.*]] = icmp ult i32 %[[VAL_40]], 32
// CHECK:   %[[VAL_43:.*]] = select i1 %[[VAL_42]], i32 %[[VAL_41]], i32 0
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds i32, i32* %[[VAL_44]], i32 %[[VAL_15]]
// CHECK:   store i32 %[[VAL_43]], i32* %[[VAL_45]], align 4
// CHECK:   %[[VAL_46:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_47:.*]] = getelementptr inbounds i32, i32* %[[VAL_46]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_48:.*]] = load i32, i32* %[[VAL_47]], align 4, !invariant.load !94
// CHECK:   %[[VAL_49:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_50:.*]] = getelementptr inbounds i32, i32* %[[VAL_49]], i32 %[[VAL_19]]
// CHECK:   %[[VAL_51:.*]] = load i32, i32* %[[VAL_50]], align 4, !invariant.load !94
// CHECK:   %[[VAL_52:.*]] = lshr i32 %[[VAL_48]], %[[VAL_51]]
// CHECK:   %[[VAL_53:.*]] = icmp ult i32 %[[VAL_51]], 32
// CHECK:   %[[VAL_54:.*]] = select i1 %[[VAL_53]], i32 %[[VAL_52]], i32 0
// CHECK:   %[[VAL_55:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_56:.*]] = getelementptr inbounds i32, i32* %[[VAL_55]], i32 %[[VAL_19]]
// CHECK:   store i32 %[[VAL_54]], i32* %[[VAL_56]], align 4
// CHECK:   %[[VAL_57:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_58:.*]] = getelementptr inbounds i32, i32* %[[VAL_57]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_59:.*]] = load i32, i32* %[[VAL_58]], align 4, !invariant.load !94
// CHECK:   %[[VAL_60:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_61:.*]] = getelementptr inbounds i32, i32* %[[VAL_60]], i32 %[[VAL_23]]
// CHECK:   %[[VAL_62:.*]] = load i32, i32* %[[VAL_61]], align 4, !invariant.load !94
// CHECK:   %[[VAL_63:.*]] = lshr i32 %[[VAL_59]], %[[VAL_62]]
// CHECK:   %[[VAL_64:.*]] = icmp ult i32 %[[VAL_62]], 32
// CHECK:   %[[VAL_65:.*]] = select i1 %[[VAL_64]], i32 %[[VAL_63]], i32 0
// CHECK:   %[[VAL_66:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_67:.*]] = getelementptr inbounds i32, i32* %[[VAL_66]], i32 %[[VAL_23]]
// CHECK:   store i32 %[[VAL_65]], i32* %[[VAL_67]], align 4
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_5]] to i32*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds i32, i32* %[[VAL_68]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_70:.*]] = load i32, i32* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   %[[VAL_71:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_7]] to i32*
// CHECK:   %[[VAL_72:.*]] = getelementptr inbounds i32, i32* %[[VAL_71]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_73:.*]] = load i32, i32* %[[VAL_72]], align 4, !invariant.load !94
// CHECK:   %[[VAL_74:.*]] = lshr i32 %[[VAL_70]], %[[VAL_73]]
// CHECK:   %[[VAL_75:.*]] = icmp ult i32 %[[VAL_73]], 32
// CHECK:   %[[VAL_76:.*]] = select i1 %[[VAL_75]], i32 %[[VAL_74]], i32 0
// CHECK:   %[[VAL_77:.*]] = bitcast [100 x [200 x i32]]* %[[VAL_9]] to i32*
// CHECK:   %[[VAL_78:.*]] = getelementptr inbounds i32, i32* %[[VAL_77]], i32 %[[VAL_27]]
// CHECK:   store i32 %[[VAL_76]], i32* %[[VAL_78]], align 4
// CHECK:   br label %[[VAL_33]]
// CHECK: }

// CHECK: define void @r45(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 16 dereferenceable(20000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_3:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_4:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x i8]]*
// CHECK:   %[[VAL_7:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_11:.*]] = getelementptr inbounds i8, i8* %[[VAL_3]], i64 0
// CHECK:   %[[VAL_12:.*]] = bitcast i8* %[[VAL_11]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_13:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_14:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 256
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_15]], %[[VAL_14]]
// CHECK:   %[[VAL_17:.*]] = icmp ult i32 %[[VAL_16]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_17]])
// CHECK:   %[[VAL_18:.*]] = mul nuw nsw i32 %[[VAL_16]], 4
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_18]], 1
// CHECK:   %[[VAL_20:.*]] = urem i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_18]], 200
// CHECK:   %[[VAL_22:.*]] = add nuw nsw i32 %[[VAL_18]], 1
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_22]], 1
// CHECK:   %[[VAL_24:.*]] = urem i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_22]], 200
// CHECK:   %[[VAL_26:.*]] = add nuw nsw i32 %[[VAL_18]], 2
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_26]], 1
// CHECK:   %[[VAL_28:.*]] = urem i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_29:.*]] = udiv i32 %[[VAL_26]], 200
// CHECK:   %[[VAL_30:.*]] = add nuw nsw i32 %[[VAL_18]], 3
// CHECK:   %[[VAL_31:.*]] = udiv i32 %[[VAL_30]], 1
// CHECK:   %[[VAL_32:.*]] = urem i32 %[[VAL_31]], 200
// CHECK:   %[[VAL_33:.*]] = udiv i32 %[[VAL_30]], 200
// CHECK:   %[[VAL_34:.*]] = icmp ult i32 %[[VAL_18]], 20000
// CHECK:   br i1 %[[VAL_34]], label %[[VAL_35:.*]], label %[[VAL_36:.*]]
// CHECK: r45.in_bounds-after:                              ; preds = %[[VAL_35]], %[[VAL_37:.*]]
// CHECK:   ret void
// CHECK: r45.in_bounds-true:                               ; preds = %[[VAL_37]]
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds i8, i8* %[[VAL_38]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_40:.*]] = load i8, i8* %[[VAL_39]], align 1, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_42:.*]] = getelementptr inbounds float, float* %[[VAL_41]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_43:.*]] = load float, float* %[[VAL_42]], align 4, !invariant.load !94
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = trunc i8 %[[VAL_40]] to i1
// CHECK:   %[[VAL_48:.*]] = select i1 %[[VAL_47]], float %[[VAL_43]], float %[[VAL_46]]
// CHECK:   %[[VAL_49:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_50:.*]] = getelementptr inbounds float, float* %[[VAL_49]], i32 %[[VAL_18]]
// CHECK:   store float %[[VAL_48]], float* %[[VAL_50]], align 4
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds i8, i8* %[[VAL_51]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_53:.*]] = load i8, i8* %[[VAL_52]], align 1, !invariant.load !94
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_56:.*]] = load float, float* %[[VAL_55]], align 4, !invariant.load !94
// CHECK:   %[[VAL_57:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_58:.*]] = getelementptr inbounds float, float* %[[VAL_57]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_59:.*]] = load float, float* %[[VAL_58]], align 4, !invariant.load !94
// CHECK:   %[[VAL_60:.*]] = trunc i8 %[[VAL_53]] to i1
// CHECK:   %[[VAL_61:.*]] = select i1 %[[VAL_60]], float %[[VAL_56]], float %[[VAL_59]]
// CHECK:   %[[VAL_62:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i32 %[[VAL_22]]
// CHECK:   store float %[[VAL_61]], float* %[[VAL_63]], align 4
// CHECK:   %[[VAL_64:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_65:.*]] = getelementptr inbounds i8, i8* %[[VAL_64]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_66:.*]] = load i8, i8* %[[VAL_65]], align 1, !invariant.load !94
// CHECK:   %[[VAL_67:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_68:.*]] = getelementptr inbounds float, float* %[[VAL_67]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_69:.*]] = load float, float* %[[VAL_68]], align 4, !invariant.load !94
// CHECK:   %[[VAL_70:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_71:.*]] = getelementptr inbounds float, float* %[[VAL_70]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_72:.*]] = load float, float* %[[VAL_71]], align 4, !invariant.load !94
// CHECK:   %[[VAL_73:.*]] = trunc i8 %[[VAL_66]] to i1
// CHECK:   %[[VAL_74:.*]] = select i1 %[[VAL_73]], float %[[VAL_69]], float %[[VAL_72]]
// CHECK:   %[[VAL_75:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_76:.*]] = getelementptr inbounds float, float* %[[VAL_75]], i32 %[[VAL_26]]
// CHECK:   store float %[[VAL_74]], float* %[[VAL_76]], align 4
// CHECK:   %[[VAL_77:.*]] = bitcast [100 x [200 x i8]]* %[[VAL_6]] to i8*
// CHECK:   %[[VAL_78:.*]] = getelementptr inbounds i8, i8* %[[VAL_77]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_79:.*]] = load i8, i8* %[[VAL_78]], align 1, !invariant.load !94
// CHECK:   %[[VAL_80:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_81:.*]] = getelementptr inbounds float, float* %[[VAL_80]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_82:.*]] = load float, float* %[[VAL_81]], align 4, !invariant.load !94
// CHECK:   %[[VAL_83:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_84:.*]] = getelementptr inbounds float, float* %[[VAL_83]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_85:.*]] = load float, float* %[[VAL_84]], align 4, !invariant.load !94
// CHECK:   %[[VAL_86:.*]] = trunc i8 %[[VAL_79]] to i1
// CHECK:   %[[VAL_87:.*]] = select i1 %[[VAL_86]], float %[[VAL_82]], float %[[VAL_85]]
// CHECK:   %[[VAL_88:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_89:.*]] = getelementptr inbounds float, float* %[[VAL_88]], i32 %[[VAL_30]]
// CHECK:   store float %[[VAL_87]], float* %[[VAL_89]], align 4
// CHECK:   br label %[[VAL_36]]
// CHECK: }

// CHECK: define void @r46(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_3:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_4:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_7:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_11:.*]] = getelementptr inbounds i8, i8* %[[VAL_3]], i64 0
// CHECK:   %[[VAL_12:.*]] = bitcast i8* %[[VAL_11]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_13:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_14:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_15:.*]] = mul nuw nsw i32 %[[VAL_13]], 256
// CHECK:   %[[VAL_16:.*]] = add nuw nsw i32 %[[VAL_15]], %[[VAL_14]]
// CHECK:   %[[VAL_17:.*]] = icmp ult i32 %[[VAL_16]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_17]])
// CHECK:   %[[VAL_18:.*]] = mul nuw nsw i32 %[[VAL_16]], 4
// CHECK:   %[[VAL_19:.*]] = udiv i32 %[[VAL_18]], 1
// CHECK:   %[[VAL_20:.*]] = urem i32 %[[VAL_19]], 200
// CHECK:   %[[VAL_21:.*]] = udiv i32 %[[VAL_18]], 200
// CHECK:   %[[VAL_22:.*]] = add nuw nsw i32 %[[VAL_18]], 1
// CHECK:   %[[VAL_23:.*]] = udiv i32 %[[VAL_22]], 1
// CHECK:   %[[VAL_24:.*]] = urem i32 %[[VAL_23]], 200
// CHECK:   %[[VAL_25:.*]] = udiv i32 %[[VAL_22]], 200
// CHECK:   %[[VAL_26:.*]] = add nuw nsw i32 %[[VAL_18]], 2
// CHECK:   %[[VAL_27:.*]] = udiv i32 %[[VAL_26]], 1
// CHECK:   %[[VAL_28:.*]] = urem i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_29:.*]] = udiv i32 %[[VAL_26]], 200
// CHECK:   %[[VAL_30:.*]] = add nuw nsw i32 %[[VAL_18]], 3
// CHECK:   %[[VAL_31:.*]] = udiv i32 %[[VAL_30]], 1
// CHECK:   %[[VAL_32:.*]] = urem i32 %[[VAL_31]], 200
// CHECK:   %[[VAL_33:.*]] = udiv i32 %[[VAL_30]], 200
// CHECK:   %[[VAL_34:.*]] = icmp ult i32 %[[VAL_18]], 20000
// CHECK:   br i1 %[[VAL_34]], label %[[VAL_35:.*]], label %[[VAL_36:.*]]
// CHECK: r46.in_bounds-after:                              ; preds = %[[VAL_35]], %[[VAL_37:.*]]
// CHECK:   ret void
// CHECK: r46.in_bounds-true:                               ; preds = %[[VAL_37]]
// CHECK:   %[[VAL_38:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_39:.*]] = getelementptr inbounds float, float* %[[VAL_38]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_40:.*]] = load float, float* %[[VAL_39]], align 4, !invariant.load !94
// CHECK:   %[[VAL_41:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_42:.*]] = getelementptr inbounds float, float* %[[VAL_41]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_43:.*]] = load float, float* %[[VAL_42]], align 4, !invariant.load !94
// CHECK:   %[[VAL_44:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_45:.*]] = getelementptr inbounds float, float* %[[VAL_44]], i32 %[[VAL_18]]
// CHECK:   %[[VAL_46:.*]] = load float, float* %[[VAL_45]], align 4, !invariant.load !94
// CHECK:   %[[VAL_47:.*]] = fcmp uge float %[[VAL_40]], %[[VAL_43]]
// CHECK:   %[[VAL_48:.*]] = select i1 %[[VAL_47]], float %[[VAL_40]], float %[[VAL_43]]
// CHECK:   %[[VAL_49:.*]] = fcmp ule float %[[VAL_46]], %[[VAL_48]]
// CHECK:   %[[VAL_50:.*]] = select i1 %[[VAL_49]], float %[[VAL_46]], float %[[VAL_48]]
// CHECK:   %[[VAL_51:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_52:.*]] = getelementptr inbounds float, float* %[[VAL_51]], i32 %[[VAL_18]]
// CHECK:   store float %[[VAL_50]], float* %[[VAL_52]], align 4
// CHECK:   %[[VAL_53:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_54:.*]] = getelementptr inbounds float, float* %[[VAL_53]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_55:.*]] = load float, float* %[[VAL_54]], align 4, !invariant.load !94
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_60:.*]] = getelementptr inbounds float, float* %[[VAL_59]], i32 %[[VAL_22]]
// CHECK:   %[[VAL_61:.*]] = load float, float* %[[VAL_60]], align 4, !invariant.load !94
// CHECK:   %[[VAL_62:.*]] = fcmp uge float %[[VAL_55]], %[[VAL_58]]
// CHECK:   %[[VAL_63:.*]] = select i1 %[[VAL_62]], float %[[VAL_55]], float %[[VAL_58]]
// CHECK:   %[[VAL_64:.*]] = fcmp ule float %[[VAL_61]], %[[VAL_63]]
// CHECK:   %[[VAL_65:.*]] = select i1 %[[VAL_64]], float %[[VAL_61]], float %[[VAL_63]]
// CHECK:   %[[VAL_66:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_67:.*]] = getelementptr inbounds float, float* %[[VAL_66]], i32 %[[VAL_22]]
// CHECK:   store float %[[VAL_65]], float* %[[VAL_67]], align 4
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds float, float* %[[VAL_68]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_70:.*]] = load float, float* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   %[[VAL_71:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_72:.*]] = getelementptr inbounds float, float* %[[VAL_71]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_73:.*]] = load float, float* %[[VAL_72]], align 4, !invariant.load !94
// CHECK:   %[[VAL_74:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_75:.*]] = getelementptr inbounds float, float* %[[VAL_74]], i32 %[[VAL_26]]
// CHECK:   %[[VAL_76:.*]] = load float, float* %[[VAL_75]], align 4, !invariant.load !94
// CHECK:   %[[VAL_77:.*]] = fcmp uge float %[[VAL_70]], %[[VAL_73]]
// CHECK:   %[[VAL_78:.*]] = select i1 %[[VAL_77]], float %[[VAL_70]], float %[[VAL_73]]
// CHECK:   %[[VAL_79:.*]] = fcmp ule float %[[VAL_76]], %[[VAL_78]]
// CHECK:   %[[VAL_80:.*]] = select i1 %[[VAL_79]], float %[[VAL_76]], float %[[VAL_78]]
// CHECK:   %[[VAL_81:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_82:.*]] = getelementptr inbounds float, float* %[[VAL_81]], i32 %[[VAL_26]]
// CHECK:   store float %[[VAL_80]], float* %[[VAL_82]], align 4
// CHECK:   %[[VAL_83:.*]] = bitcast [100 x [200 x float]]* %[[VAL_6]] to float*
// CHECK:   %[[VAL_84:.*]] = getelementptr inbounds float, float* %[[VAL_83]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_85:.*]] = load float, float* %[[VAL_84]], align 4, !invariant.load !94
// CHECK:   %[[VAL_86:.*]] = bitcast [100 x [200 x float]]* %[[VAL_8]] to float*
// CHECK:   %[[VAL_87:.*]] = getelementptr inbounds float, float* %[[VAL_86]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_88:.*]] = load float, float* %[[VAL_87]], align 4, !invariant.load !94
// CHECK:   %[[VAL_89:.*]] = bitcast [100 x [200 x float]]* %[[VAL_10]] to float*
// CHECK:   %[[VAL_90:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 %[[VAL_30]]
// CHECK:   %[[VAL_91:.*]] = load float, float* %[[VAL_90]], align 4, !invariant.load !94
// CHECK:   %[[VAL_92:.*]] = fcmp uge float %[[VAL_85]], %[[VAL_88]]
// CHECK:   %[[VAL_93:.*]] = select i1 %[[VAL_92]], float %[[VAL_85]], float %[[VAL_88]]
// CHECK:   %[[VAL_94:.*]] = fcmp ule float %[[VAL_91]], %[[VAL_93]]
// CHECK:   %[[VAL_95:.*]] = select i1 %[[VAL_94]], float %[[VAL_91]], float %[[VAL_93]]
// CHECK:   %[[VAL_96:.*]] = bitcast [100 x [200 x float]]* %[[VAL_12]] to float*
// CHECK:   %[[VAL_97:.*]] = getelementptr inbounds float, float* %[[VAL_96]], i32 %[[VAL_30]]
// CHECK:   store float %[[VAL_95]], float* %[[VAL_97]], align 4
// CHECK:   br label %[[VAL_36]]
// CHECK: }

// CHECK: define void @r47(i8* noalias align 16 dereferenceable(80000) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(80000) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(80000) %[[VAL_2:.*]], i8* noalias align 64 dereferenceable(160000) %[[VAL_3:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_4:.*]] = alloca float, align 4
// CHECK:   %[[VAL_5:.*]] = alloca float, align 4
// CHECK:   %[[VAL_6:.*]] = alloca float, align 4
// CHECK:   %[[VAL_7:.*]] = alloca float, align 4
// CHECK:   %[[VAL_8:.*]] = alloca float, align 4
// CHECK:   %[[VAL_9:.*]] = alloca float, align 4
// CHECK:   %[[VAL_10:.*]] = alloca float, align 4
// CHECK:   %[[VAL_11:.*]] = alloca float, align 4
// CHECK:   %[[VAL_12:.*]] = alloca float, align 4
// CHECK:   %[[VAL_13:.*]] = alloca float, align 4
// CHECK:   %[[VAL_14:.*]] = alloca float, align 4
// CHECK:   %[[VAL_15:.*]] = alloca float, align 4
// CHECK:   %[[VAL_16:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 0
// CHECK:   %[[VAL_17:.*]] = bitcast i8* %[[VAL_16]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_18:.*]] = getelementptr inbounds i8, i8* %[[VAL_1]], i64 0
// CHECK:   %[[VAL_19:.*]] = bitcast i8* %[[VAL_18]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_20:.*]] = getelementptr inbounds i8, i8* %[[VAL_2]], i64 0
// CHECK:   %[[VAL_21:.*]] = bitcast i8* %[[VAL_20]] to [100 x [200 x float]]*
// CHECK:   %[[VAL_22:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !92
// CHECK:   %[[VAL_23:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !93
// CHECK:   %[[VAL_24:.*]] = mul nuw nsw i32 %[[VAL_22]], 256
// CHECK:   %[[VAL_25:.*]] = add nuw nsw i32 %[[VAL_24]], %[[VAL_23]]
// CHECK:   %[[VAL_26:.*]] = icmp ult i32 %[[VAL_25]], 5120
// CHECK:   call void @llvm.assume(i1 %[[VAL_26]])
// CHECK:   %[[VAL_27:.*]] = mul nuw nsw i32 %[[VAL_25]], 4
// CHECK:   %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 200
// CHECK:   %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 200
// CHECK:   %[[VAL_31:.*]] = add nuw nsw i32 %[[VAL_27]], 1
// CHECK:   %[[VAL_32:.*]] = udiv i32 %[[VAL_31]], 1
// CHECK:   %[[VAL_33:.*]] = urem i32 %[[VAL_32]], 200
// CHECK:   %[[VAL_34:.*]] = udiv i32 %[[VAL_31]], 200
// CHECK:   %[[VAL_35:.*]] = add nuw nsw i32 %[[VAL_27]], 2
// CHECK:   %[[VAL_36:.*]] = udiv i32 %[[VAL_35]], 1
// CHECK:   %[[VAL_37:.*]] = urem i32 %[[VAL_36]], 200
// CHECK:   %[[VAL_38:.*]] = udiv i32 %[[VAL_35]], 200
// CHECK:   %[[VAL_39:.*]] = add nuw nsw i32 %[[VAL_27]], 3
// CHECK:   %[[VAL_40:.*]] = udiv i32 %[[VAL_39]], 1
// CHECK:   %[[VAL_41:.*]] = urem i32 %[[VAL_40]], 200
// CHECK:   %[[VAL_42:.*]] = udiv i32 %[[VAL_39]], 200
// CHECK:   %[[VAL_43:.*]] = icmp ult i32 %[[VAL_27]], 20000
// CHECK:   br i1 %[[VAL_43]], label %[[VAL_44:.*]], label %[[VAL_45:.*]]
// CHECK: r47.in_bounds-after:                              ; preds = %[[VAL_44]], %[[VAL_46:.*]]
// CHECK:   ret void
// CHECK: r47.in_bounds-true:                               ; preds = %[[VAL_46]]
// CHECK:   %[[VAL_47:.*]] = bitcast [100 x [200 x float]]* %[[VAL_17]] to float*
// CHECK:   %[[VAL_48:.*]] = getelementptr inbounds float, float* %[[VAL_47]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_49:.*]] = load float, float* %[[VAL_48]], align 4, !invariant.load !94
// CHECK:   %[[VAL_50:.*]] = bitcast [100 x [200 x float]]* %[[VAL_19]] to float*
// CHECK:   %[[VAL_51:.*]] = getelementptr inbounds float, float* %[[VAL_50]], i32 %[[VAL_27]]
// CHECK:   %[[VAL_52:.*]] = load float, float* %[[VAL_51]], align 4, !invariant.load !94
// CHECK:   store float %[[VAL_49]], float* %[[VAL_14]], align 4
// CHECK:   store float %[[VAL_52]], float* %[[VAL_13]], align 4
// CHECK:   call void @region_1_3(float* %[[VAL_14]], float* %[[VAL_13]], float* %[[VAL_15]])
// CHECK:   %[[VAL_53:.*]] = load float, float* %[[VAL_15]], align 4
// CHECK:   %[[VAL_54:.*]] = bitcast [100 x [200 x float]]* %[[VAL_21]] to float*
// CHECK:   %[[VAL_55:.*]] = getelementptr inbounds float, float* %[[VAL_54]], i32 %[[VAL_27]]
// CHECK:   store float %[[VAL_53]], float* %[[VAL_55]], align 4
// CHECK:   %[[VAL_56:.*]] = bitcast [100 x [200 x float]]* %[[VAL_17]] to float*
// CHECK:   %[[VAL_57:.*]] = getelementptr inbounds float, float* %[[VAL_56]], i32 %[[VAL_31]]
// CHECK:   %[[VAL_58:.*]] = load float, float* %[[VAL_57]], align 4, !invariant.load !94
// CHECK:   %[[VAL_59:.*]] = bitcast [100 x [200 x float]]* %[[VAL_19]] to float*
// CHECK:   %[[VAL_60:.*]] = getelementptr inbounds float, float* %[[VAL_59]], i32 %[[VAL_31]]
// CHECK:   %[[VAL_61:.*]] = load float, float* %[[VAL_60]], align 4, !invariant.load !94
// CHECK:   store float %[[VAL_58]], float* %[[VAL_11]], align 4
// CHECK:   store float %[[VAL_61]], float* %[[VAL_10]], align 4
// CHECK:   call void @region_1_3(float* %[[VAL_11]], float* %[[VAL_10]], float* %[[VAL_12]])
// CHECK:   %[[VAL_62:.*]] = load float, float* %[[VAL_12]], align 4
// CHECK:   %[[VAL_63:.*]] = bitcast [100 x [200 x float]]* %[[VAL_21]] to float*
// CHECK:   %[[VAL_64:.*]] = getelementptr inbounds float, float* %[[VAL_63]], i32 %[[VAL_31]]
// CHECK:   store float %[[VAL_62]], float* %[[VAL_64]], align 4
// CHECK:   %[[VAL_65:.*]] = bitcast [100 x [200 x float]]* %[[VAL_17]] to float*
// CHECK:   %[[VAL_66:.*]] = getelementptr inbounds float, float* %[[VAL_65]], i32 %[[VAL_35]]
// CHECK:   %[[VAL_67:.*]] = load float, float* %[[VAL_66]], align 4, !invariant.load !94
// CHECK:   %[[VAL_68:.*]] = bitcast [100 x [200 x float]]* %[[VAL_19]] to float*
// CHECK:   %[[VAL_69:.*]] = getelementptr inbounds float, float* %[[VAL_68]], i32 %[[VAL_35]]
// CHECK:   %[[VAL_70:.*]] = load float, float* %[[VAL_69]], align 4, !invariant.load !94
// CHECK:   store float %[[VAL_67]], float* %[[VAL_8]], align 4
// CHECK:   store float %[[VAL_70]], float* %[[VAL_7]], align 4
// CHECK:   call void @region_1_3(float* %[[VAL_8]], float* %[[VAL_7]], float* %[[VAL_9]])
// CHECK:   %[[VAL_71:.*]] = load float, float* %[[VAL_9]], align 4
// CHECK:   %[[VAL_72:.*]] = bitcast [100 x [200 x float]]* %[[VAL_21]] to float*
// CHECK:   %[[VAL_73:.*]] = getelementptr inbounds float, float* %[[VAL_72]], i32 %[[VAL_35]]
// CHECK:   store float %[[VAL_71]], float* %[[VAL_73]], align 4
// CHECK:   %[[VAL_74:.*]] = bitcast [100 x [200 x float]]* %[[VAL_17]] to float*
// CHECK:   %[[VAL_75:.*]] = getelementptr inbounds float, float* %[[VAL_74]], i32 %[[VAL_39]]
// CHECK:   %[[VAL_76:.*]] = load float, float* %[[VAL_75]], align 4, !invariant.load !94
// CHECK:   %[[VAL_77:.*]] = bitcast [100 x [200 x float]]* %[[VAL_19]] to float*
// CHECK:   %[[VAL_78:.*]] = getelementptr inbounds float, float* %[[VAL_77]], i32 %[[VAL_39]]
// CHECK:   %[[VAL_79:.*]] = load float, float* %[[VAL_78]], align 4, !invariant.load !94
// CHECK:   store float %[[VAL_76]], float* %[[VAL_5]], align 4
// CHECK:   store float %[[VAL_79]], float* %[[VAL_4]], align 4
// CHECK:   call void @region_1_3(float* %[[VAL_5]], float* %[[VAL_4]], float* %[[VAL_6]])
// CHECK:   %[[VAL_80:.*]] = load float, float* %[[VAL_6]], align 4
// CHECK:   %[[VAL_81:.*]] = bitcast [100 x [200 x float]]* %[[VAL_21]] to float*
// CHECK:   %[[VAL_82:.*]] = getelementptr inbounds float, float* %[[VAL_81]], i32 %[[VAL_39]]
// CHECK:   store float %[[VAL_80]], float* %[[VAL_82]], align 4
// CHECK:   br label %[[VAL_45]]
// CHECK: }

// CHECK: define internal void @region_1_3(float* dereferenceable(4) %[[VAL_0:.*]], float* dereferenceable(4) %[[VAL_1:.*]], float* dereferenceable(4) %[[VAL_2:.*]]) {
// CHECK: entry:
// CHECK:   %[[VAL_3:.*]] = alloca float, align 4
// CHECK:   %[[VAL_4:.*]] = load float, float* %[[VAL_0]], align 4
// CHECK:   %[[VAL_5:.*]] = load float, float* %[[VAL_1]], align 4
// CHECK:   %[[VAL_6:.*]] = fadd float %[[VAL_4]], %[[VAL_5]]
// CHECK:   store float %[[VAL_6]], float* %[[VAL_3]], align 4
// CHECK:   %[[VAL_7:.*]] = load float, float* %[[VAL_3]], align 4
// CHECK:   store float %[[VAL_7]], float* %[[VAL_2]], align 4
// CHECK:   ret void
// CHECK: }
// CHECK: attributes #0 = { nounwind readnone }
// CHECK: attributes #1 = { inaccessiblememonly nofree nosync nounwind willreturn }
// CHECK: attributes #2 = { nofree nosync nounwind readnone speculatable willreturn }
// CHECK: !nvvm.annotations = !{!0, !1, !2, !3, !4, !5, !6, !7, !8, !9, !10, !11, !12, !13, !14, !15, !16, !17, !18, !19, !20, !21, !22, !23, !24, !25, !26, !27, !28, !29, !30, !31, !32, !33, !34, !35, !36, !37, !38, !39, !40, !41, !42, !43, !44, !45, !46, !47, !48, !49, !50, !51, !52, !53, !54, !55, !56, !57, !58, !59, !60, !61, !62, !63, !64, !65, !66, !67, !68, !69, !70, !71, !72, !73, !74, !75, !76, !77, !78, !79, !80, !81, !82, !83, !84, !85, !86, !87, !88, !89, !90, !91}
// CHECK: !0 = !{void (i8*, i8*, i8*)* @r0, !"kernel", i32 1}
// CHECK: !1 = !{void (i8*, i8*, i8*)* @r0, !"reqntidx", i32 256}
// CHECK: !2 = !{void (i8*, i8*, i8*)* @r1, !"kernel", i32 1}
// CHECK: !3 = !{void (i8*, i8*, i8*)* @r1, !"reqntidx", i32 256}
// CHECK: !4 = !{void (i8*, i8*, i8*)* @r2, !"kernel", i32 1}
// CHECK: !5 = !{void (i8*, i8*, i8*)* @r2, !"reqntidx", i32 256}
// CHECK: !6 = !{void (i8*, i8*, i8*)* @r3, !"kernel", i32 1}
// CHECK: !7 = !{void (i8*, i8*, i8*)* @r3, !"reqntidx", i32 256}
// CHECK: !8 = !{void (i8*, i8*, i8*)* @r4, !"kernel", i32 1}
// CHECK: !9 = !{void (i8*, i8*, i8*)* @r4, !"reqntidx", i32 256}
// CHECK: !10 = !{void (i8*, i8*, i8*)* @r5, !"kernel", i32 1}
// CHECK: !11 = !{void (i8*, i8*, i8*)* @r5, !"reqntidx", i32 256}
// CHECK: !12 = !{void (i8*, i8*, i8*)* @r7, !"kernel", i32 1}
// CHECK: !13 = !{void (i8*, i8*, i8*)* @r7, !"reqntidx", i32 1024}
// CHECK: !14 = !{void (i8*, i8*, i8*)* @r8, !"kernel", i32 1}
// CHECK: !15 = !{void (i8*, i8*, i8*)* @r8, !"reqntidx", i32 256}
// CHECK: !16 = !{void (i8*, i8*, i8*)* @r9, !"kernel", i32 1}
// CHECK: !17 = !{void (i8*, i8*, i8*)* @r9, !"reqntidx", i32 256}
// CHECK: !18 = !{void (i8*, i8*, i8*)* @r10, !"kernel", i32 1}
// CHECK: !19 = !{void (i8*, i8*, i8*)* @r10, !"reqntidx", i32 256}
// CHECK: !20 = !{void (i8*, i8*, i8*)* @r11, !"kernel", i32 1}
// CHECK: !21 = !{void (i8*, i8*, i8*)* @r11, !"reqntidx", i32 256}
// CHECK: !22 = !{void (i8*, i8*, i8*)* @r12, !"kernel", i32 1}
// CHECK: !23 = !{void (i8*, i8*, i8*)* @r12, !"reqntidx", i32 256}
// CHECK: !24 = !{void (i8*, i8*, i8*)* @r13, !"kernel", i32 1}
// CHECK: !25 = !{void (i8*, i8*, i8*)* @r13, !"reqntidx", i32 256}
// CHECK: !26 = !{void (i8*, i8*, i8*)* @r14, !"kernel", i32 1}
// CHECK: !27 = !{void (i8*, i8*, i8*)* @r14, !"reqntidx", i32 256}
// CHECK: !28 = !{void (i8*, i8*, i8*)* @r15, !"kernel", i32 1}
// CHECK: !29 = !{void (i8*, i8*, i8*)* @r15, !"reqntidx", i32 256}
// CHECK: !30 = !{void (i8*, i8*, i8*)* @r16, !"kernel", i32 1}
// CHECK: !31 = !{void (i8*, i8*, i8*)* @r16, !"reqntidx", i32 256}
// CHECK: !32 = !{void (i8*, i8*, i8*)* @r17, !"kernel", i32 1}
// CHECK: !33 = !{void (i8*, i8*, i8*)* @r17, !"reqntidx", i32 256}
// CHECK: !34 = !{void (i8*, i8*, i8*)* @r18, !"kernel", i32 1}
// CHECK: !35 = !{void (i8*, i8*, i8*)* @r18, !"reqntidx", i32 256}
// CHECK: !36 = !{void (i8*, i8*, i8*)* @r19, !"kernel", i32 1}
// CHECK: !37 = !{void (i8*, i8*, i8*)* @r19, !"reqntidx", i32 256}
// CHECK: !38 = !{void (i8*, i8*, i8*)* @r20, !"kernel", i32 1}
// CHECK: !39 = !{void (i8*, i8*, i8*)* @r20, !"reqntidx", i32 256}
// CHECK: !40 = !{void (i8*, i8*, i8*)* @r22, !"kernel", i32 1}
// CHECK: !41 = !{void (i8*, i8*, i8*)* @r22, !"reqntidx", i32 256}
// CHECK: !42 = !{void (i8*, i8*, i8*)* @r23, !"kernel", i32 1}
// CHECK: !43 = !{void (i8*, i8*, i8*)* @r23, !"reqntidx", i32 1024}
// CHECK: !44 = !{void (i8*, i8*, i8*)* @r24, !"kernel", i32 1}
// CHECK: !45 = !{void (i8*, i8*, i8*)* @r24, !"reqntidx", i32 256}
// CHECK: !46 = !{void (i8*, i8*, i8*)* @r25, !"kernel", i32 1}
// CHECK: !47 = !{void (i8*, i8*, i8*)* @r25, !"reqntidx", i32 256}
// CHECK: !48 = !{void (i8*, i8*, i8*)* @r26, !"kernel", i32 1}
// CHECK: !49 = !{void (i8*, i8*, i8*)* @r26, !"reqntidx", i32 256}
// CHECK: !50 = !{void (i8*, i8*, i8*, i8*)* @r27, !"kernel", i32 1}
// CHECK: !51 = !{void (i8*, i8*, i8*, i8*)* @r27, !"reqntidx", i32 256}
// CHECK: !52 = !{void (i8*, i8*, i8*, i8*)* @r28, !"kernel", i32 1}
// CHECK: !53 = !{void (i8*, i8*, i8*, i8*)* @r28, !"reqntidx", i32 1024}
// CHECK: !54 = !{void (i8*, i8*, i8*, i8*)* @r29, !"kernel", i32 1}
// CHECK: !55 = !{void (i8*, i8*, i8*, i8*)* @r29, !"reqntidx", i32 256}
// CHECK: !56 = !{void (i8*, i8*, i8*)* @r30, !"kernel", i32 1}
// CHECK: !57 = !{void (i8*, i8*, i8*)* @r30, !"reqntidx", i32 256}
// CHECK: !58 = !{void (i8*, i8*, i8*, i8*)* @r31, !"kernel", i32 1}
// CHECK: !59 = !{void (i8*, i8*, i8*, i8*)* @r31, !"reqntidx", i32 256}
// CHECK: !60 = !{void (i8*, i8*, i8*, i8*)* @r32, !"kernel", i32 1}
// CHECK: !61 = !{void (i8*, i8*, i8*, i8*)* @r32, !"reqntidx", i32 256}
// CHECK: !62 = !{void (i8*, i8*, i8*, i8*)* @r33, !"kernel", i32 1}
// CHECK: !63 = !{void (i8*, i8*, i8*, i8*)* @r33, !"reqntidx", i32 256}
// CHECK: !64 = !{void (i8*, i8*, i8*, i8*)* @r34, !"kernel", i32 1}
// CHECK: !65 = !{void (i8*, i8*, i8*, i8*)* @r34, !"reqntidx", i32 256}
// CHECK: !66 = !{void (i8*, i8*, i8*, i8*)* @r35, !"kernel", i32 1}
// CHECK: !67 = !{void (i8*, i8*, i8*, i8*)* @r35, !"reqntidx", i32 1024}
// CHECK: !68 = !{void (i8*, i8*, i8*, i8*)* @r36, !"kernel", i32 1}
// CHECK: !69 = !{void (i8*, i8*, i8*, i8*)* @r36, !"reqntidx", i32 256}
// CHECK: !70 = !{void (i8*, i8*, i8*, i8*)* @r37, !"kernel", i32 1}
// CHECK: !71 = !{void (i8*, i8*, i8*, i8*)* @r37, !"reqntidx", i32 256}
// CHECK: !72 = !{void (i8*, i8*, i8*, i8*)* @r38, !"kernel", i32 1}
// CHECK: !73 = !{void (i8*, i8*, i8*, i8*)* @r38, !"reqntidx", i32 256}
// CHECK: !74 = !{void (i8*, i8*, i8*, i8*)* @r39, !"kernel", i32 1}
// CHECK: !75 = !{void (i8*, i8*, i8*, i8*)* @r39, !"reqntidx", i32 256}
// CHECK: !76 = !{void (i8*, i8*, i8*, i8*)* @r40, !"kernel", i32 1}
// CHECK: !77 = !{void (i8*, i8*, i8*, i8*)* @r40, !"reqntidx", i32 256}
// CHECK: !78 = !{void (i8*, i8*, i8*, i8*)* @r41, !"kernel", i32 1}
// CHECK: !79 = !{void (i8*, i8*, i8*, i8*)* @r41, !"reqntidx", i32 256}
// CHECK: !80 = !{void (i8*, i8*, i8*, i8*)* @r42, !"kernel", i32 1}
// CHECK: !81 = !{void (i8*, i8*, i8*, i8*)* @r42, !"reqntidx", i32 256}
// CHECK: !82 = !{void (i8*, i8*, i8*, i8*)* @r43, !"kernel", i32 1}
// CHECK: !83 = !{void (i8*, i8*, i8*, i8*)* @r43, !"reqntidx", i32 256}
// CHECK: !84 = !{void (i8*, i8*, i8*, i8*)* @r44, !"kernel", i32 1}
// CHECK: !85 = !{void (i8*, i8*, i8*, i8*)* @r44, !"reqntidx", i32 256}
// CHECK: !86 = !{void (i8*, i8*, i8*, i8*, i8*)* @r45, !"kernel", i32 1}
// CHECK: !87 = !{void (i8*, i8*, i8*, i8*, i8*)* @r45, !"reqntidx", i32 256}
// CHECK: !88 = !{void (i8*, i8*, i8*, i8*, i8*)* @r46, !"kernel", i32 1}
// CHECK: !89 = !{void (i8*, i8*, i8*, i8*, i8*)* @r46, !"reqntidx", i32 256}
// CHECK: !90 = !{void (i8*, i8*, i8*, i8*)* @r47, !"kernel", i32 1}
// CHECK: !91 = !{void (i8*, i8*, i8*, i8*)* @r47, !"reqntidx", i32 256}
// CHECK: !92 = !{i32 0, i32 20}
// CHECK: !93 = !{i32 0, i32 256}
// CHECK: !94 = !{}
// CHECK: !95 = !{i32 0, i32 1024}

HloModule Test

add_F32 {
  lhs = f32[] parameter(0)
  rhs = f32[] parameter(1)
  ROOT add = f32[] add(lhs, rhs)
}

ENTRY main {
  a = f32[100, 200]{1,0} parameter(0)
  b = f32[100, 200]{1,0} parameter(1)
  c = f32[100, 200]{1,0} parameter(2)
  i0 = s32[100, 200]{1,0} parameter(3)
  i1 = s32[100, 200]{1,0} parameter(4)
  cplx = c64[100, 200]{1,0} parameter(5)
  p0 = pred[100, 200]{1,0} parameter(6)
  p1 = pred[100, 200]{1,0} parameter(7)

  r0 = f32[100, 200]{1,0} abs(a)
  r1 = f32[100, 200]{1,0} round-nearest-afz(a)
  r2 = f32[100, 200]{1,0} ceil(a)
  r3 = s32[100, 200]{1,0} count-leading-zeros(i0)
  r4 = f32[100, 200]{1,0} convert(a)
  r5 = f32[100, 200]{1,0} bitcast-convert(a)
  r6 = f32[100, 200]{1,0} copy(a)
  r7 = f32[100, 200]{1,0} cosine(a)
  r8 = f32[100, 200]{1,0} exponential(a)
  r9 = f32[100, 200]{1,0} exponential-minus-one(a)
  r10 = f32[100, 200]{1,0} floor(a)
  r11 = f32[100, 200]{1,0} imag(cplx)
  r12 = pred[100, 200]{1,0} is-finite(a)
  r13 = f32[100, 200]{1,0} log(a)
  r14 = f32[100, 200]{1,0} log-plus-one(a)
  r15 = pred[100, 200]{1,0} not(p0)
  r16 = f32[100, 200]{1,0} negate(a)
  r17 = s32[100, 200]{1,0} popcnt(i0)
  r18 = f32[100, 200]{1,0} real(cplx)
  r19 = f32[100, 200]{1,0} reduce-precision(a), exponent_bits=5, mantissa_bits=12
  r20 = f32[100, 200]{1,0} rsqrt(a)
  // r21 = f32[100, 200]{1,0} logistic(a)
  r22 = f32[100, 200]{1,0} sign(a)
  r23 = f32[100, 200]{1,0} sine(a)
  r24 = f32[100, 200]{1,0} sqrt(a)
  r25 = f32[100, 200]{1,0} cbrt(a)
  r26 = f32[100, 200]{1,0} tanh(a)

  r27 = f32[100, 200]{1,0} add(a, b)
  r28 = f32[100, 200]{1,0} atan2(a, b)
  r29 = pred[100, 200]{1,0} compare(a, b), direction=EQ
  r30 = c64[100, 200]{1,0} complex(a, b)
  r31 = f32[100, 200]{1,0} divide(a, b)
  r32 = f32[100, 200]{1,0} maximum(a, b)
  r33 = f32[100, 200]{1,0} minimum(a, b)
  r34 = f32[100, 200]{1,0} multiply(a, b)
  r35 = f32[100, 200]{1,0} power(a, b)
  r36 = f32[100, 200]{1,0} remainder(a, b)
  r37 = f32[100, 200]{1,0} subtract(a, b)
  r38 = pred[100, 200]{1,0} and(p0, p1)
  r39 = pred[100, 200]{1,0} or(p0, p1)
  r40 = pred[100, 200]{1,0} xor(p0, p1)
  r41 = pred[100, 200]{1,0} add(p0, p1)
  r42 = s32[100, 200]{1,0} shift-left(i0, i1)
  r43 = s32[100, 200]{1,0} shift-right-arithmetic(i0, i1)
  r44 = s32[100, 200]{1,0} shift-right-logical(i0, i1)

  r45 = f32[100, 200]{1,0} select(p0, b, c)
  r46 = f32[100, 200]{1,0} clamp(a, b, c)

  ROOT r47 = f32[100, 200]{1,0} map(a, b), dimensions={0, 1}, to_apply=add_F32
}
