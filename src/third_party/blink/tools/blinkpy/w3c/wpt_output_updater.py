# Copyright 2019 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Updates the output of a WPT run.

The purpose of this script is to do any post-processing on the WPT output that
is necessary to support features unavailable in WPT. For example, TestExpectations
for flaky tests where multiple possible statuses are listed for a test. However,
WPT only supports a single status so it can't understand that a test is flaky.

This script takes two inputs:
1. The output JSON file of a WPT run
2. The set of test expectations files

It then creates a new output JSON file with any required adjustments (eg: the
expected statuses of flaky tests filled in from the expectations file).
"""

import argparse
import json
import logging
import os

from blinkpy.common.system.log_utils import configure_logging

_log = logging.getLogger(__name__)


class WPTOutputUpdater(object):

    def __init__(self, expectations):
        """
        Args:
            expectations: a blinkpy.web_tests.models.test_expectations.TestExpectations object
        """
        self.expectations = expectations
        self.old_json_output_file_path = None
        self.new_json_output_dir = None
        self.new_json_output_filename = None

    def run(self, args=None):
        """Main entry point to parse flags and execute the script."""
        parser = argparse.ArgumentParser(description=__doc__)
        parser.add_argument("--old-json-output-file-path",
                            help="The JSON output file to be updated, generated by WPT.")
        parser.add_argument("--new-json-output-dir",
                            help="The directory to put the new JSON output file.")
        parser.add_argument("--new-json-output-filename",
                            help="The name of the new JSON output file.")
        parser.add_argument('-v', '--verbose', action='store_true', help='More verbose logging.')
        args = parser.parse_args(args)

        log_level = logging.DEBUG if args.verbose else logging.INFO
        configure_logging(logging_level=log_level, include_time=True)

        self.old_json_output_file_path = args.old_json_output_file_path
        self.new_json_output_dir = args.new_json_output_dir
        self.new_json_output_filename = args.new_json_output_filename
        self._update_output_and_write()

        return 0

    def _update_output_and_write(self):
        """Updates the output JSON and writes it to disk."""
        old_output_file = open(self.old_json_output_file_path, "r")
        new_output_json = self.update_output_json(json.load(old_output_file))

        # Ok, we updated all the tests that were in this file, output it
        if not os.path.exists(self.new_json_output_dir):
            os.makedirs(self.new_json_output_dir)
        new_file_path = os.path.join(self.new_json_output_dir,
                                     self.new_json_output_filename)
        if os.path.exists(new_file_path):
            _log.warning("Output file already exists, will be overwritten: %s",
                         new_file_path)

        with open(new_file_path, "w") as new_output_file:
            json.dump(new_output_json, new_output_file)

    def update_output_json(self, output_json):
        """Updates the output JSON object in place.

        Args:
            output_json: a nested dict containing the JSON output from WPT.

        Returns:
            The updated JSON dictionary
        """
        delim = output_json['path_delimiter']
        # Go through each WPT expectation, try to find it in the output, and
        # then update the expected statuses in the output file.
        for e in self.expectations.expectations():
            test_leaf = self._find_test_for_expectation(e, delim, output_json)
            if test_leaf is not None:
                self._update_output_for_test(e, test_leaf)
        return output_json

    def _find_test_for_expectation(self, exp, delim, output_json):
        """Finds the test output for the specified expectation.

        We only handle expectations for WPT tests, so we skip non-WPT entries as
        well as directory-wide expectations for WPT. We also want to ensure that
        the test from the expectation was run by the specific shard that created
        the output.

        Args:
            exp: an expectation object representing a line from the expectation
                    file.
            delim: the delimiter character in the test names.
            output_json: a nested dict containing the JSON output from WPT.

        Returns:
            The leaf node from the JSON output file for the test that needs to
            be updated, which is a dictionary. Or None if this expectation is
            not supposed to be handled.
        """
        # Skip expectations for non-WPT tests
        if not exp.name or not exp.name.startswith('external/wpt'):
            return None

        # Split the test name by the test delimiter. We omit the first 2 entries
        # because they are 'external' and 'wpt' and these don't exist in the WPT
        # run.
        test_name_parts = exp.name.split(delim)[2:]

        # Drill down through the JSON output file using the parts of the test
        # name to find the leaf node containing the results for this test.
        test_leaf = output_json['tests']
        for name_part in test_name_parts:
            # Since this script runs on each shard, it's possible that the test
            # from the expectations file was not run in this shard. If we don't
            # find the test in the WPT output, then skip the expectation.
            if name_part not in test_leaf.keys():
                _log.debug("Test was not run: %s", exp.name)
                return None
            test_leaf = test_leaf[name_part]

        # Ensure that the expectation is for an actual test, not a directory. To
        # do this we check that we're at a leaf, which should have 'actual' and
        # 'expected' fields.
        if 'actual' not in test_leaf or 'expected' not in test_leaf:
            _log.debug("Expectation was not for a test, skipping: %s", exp.name)
            return None

        # If we get this far then we have an expectation for a single test that
        # was actually run by this shard, so return the test leaf that needs to
        # be updated in the JSON output.
        return test_leaf

    def _update_output_for_test(self, exp, test_leaf):
        """Updates the output of a specific test based on the expectations file.

        Args:
            exp: an expectation object representing a line from the expectation
                 file.
            test_leaf: a dictionary containing the JSON output for a test.
        """
        expectation_string = ' '.join(exp.expectations)
        _log.info("Updating expectation for test %s from %s to %s",
                  exp.name, test_leaf['expected'], expectation_string)
        test_leaf['expected'] = expectation_string

        # Also update the "is_regression" and "is_unexpected" fields.
        is_unexpected = test_leaf['actual'] not in expectation_string
        test_leaf['is_unexpected'] = is_unexpected
        test_leaf['is_regression'] = is_unexpected and test_leaf['actual'] != 'PASS'
