#!/usr/bin/python3
# Copyright 2021 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.
"""Parses allocation profiles from a trace and computes the external
fragmentation from PartitionAlloc

This parses an allocation profile generated by PartitionAlloc in the thread
cache. This will only give data on Chrome instances where the thread cache is
enabled, and PA_THREAD_CACHE_ALLOC_STATS is defined, that is non-official
builds.

To collect a profile:
- Build a non-official chrome version (Should be a release build for accurate
  reports)
- Collect a trace with the memory-infra category enabled (in chrome://tracing)
- Save it as json.gz, and load it here.
"""

import argparse
import logging
import os
import re

from matplotlib import pylab as plt
import numpy as np

from parse_trace import LoadTrace, GetAllocatorDumps, ProcessNamesAndLabels


def _ParseTrace(trace: dict) -> dict:
  """Parses a trace, and returns thread cache stats.

  Args:
    trace: As returned by LoadTrace()

  Returns:
    {pid  -> {'name': str, 'labels': str, 'data': np.array}.
    Where the data array contains 'size' and 'count' columns.
  """
  dumps = GetAllocatorDumps(trace)
  pid_to_name, pid_to_labels = ProcessNamesAndLabels(trace)

  result = {}
  for dump in dumps:
    pid = dump['pid']
    allocators = dump['args']['dumps']['allocators']

    # The browser process also has global dumps, we do not care about these.
    if 'global' in allocators:
      continue

    result[pid] = {
        'name': pid_to_name[pid],
        'labels': pid_to_labels.get(pid, '')
    }
    size_counts = []
    pattern = re.compile("malloc/partitions/allocator/buckets/bucket_\d+")
    for allocator in allocators:
      if (not pattern.match(allocator)):
        continue
      attrs = allocators[allocator]['attrs']
      allocated_objects_size = int(attrs['allocated_objects_size']['value'],
                                   base=16)
      slot_size = int(attrs['slot_size']['value'], base=16)
      try:
        size = int(attrs['size']['value'], base=16)
        fragmentation = 1 - allocated_objects_size / size
      except KeyError:
        assert allocated_objects_size == 0
        size = 0
        fragmentation = 0
      assert allocated_objects_size <= size

      size_counts.append(
          (slot_size, 100 * fragmentation, size - allocated_objects_size))
      size_counts.sort()
      result[pid]['data'] = np.array(size_counts,
                                     dtype=[('size', np.int),
                                            ('fragmentation', np.int),
                                            ('unused', np.int)])

  return result


def _PlotProcess(all_data: dict, pid: int, output_prefix: str):
  """Represents the allocation size distribution.

  Args:
    all_data: As returned by _ParseTrace().
    pid: PID to plot the data for.
    output_prefix: Prefix of the output file.
  """
  data = all_data[pid]
  logging.info('Plotting data for PID %d' % pid)

  # Fragmentation vs size.
  plt.figure(figsize=(16, 8))
  plt.title('Fragmentation (%%) vs Size - %s - %s' %
            (data['name'], data['labels']))
  plt.stem(data['data']['size'], data['data']['fragmentation'])
  plt.xscale('log', base=2)
  plt.yscale('linear')
  plt.ylim(ymin=0, ymax=100)
  plt.xlabel('Size (log)')
  plt.ylabel('Fragmentation (%)')
  plt.savefig('%s_%s_fragmentation.png' % (output_prefix, pid),
              bbox_inches='tight')
  plt.close()

  # Unused vs size.
  plt.figure(figsize=(16, 8))
  plt.title('Unused Memory vs Size - %s - %s' % (data['name'], data['labels']))
  plt.xscale('log', base=2)
  plt.yscale('log', base=2)
  plt.stem(data['data']['size'][data['data']['unused'] != 0],
           data['data']['unused'][data['data']['unused'] != 0])
  plt.ylim(ymin=1, ymax=2**20)
  plt.xlabel('Size (log)')
  plt.ylabel('Unused Size (log)')
  plt.savefig('%s_%d_unused.png' % (output_prefix, pid), bbox_inches='tight')
  plt.close()


def _CreateArgumentParser():
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--trace',
      type=str,
      required=True,
      help='Path to a trace.json[.gz] with memory-infra enabled.')
  parser.add_argument('--output-dir',
                      type=str,
                      required=True,
                      help='Output directory for graphs.')
  return parser


def main():
  logging.basicConfig(level=logging.INFO)
  parser = _CreateArgumentParser()
  args = parser.parse_args()

  logging.info('Loading the trace')
  trace = LoadTrace(args.trace)

  logging.info('Parsing the trace')
  stats_per_process = _ParseTrace(trace)

  logging.info('Plotting the results')
  for pid in stats_per_process:
    if 'data' in stats_per_process[pid]:
      _PlotProcess(stats_per_process, pid,
                   os.path.join(args.output_dir, 'fragmentation_result'))


if __name__ == '__main__':
  main()
